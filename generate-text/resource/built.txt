I started building the app by first drawing the mockups or rough sketches on paper. After designing came the part of development. For backend I used firebase. For version control system, I used github and whole source code of this app is present on my github repository.    We use ctags to extract symbol information from source code. From there, we create a dependency graph and turn it into a Lucidchart diagram. Finally, we use the hierarchy layout in Lucidchart to space out the elements perfectly, even for large diagrams.This add-on was developed using Atlassian Connect for Bitbucket Cloud. We were also able to try out the new API Proxy feature and save hours of development time.What's next for Class Diagrams for BitbucketWe would love to hear and incorporate your feedback as we continue developing this integration! So far, wed like to incorporate these features:Automatic diagram update when new code is pushedImproved performance when generating large diagramsSupport for multiple programming languagesAbility to embed diagram into the repos README in one clickBuilt Withatlassian-connectbitbucketbitbucket-api-proxyjavascriptscalaTry it outwww.lucidchart.com      Submitted to    Atlassian Codegeist 2017Winner                Best Software Team App                  Created by  Dmitry PashkevichI am a software engineer at Lucid Software where I work on Lucidchart and its integrations with Atlassian products.Gregg Hernandez  ParkKing consists of four main components:ParkKing Android app for Citizens: It helps citizens to find, book, avail and pay for parking online. Also, he can report and view his violations timely before an e-challan gets issued.ParkKing Android app for Parking Site: It would install in Android/Linux powered custom build device for scanning QR Code or RFID tag at parking entry and exit for safety and time recording for analytics and invoice generation.ParkKing Management Console (PHP based Web Application) for City Traffic Department: On login, traffic officer would be able to view and approve or reject submitted violations or suggested parkings. Further, analytics and reports can be checked for understanding parking patterns and violations within the city.ParkKing Gamification with Analytics Engine and Backend: We have a robust gamification engine to gamify users experience of the app, thereby motivating him to adapt parking discipline and spread among others.  We reimplemented some of the back end code of our cloud app within the Confluence environment and tweaked the front end to work with it. In the process we had to make significant changes to core system concepts like users, collaboration, comments, diagrams, and attachments so that they make sense on this new platform. Since this add-on has to work on instances without any internet access, we had to bundle all the necessary resources (js, css, images, fonts, templates, etc.) inside the add-on instead of loading them on-demand from a CDN. Implementing some of our unique features like real-time collaboration was particularly challenging as this is not a feature implemented for Server-based add-ons, and the approach was new to the add-on ecosystem.  It took us quite a lot of time because we already have an integration with Jira that connects from Cronforce to Jira and pulls all booked times. Now we had to implement the opposite plus ensure both integration work smoothly with each other as well as make the addon smarter and user friendly e.g. pre-select project to track etc.  Using eclipe (Android) + Mysql +GCM +FACEBOOK API + PICASSO + SQLLITE...  I started building the app by first drawing the mockups or rough sketches on paper. After designing came the part of development. For backend I used firebase. For version control system, I used github and whole source code of this app is present on my github repository.    We use ctags to extract symbol information from source code. From there, we create a dependency graph and turn it into a Lucidchart diagram. Finally, we use the hierarchy layout in Lucidchart to space out the elements perfectly, even for large diagrams.This add-on was developed using Atlassian Connect for Bitbucket Cloud. We were also able to try out the new API Proxy feature and save hours of development time.What's next for Class Diagrams for BitbucketWe would love to hear and incorporate your feedback as we continue developing this integration! So far, wed like to incorporate these features:Automatic diagram update when new code is pushedImproved performance when generating large diagramsSupport for multiple programming languagesAbility to embed diagram into the repos README in one clickBuilt Withatlassian-connectbitbucketbitbucket-api-proxyjavascriptscalaTry it outwww.lucidchart.com      Submitted to    Atlassian Codegeist 2017Winner                Best Software Team App                  Created by  Dmitry PashkevichI am a software engineer at Lucid Software where I work on Lucidchart and its integrations with Atlassian products.Gregg Hernandez  ParkKing consists of four main components:ParkKing Android app for Citizens: It helps citizens to find, book, avail and pay for parking online. Also, he can report and view his violations timely before an e-challan gets issued.ParkKing Android app for Parking Site: It would install in Android/Linux powered custom build device for scanning QR Code or RFID tag at parking entry and exit for safety and time recording for analytics and invoice generation.ParkKing Management Console (PHP based Web Application) for City Traffic Department: On login, traffic officer would be able to view and approve or reject submitted violations or suggested parkings. Further, analytics and reports can be checked for understanding parking patterns and violations within the city.ParkKing Gamification with Analytics Engine and Backend: We have a robust gamification engine to gamify users experience of the app, thereby motivating him to adapt parking discipline and spread among others.  We reimplemented some of the back end code of our cloud app within the Confluence environment and tweaked the front end to work with it. In the process we had to make significant changes to core system concepts like users, collaboration, comments, diagrams, and attachments so that they make sense on this new platform. Since this add-on has to work on instances without any internet access, we had to bundle all the necessary resources (js, css, images, fonts, templates, etc.) inside the add-on instead of loading them on-demand from a CDN. Implementing some of our unique features like real-time collaboration was particularly challenging as this is not a feature implemented for Server-based add-ons, and the approach was new to the add-on ecosystem.  It took us quite a lot of time because we already have an integration with Jira that connects from Cronforce to Jira and pulls all booked times. Now we had to implement the opposite plus ensure both integration work smoothly with each other as well as make the addon smarter and user friendly e.g. pre-select project to track etc.  Using eclipe (Android) + Mysql +GCM +FACEBOOK API + PICASSO + SQLLITE...  Maasto-101 Version1 is built using Arduino 101 that transmits  6-axis Gyroscope and Accelerometer data through BLE to Raspberry Pi 3, Here the data is stored for further processing and also a real time 3D graph is drawn using Plotly. NEO 6M GPS attached to Raspberry Pi 3 gives continuous latitude and longitude of the location, those location details are stored in the database and also drawn on Map to give the visual of the location to the driver. A camera attached to Maasto-101 will capture images and process those images to identify potholes. Identified potholes are highlighted with a green box on the actual image and the visual is shared with the user, this helps him to avoid the potholes when he/she drives through the region. The images captured are processed using Open source OpenCV python library.Image captured along with the processed image, location is highlighted on the Map, and a 3D graph of the pothole is drawn on a dashboard to the user. This dashboard can be accessed either in mobile or a tab.  Using TensorFlow DNN classifier    Using scikit-learn    We developed the Image Editor as a Confluence macro. It stores the original files and the annotation files as page attachments, which allows us to save data only within a customers Confluence instance. Such an approach ensures its privacy and integrity. Moreover, we built it for the Confluence Cloud and Server.      Rapid prototyping Techniques such as 3D printing is used extensively in the development of Prototypes. Initially all  circuits are designed and tested in breadboards before final integration. NRF Connect App is used to verify the Sensor Nodes Services and Characteristics UUIDs.      The development environment was set up with Xcode IDE  for SWIFT coding and using an AWS account for usage of its web services.  Our chrome-extension is built using javascript that uses advanced web scraping techniques to extract links, posts, and images. This is then sent to an AI. The AI is a collection of API calls that we collectively process to produce a single "trust" factor. The APIs include Microsoft's cognitive services such as image analysis, text analysis, bing web search, Twitter's search API and Google's Safe Browsing API. The backend is written in Python and hosted on Heroku. The chatbot was built using Facebook's wit.ai  I used Android Studio to start the app and then used Flir SDk to create the app.I used the Java and basic ImageView technique to get started further we will add OpenCV for counting and then Flir to Store data generate alert.    This is a hybrid app which is built using Ionic 2 Framework and Cordova Phonegap. Its Admin panel website is built using Angular JS and Bootstrap framework and hosted on Github pages.. For the database of feeds we have used Google Firebase Database along with AngularFire.Accomplishments that I'm proud ofTraffeed helps in reducing the communication gap between the general pubic and the traffic police department, as an application is an much easier way to publish updates about the path obstructions that can not possibly be predicted and printed in the newspaper a day before. What's next for TraffeedWill be trying to reach more people by updating in different language too along with added features like custom search and sort , push notifications and would advance to a social platform in further updates.Built Withangular.jsangularfirecssfirebasegithubgoogle-mapshtmlionic-2javascriptscsstypescriptTry it outbuild.phonegap.commohitkh7.github.iogithub.com      Submitted to    Hack and Roll Indore    Created by  I Worked on App & Website Development.I implemented the UI designed by team mates. Mohit KhandelwalI worked on the video and also wrote the content (feeds,alerts etc)for the app.Vasundhara  TrivediI worked on the app video.Yashwini  NathwaniI worked on the design of the app as well as the content describing it.chhavi sharmaAkshita KanojiaAshita SaxenaAnjali Chourey  Kins aim is to give game developers a unique way to monetize their apps in ways that enhance, not interrupt, the user experience. Kin provides the best solution for Unity developers. However, Kin needed to ensure that its specific needs are addresseding their pain points. The companies collaborated to remove barriers. Following are the building blocks of Kin SDK plugin:       Used Kin Name-Space in project       Added Block-chain Listeners provided in the SDK       Created Instances for Client and Account       Created an Account       On-boarded the Account with Kin Server and Funded the Account       Handled Transactions       Sent Transactions to Server for playing the game       Implemented event listeners on the block-chain       Converted the project to 64 bit       Built the APK for testing on Mobile       Built the .AAB for uploading to Google Play Store  The visualization top layer is pure Javascript using AUI components, VisJS and backbone.js. The middle layer containing the business logic is written in Java and uses technologies like EMF for modeling. The bottom layer provides bi-directional synchronization with native JIRA objects such as workflows, issue-types, schemes and issues.  First collected video feeds from traffic signal for training our algorithms to make them differentiate between who is breaking law & who is not.Break those video into images and then store them different folder for training, validation purpose.Trained machine learning algorithm for making classification on traffic signal and no-parking area video feed.Built solution to read live video feed and pass it to machine learning algorithms for classification purpose like are anyone breaking law?Accomplishments that I'm proud ofThis is first machine learning task i have perform for practical purpose. I am happy with the result it is showing and making Indore more smarter by using existing hardware infrastructure it has like cctv, speakers.What I learnedHow to think for any practical problems and come up with solutions. First think about solution which can use existing infrastructure so new solution can cost less. Many new think about machine learning like overfitting issues, training and validation data issue.  What's next for TrafficoNow we want build more smart solution like if someone breaking traffic rules capture his number plate and generate challan with that automate complete challan system so people can get alert about challan on their mobile number. They have to pay the same online for more transparency.Built Withdata-analysisdeep-learningmachine-learningneural-networkpythonTry it outgoo.glgoo.gl      Submitted to    Hack and Roll Indore    Created by  Shubham GuptaPradeep Raghuwanshi    We have used Google Maps, java, android-studio, php, sql to provide you a workable product.    We used React for the front end, and a combination of Node.js and Python for the back-end. Our machine learning models for recommending articles were built using Python's Tensorflow library, and NLP was performed using the Alyien, Semantria, and Google Cloud Natural Language APIs.What we learnedWe learned a great deal more about fake news, and NLP in particular.What's next for Open MindWe aim to implement a "political thermometer" that appears next to political articles, showing the degree to which the particular article is conservative or liberal. In addition, we aim to verify a Facebook-specific "share verification" feature, where users are asked if they are sure they want to share an article that they have not already read (based on their browser history).Built Withnode.jspythonreacttensorflowTry it outopenmind.press      Submitted to    YHack 2017Winner                Poynter Fellowship: Best Hack to Counter Fake News                  Created by  I took the great APIs Jeff found and found the best way to call them to maximize the accuracy of our bias ranking and counter-article search results. I also helped integrate the RNN Michael set up into our counter-article algorithm. Finally, I set up a reliable and secure backend in the cloud. Alexander CuiRandom idea generator with a good filterStefan UddenbergPostdoctoral Fellow at Princeton Neuroscience InstituteJeff AnFull stack developer with a passion for socially meaningful projects.Michael Lopez-Brau  Make & Get Jobs is built using Xcode. Before the app was completed I used the Sketch software to sketch the layout of the app. Later came the hardest part; how the app is going to retrieve and upload information from/to the server. After all that hardwork the app was finally created.  It uses PHP codes to interact with the Linux server. Some tasks are done directly through the PHP codes and for some, a shell script is created, executed to accomplish the task and then deleted as soon as the task is done. All the Script handling is also done through the PHP codes only.Just write the command inside <command> to execute on Linux.    We introduced the 4G technology  and the concepts of Big Data by access to the open data in the website of Taiwan Area National Freeway Bureau. The software resources of Node.js, MySQL, and Java were applied to setup the APP of Traffic Transformer for the adjacent areas in Hsuehshan Tunnel.    We used TensorFlow, Python and NLTK.Accomplishments that we're proud ofAll the models have good accuracy and can enhance further if more diverse data is present.What's next for Machine learning modelsTo improve the model accuracy by collecting customer satisfaction reviews and the data generated by the customers after integrating with PowerUp may even be used for further training the model using that data for better performance.Built WithpythontensorflowTry it outgithub.com      Submitted to    Power Up Automation    Created by  Mashrin SrivastavaSaumya Suvarna  The extension utilizes Google Chrome's desktop capture API for initial screen and voice recording. We then use Chrome's native client capabilities to run native C++ code that encodes the video in a compact WEBM format. A view link is immediately generated by HYFY servers while the videos are uploaded to AWS S3 buckets where they are transcoded to multiple common formats for optimized desktop and mobile viewing.  For sharing, users can copy/paste the link or use one of our integrations to HipChat, JIRA or Slack.  The HipChat integration uses the new connect framework to generate custom glances, cards, actions and dialogs.  The JIRA integration users JIRA's APIs to construct and auto-complete form fields specific to the JIRA project and allows users to create new issues or update existing ones with HYFY videos.  Videos within JIRA include debugging information like the original URL and browser and OS details to assist with debugging.Here are videos that demonstrate the integrations:JIRA Admins - Authorizing HYFYSharing HYFY videos in JIRAHipChat HYFY setupSharing HYFY videos in HipChat    Using Jira APIs, java, jquery, velocity template files.  There was no better way of demonstrating the solution than actually applying it to a scaled down version of a manufacturing facility.  The first step was to build a scaled down model of the facility with touch points that we could integrate and control electronically. Our scaled down model included functioning boom barriers, weighing scales, Unloading Bays, Parking Areas and Surveillance cameras.A boom barrier, for instance was created using a Servo motor and Ultrasonic Sensors connected to ESP 12E microcontroller. The boom barrier is actuated by commands sent over MQTT by a Raspberry Pi which doubles up as a BLE receiver and IoT Hub client. Each of these devices are registered in Azure's IoT Hub. When a truck (tagged with a BLE beacon) approaches the boom barrier, a series of parameters are sent over to Azure for authentication. The data is validated and an Acknowledged/Not Acknowledged command is sent back to the BLE receiver. Based on the response, the boom barrier is opened. This principle is followed for the weighing scale and loading bay as well.The data captured by the device is passed on to the gateway which is then passed on to IoT Hub. IoT Hub is responsible for passing it on to Logic apps which is in responsible for processing, storing events and responding to the device with commands. The entire model as seen in the video was built using a combination of Actuators (motors), Sensors, Controllers and Raspberry Pis. The weighing scale used CZL 601 Load cell, HX 711 sensor connected to ESP8266 node MCU and the unloading bay used a water pump motor connected to motor driver L298N motor driver, all connected to an ESP 12E NodeMCU.We used toy trucks to model movable assets and proximity BLE Beacons for location tracking of the movable assets indoor. There were several components like the buildings, roads etc that were built for aesthetics and were not necessarily electronically controlled.The compliance was constructed using Deep Learning.A camera is connected to a Raspberry Pi which is running an object detection model based on Google's Mobilenet_SSD. Whenever a human is detected, the frame of the live video is sent over to Azure's CustomVision.Here, we trained a model to check if the input frame consists of a Helmet and Vest, which are two essential wearables for safety.  I built out an app that uses several different elements. The main mobile app is a ionic based mobile engine that takes advantage of multiple APIs and back end services. It connects to a angular.js backend.  We took the word Bulit so seriously that we actually built from scratch, a livable prototype model of a home having living room, kitchen, washroom and a garden area within our startup office space. The new home and bean bag is so comfy that nobody wants to work in their desks anymore!Next step was to make it a Smart Home. Wandering the market place in the scorching heats of Delhi and digging out the best deal for our sensors, we found that the prices for water meters vary quite drastically (from $250+ for industrial grade sensors to $6 for garden hose attachments that limit total usage based on rough estimates). We settled on a $6 water meter used for water heater (the YF-S201 Hall Effect Water Flow Counter) due to its accuracy, low price, small size, electronic sensing method that could be easily measured by a microcontroller (such as an Arduino) and obviously for its price too! Once we finished enjoying our milkshakes and admiring our new built home, it was time to get to the fun part. Firstly we embedded the water tank and all the pipes with sensors like water flow meters and outdoor garden areas with soil moisture sensor. Secondly, the conventional water meter was replaced with Samsung ARTIK 10. As MQTT has emerged as a standard messaging protocol for IoT, we followed the publish-subscribe (pub/sub) model here and ARTIK 10 plays a key role on the network as the "message broker". We set up an ARTIK 10 device with Mosquitto to become a simple MQTT broker and used Node-RED development tool to easily send and receive MQTT messages.Since we wanted to have an internet connected data-stream of our water usage from all the pipes, we decided to try out the Node-MCU with built-in WiFi and made them as our edge devices. Each water flow meter is connected to its Node-MCU module having a code for sending water flow readings to the ARTIK running MQTT server. We then subscribed them to messages published to the broker (i.e. ARTIK 10). Data was then sent to the server (i.e. ARTIK 10) at regular intervals when the water source was in use.In total we have used:One flow meter for kitchenOne each for shower, water closet and wash basin respectively.One flow meter for water inlet of tank and one flow meter for water outlet of the tank.Apart from it we have used two water solenoids with Node-MCU which can be controlled wirelessly through ARTIK 10 and being a server connected to internet, the solenoids can be controlled externally by the authorities. First solenoid, integrated with soil moisture sensor for turning on the controlled water supply used for gardening the plants and another on the inlet of water tank. Only gardening solenoid can be controlled via our interactive dashboard.To visualize the data and make sense of water usage patterns over time, we made a dashboard using iobroker.vis on ARTIK 10 to achieve user friendly interactive display. This allowed us to place a display next to the drawing room for immediate feedback, while still offering us a more holistic view to compare multiple sources and look at first-order trends.  StepShot at the second consists of two parts - Java Confluence plugin and Windows desktop client (we have a cross-platform client prototype ready, but it's not yet fully functional).Desktop client allows you to capture a process in a form of step-by-step guide. Additionally - you can edit separate screenshots and annotate text descriptions for your steps. When you hit save - everything is pushed back to the Confluence.    This snippet has been built on uipathAccomplishments that we're proud ofIt works perfectly fine on gmailWhat's next for Downloading Attachment from EmailYou need to enter the email address, password and folder path where you want to download the attachement.Built WithuipathTry it outdrive.google.com      Submitted to    Power Up Automation    Created by  I contributed in the capacity of development of the snippet.Sachin BhataleI developed Snippets for Gmail and Outlook attachments download using UiPathPratik Hase  Using Azure ML I had done deep analysis on the dataset so that I get a good pattern on results.  Metropia has been a project many years in the making. Our talented team includes leaders in transportation management who have vast experience in using software and technology to improve our roadways. While the foundation for this app is already active in several markets, adapting it for this solution took a new approach.  Android StudioArduino Uno and kits associatedESP32Orb Slam  I integrated Microsoft Graph API with a telephony API and built the website and app to authenticate the user on both platforms.  Built using Uipath activitiesAccomplishments that I'm proud ofIt segregates the PDFs files with 100% accuracyWhat I learnedI learned almost every automation is possible using UiPath, You just need to identify the way of doing it.What's next for Native and Scanned PDF SegregationThe files which have been moved under scanned folder can be utilized by another automation which has The OCR Engines activities Like Abbyy Flexi capture, Microsoft OCR or Google OCR to extract the information from PDFs, And the files which have been moved under Native Folder can be utilized by another automation which consists of Full text or Native methods to extract the data from PDFs. Or Anyone can use it or modify it evolve and develop further activities associated with PDFs.Built WithuipathTry it outdrive.google.com      Submitted to    Power Up Automation    Created by  Developed the idea and automationMustaheer SayyedDefined the architecture, To Be flow and performed testingAbhishek KumarHello World !    We iterated with Facebook Messenger API to create an interactive chat experience for immigrants. Just head to our Facebook Fan Page and start a message.We used heroku, postgresql, bootbot, node.js  I got the malaria cell images data-set from KaggleI examined the images i got and separated them into different categories for training, testing and for validation3.I setup my Microsoft Azure environment by registering and signing up for azure services.I logged on to my Microsoft Azure portal, and created a resource for the project.I then logged on to link where I loaded my images and trained my classification model or image recognition system.I exported my trained model for deployment into Android as an android application by exporting the model for further development in Android studio using Java programming language to build the application.I developed the android application and generated the signed apk for it, so as to test it and make it available for use.  Built with software tools, android studio, KIN, love and cloud.  TouchWizards was built using Unity and C# , we using "Dotween" for UI animation , Unity Collab and github for collaborating and trello for task management  Bootstrap, HTML5, CSS3, php, javascript, fonteawesome,   On the backend, I chose Python as the preferred coding language, and luckily, Amazon already has an Alexa skills kit example for Python. I then connected my code to the Google Analytics API for querying. The skill also uses a bit of Amazon's DynamoDB to store the user's view/profile ID, so that it runs smoothly without the user asking to configure every time.    We built it using AWS Lambda functions and dynamoDb. When a viewer first enters their tracking number we attempt to match it up to one of our compatible carriers, if we're unsure of the exact carrier we may prompt the user. This lookup is done by a Lambda function. Once the lookup is complete and the package is valid, we ask the user to send their message and we create an entry in our dynamoDb. We then continue to check the status and will alert the streamer in the Live Config when a package is delivered.  We had some previous experience with applications like phonegap and Xamarin, but we were never really impressed with any non-native mobile development tools. This time we tried to test our luck and went with React Native which ended up paying off big time. We managed to very easily create a beautiful application for both iOS and Android and link it to our back-end using packages that we are familiar from using node.js. Our back-end was more of a challenge. First we had to analyze the photo we received from the phone. At first, we tried to use an LSTM based OCR processing tool called Tesseract. It seemed to work properly only half the time, so we tried to find something better. The big problem with OCR is that all the good systems are proprietary so we had to go with Azure's OCR offering. It works quite well. After getting the text out of the image we had to process it. We had made an attempt at vectorizing the words we got and tried creating a conditional random field model. The problem is that they mostly ignore the order of words, so it didn't work for us. Something like an LSTM would have probably worked better, but we didn't have enough data and it was outside the scope of this hackathon. So we resorted to designing a hand made tool-set that we used to process the words. Overall from a technical perspective the whole process works quite well.  This application was built using Android Studio, Python, Django, and other softwares/languages/services in order to host a database server for the user data.  With caffeine and a lack of sleep. But really, we used first started with a brainstorming session where each one of us chose two favorite ideas to research on, and to defend for. After settling on an idea, we moved on to planning the different phases needed to reach our end goal.As for the technologies used, Android Studio and its speech to text conversion library to create an app that allows users to speak aloud the object they seek and convert their message to text while identifying the name of the object from their request. Next, using the open source machine learning framework TensorFlow  we altered it's  Object Detection API only identify       We built this web-app starting with a WixCode front-end, that communicates with an Azure VM that will detect sobriety through computer vision. One of the tests uses a custom-built, custom-trained TensorFlow CNN model. The other uses Azure's custom vision classifier. We leveraged NC6's immense server power to effectively train our neural networks very quickly, with our custom-created data-sets, to achieve stellar results for an accurate result, all in hackathon-amounts of time.  We built a WebVR  Editor that is able to generate 360 HTML pages as well as 360 videos using React and Three.Js.The WebVR editor allows you to quickly and easily create 360 web scenes and experiences in a few minutes. These scenes are then made available in the extension for streamers to use either as transparent scenes or non transparent scenes. The extension itself was built using a few of AWS services such as S3 and Dynamodb.  We use a plethora of sensors that are connected to a Raspberry Pi. Sensors range from temperature to light-sensitivity, with one sensor even detecting humidity levels. Through this, we're able to collect data from the sensors and post it on a google sheet, using the Google Drive API.Once the data is posted on the google sheet, we use a python script to retrieve the 3 latest values and make an average of those values. This allows us to detect a change and send a flag to other parts of our algorithm.For the user, it is very simple. They simply have to text a number dedicated to a certain garden. This will allow them to create an account and to receive alerts if a plant needs attention.This part is done through the Twilio API and python scripts that are triggered when the user sends an SMS to the dedicated cell-phone number.We even thought about implementing credit and verification systems that allow active users to gain points over time. These points are earned once the user decides to take action in the garden after receiving a notification from the Twilio API. The points can be redeemed through the app via Interac transfer or by simply keeping the plant once it is fully grown. In order to verify that the user actually takes action in the garden, we use a visual recognition software that runs the Azure API. Through a very simple system of QR codes, the user can scan its QR code to verify his identity.Try it outgithub.com      Submitted to    McHacks 6Winner                People's Choice              Winner                Telus                  Created by  Greg WooMilo SobralRomain NithZoe Lapomme  We built it with HTML and Javascript First, we will be integrating it with an SMS API and a Facebook Messenger API later for the working chatbot.  I build the app with Java using Android Studio for Android. All the UI elements were prototyped initially with the help of Sketch. I used Parse by FaceBook as a backend for the app.    Native android development, built with android studio.    I building this application in Javascript using Express.js. To solve Math problems, this application uses mathjs library which makes everything work smooth.  Hardware: A Raspberry Pi with camera snaps a photo periodically and sends it to the backend, and if it determines that the item is recyclable, a visual alert is displayed on the OLED screen.Backend: The Python/Flask server first sends the photo to Microsoft Cognitive Services' Computer Vision API for analysis, which returns a textual description of the image. If that result is not conclusive, then the image is sent to a convolutional neural network (GoogLeNet) built on the Caffe library that has been pre-trained to do material classification.Database: The backend logs successful recognitions to an Elasticsearch database, including the type of recycled material seen and the timestamp.Frontend: Kibana is used to serve a dynamic dashboard.  Blood sweat and tears - and a LOT of redbull. I developed a PHP backend with Swift frontend on the mobile application.  The first problem to be solved was the Lumos device that could connect to Diesel generators and retrieve information via MODBUS. We used serial port available to retrieve information through RS-485. The device had the logic of mapping retrieved values to pre - defined register mapping for the controller.The raspberry PI ran an IOT Hub client which could communicate information seamlessly to IOT Hub using MQTT. Python SDK for IOT Client was used for the purpose.       We built the IoT device (Smart Container) using load cells to sense and post the weight to AWS cloud. Cloud application was built on Amazon AWS cloud with integration to Amazon shopping api and AWS IoT. The word document (GrocMan.docx) attached in the submission post depicts the high level solution architecture and what pieces of AWS are getting used for the solution.  This solution consists of the device, cloud and business insights. I got a Microsoft HD lifecam HD-3000 and raspberry pi 3 which i installed raspbian stretch operating system. Then i installed opencv, cognitive face python sdk, and azure IoT python sdk. OpenCV was used to access the camera for real time capturing. I created a Face resource after getting my free account and then copied the endpoint and key which i used in my python script to access the API. The reply from the API was parsed and the emotion data gotten was sent to the IoT hub.The IoT hub was created and the device was registered. A stream job was created to stream data coming from the hub into a Datalake gen1 storage, where power BI picks from. Created nice visualizations with power BI for providing insights.  We used the Unity engine, supplemented by calculations and prototyping with Mathematica.  Alertness Index was built using an algorithm that analyses the user responses and gives weight after compiling usual and normal alertness levels of individuals based on their reaction to scenarios . Depart By feature was built using Google Maps API , open weather API and other traffic APIs . The algorithm for attention index calculation can be found here  We built this Internet of Things application by integrating various hardware and software. We chose Arduino as our micro controller (C) (Thanks to Intel Corporation for providing the same) and GSM module as gateway device. By interfacing different sensor to C mentioned below we constructed a complete data collector unit.-> HC-SR04 Ultrasonic Sensor to measure level of water-> DS18B20 Temperature Probe-> Generic pH Probe-> SEN0189 Turbidity Sensor-> ENV-40-DO Atlas Scientific Dissolve Oxygen ProbeLets see how people can access this data..In order to store this data and perform various analytics on it we interfaced this system with cloud using GPRS packet. We choose IBM Bluemix cloud to store and analyse data because Bluemix enables web and mobile applications to be rapidly and incrementally composed from services.An application to collect received data was created using Node-RED which is supported by many services on bluemix along with a beautiful dashboard link .This application performs various analytics like where is the system located?, What is the average temperature in a day, week and year?, Is the water pH and DO2 are under the range recommended by WHO or not? , Is the Water Drinkable?    the backend is in firebase, the app is built with cordova/ionic  The central axis of this project is the Arduino 101. This board takes the Intel Curie microcontroller.It is a small but complete chip. Among its features it has a built-in IMU or Inertial Measurement Unit that has an accelerometer and gyroscope and Bluetooth connectivity through BLE or Bluetooth Low Energy technology.One important thing to keep in mind with this prototype board is that you have to have the most current Firmware possible. This is because what you are actually doing is emulating an Arduino.To update the Firmware follow these instructions.Going back to the initial approach of the project, we must do several things with the Arduino 101. Measure various temperatures, power consumption and control the fans to generate that fresh air flow in the back of the refrigerator.Particularly, we made the decision to measure back side temperature, front temperature, freezer and refrigerator. This gives us 4 temperature sensors and a sensor to measure the electrical consumption.For data collection, we used the DS18B20 temperature sensor and the SCT-013 sensor that measures the electrical consumption.It is not mandatory to use these models, any other similar sensor can be used to obtain the temperature and consumption information.DS18B20 Temperature SensorThe Maxim Integrated DS18B20 temperature sensor offers a temperature range between -55  C and 155  C, with an accuracy of  0.5  C.The model we use is the commercialized as a waterproof probe avoiding any problem inside the refrigerator or freezer. This sensor uses Maxim Integrated's 1-Wire communication protocol allowing the simultaneous connection of more than 100 sensors through a single pin.Electric consumption sensor SCT-013To measure consumption, we use a non-invasive current sensor. So we do not worry about having to alter the refrigerator power cord too much. The only thing we have to do is peel the cable and embrace only one of the two power cables, live or ground.This type of sensors are known as Current Transformers and one of the most common is the SCT-013. They operate under the principle of electromagnetic induction. The output of these sensors is an intensity proportional to that which passes through a cable or electric circuit.There are two types of sensors, which provide either a current or a voltage. The same thing you use, but the simplest is the one that offers us a voltage.FansFinally we connect the fans. Taking into account the consumption, dimensions and noise they can generate. We can use any model, but we must keep a close eye on these components since there are different models for different uses and not all are worth.The idea is to be able to adapt the project to use the typical fans of the computers since they consume very little and are very economic. Those who are testing Jos Manuel and Germn are Boxer Fan type.Connecting Arduino 101 to the InternetAlthough, as I said, it is a very robust and very useful plate, the big drawback is that it does not have an Internet connection. This forced us to look for solutions to send all the information acquired to a platform in the cloud.There are different options like using Bluetooth to connect to another device that has an Internet connection. But perhaps the simplest and cheapest we found was to use a NodeMCU.This development kit is based on the ESP8266 and is very easy to use (it is compatible with the Arduino IDE) and comes at a very economical price and have a library to configure WiFi like WiFiManagerAll logic of the project resides in the Arduino 101 and NodeMCU only makes bridge between the plate and Internet.Send information to the platform in the cloudThe data must be visible in real time at all times. Only then will we verify if the project is viable and we can make the right decisions.As a cloud platform we choose Firebase, then we will detail its features.One of its advantages is that it is very easy to use thanks to the REST API that incorporates. With only one HTTP call we can store the data obtained from temperature, consumption and use of the fans.All of this is sent in JSON format. Therefore, the integration between Firebase and NodeMCU is instantaneous.The Firebase databaseThe features we seek to integrate a cloud service into this project are ease and cost. At present there are many services that we can get to use like Thingspeak, ThingerIO or the Arduino Cloud itself. All may be a good choice.But perhaps because of our experience in web development, we chose Firebase. The first thing to note is that you have APIs for many programming languages among which JavaScript is found.The database has limitations as it is a free account.It does not allow more than 100 simultaneous connections.You have a limit of 1 GB of storage.You have a limit of 10GB of download.With all this, our project after a month of data collection every minute of two refrigerators, we have a storage consumption of 3.9 MB and 158.4 MB of download.Web application for data visualizationWith everything previously mounted, there is nothing left to see the information in a friendly way on any device.If we want to monitor the data in real time, from anywhere in the world and any device, one of the faster options is to create a web application.Firebase has a Javascript API that makes it very easy to integrate a database from this platform into a web page.In addition we added the Bootstrap framework for it to be responsive. We have used an open template for this framework, SB Admin 2.All this has resulted in a website that can be viewed at www.iotfridgesaver.com. It consists of two screens.The initial screen shows a map of Google where we will be locating the different refrigerators that are used.Once you find the one you want to consult, just give the icon and click on info. From here you can access the historical temperature, consumption and use of fan.  We prototyped stratejos as a set of web services that communicated with the JIRA REST API. Once we began understanding our users, we developed the JIRA Cloud integration. The Cloud integration was then used to inform the JIRA Server integration. We use a microservices architecture. For example, we have services for data collection, visualisation (reports & dashboards), intelligence and authentication.   CITYSCOPE connects APIS like ECOBICI, UBER and SIN-TRAFICO. Also is crowd sourced, we need people help us to mapping the bus routes.   Hootchat is built in 2 components: Client and Server    Client: we used JavaScript with Angular.js working on the IONIC platform to generate the apps for Android and IOs. CSS3 is also used, following the concepts about flat design.    Server: built with framework spring using language JAVA 8, with WebSockets implementation, SOA architecture, docker and MySQL database. Server hosts the application and database in separated containers. WebService is integrated with IONIC sending async requisitions for the push notification, and its deployed in the VPS Digital Ocean.    I used google VR SDK, wrote the code in C# using visual studio 2017 as IDE and built the app with Unity 3D; harnessing its on-board physics engine. I designed the architectural model using ArchiCAD 18 and carried out its file format conversion using Cinema 4D R20 Demo.  Leveraging Facebook Messenger bot channel for residents (No app needed + max reach) to help them report cases and participate in 4 of 5 DKB contests. Also to view traffic rules, leaderboard standingsDKB platform server (Java Spring) to maintain all user, reported cases, game calculations, leaderboards, traffic rules etc. Also for analyzing openXC vehicle commands received to analyze driving behaviour of resident and rate him for Top Driver categoryAndroid app for resident for 5th contest only for people having DKB Android app & openXCAndroid app for validator role (traffic authority) to validate reported alert, violation etc. cases by residentsAndroid app for admin to change traffic rules of city and notify (via FB Messenger) all users on DKB platform       I used JavaScript, with node.js running on the backend with the Express web server, and a custom function to override the browser's default window.onerror in order to make the GET request. I utilized Node's fs module to edit files on the computer.  We built park.AI with a Node.js backend using Express as a router, Bootstrap and jQuery on the frontend and SightHound as our computer vision API. Anything needed to be stored is on a MongoDB.  AI is built using Tensorflow, web application is NodeJS, and search and aggregation supported by ElasticSearch.  I have built it using HTML, CSS, JQuery, Microsoft apis    C#, EWS, Angular JS, SQLServer. Azure, Microsoft Graph, Office 365 REST API    Android Studio IDE.Android Design Support Library for Material Design.Parse Backend with Push Notification.Google Maps.Google Places.Picasso, Retrofit y OkHttp.Dagger2 y Butterknife.  We used an AD8232, a GPS module, Arduino 101, ESP8266, Android studio to develop the app, a web console for uploading data to server (cloudmqtt.com)  We used 2 ARTIK10 development boards and a Particle Photon, as well as a relay board operating valves. The video has details about what each one's role is in the solution. Between them, they provide mobile phone application hosting and physical system control and instrumentation, emulationSvr hosting, database hosting and messaging to tie it all together. The solution also uses a Particle Photon running a NATS messaging client that will talk to Apcera's NATS messaging server running on one of the ARTIK10 boards. The Particle Photons could also have easily been replaced with ARTIK1 or ARTIK5 chips. The overall concept, if implemented would result in additional smart devices in society.  We built it using Javascript and the Google Maps API.    I used the following components to build this solution. Configured Raspberry Pi to run Debian Buster and installed Azure IOT edge Runtime on it. Using the custom vision demo model for running AI module on edge devices as reference I created a new custom model by tagging child in a car seat for prediction. Configured the temperature simulator module and deployed  it to Azure IOT hub.   Azure IOT hub as a response to the IOT device messages is setup to trigger a function to call the Twilio API for making emergency calls and send messages with Car Details and location.    We used the not so widely known Widmark formula (scientifically proven) to calculate BAC. We take the weight, drink consumed, serving size and the number of drinks as inputs. The user can also give multiple inputs in one instance. The complete logic is built in our API invoked through the lambda function.  We used a raspberry pi to handle the motion sensors and the audio.The bins themselves we forged in the fires of Mount Doom.We used a zoom mic to record the audio and audacity to edit it.  It's a Meteor app that lets you store descriptions of REST API endpoints (URL, etc). For each one it provides a publication that can be subscribed to by any DDP client.The core of the app is a Meteor.publish function that connects to a REST API. It loads the data, parses out an array and converts that array into documents that get published to the client.It then continues to poll the REST endpoint until the subscription is stopped (on a frequency that you define), comparing the new results with the previous and sending just the changes over the DDP connection to the subscribed client.Meteor packagesaccounts-githubaccounts-uialdeed:autoformaldeed:collection2aldeed:simple-schemacheckhttpkadira:blaze-layoutkadira:flow-routerlessmomentjs:momentmquandalle:jadetwbs:bootstraprandomreactive-varNpm packagesdeep-diffjsonpathmoniker  The development of the application is built using MIT app inventor.  Designed a 3D model of a power socket that can be combined into a power strip by just adding more modules (like LEGO)Assembled and coded an Arduino Nano for each socket with power consumption and temperature sensorsUsed I2C protocol to connect each Slave Arduino Nano in each socket to the Master ESP8266 so information from the Slaves can be uploaded to the cloudConnected the ESP8266 to a Heroku server to post the information retrieved from each arduino nano on the server so it could be acessed by an android applicationBuilt an android application to present all sensors information to the user    We built a node.js server on which our front-end was deployed. Queries from our front-end are passed to the blockchain via rest-api server. Anomaly detection model was run on flask server which is used to detect tampered IoT meter readings.   To build the web application, we used HTML, Javascript, CSS, and Node.js. To add the main functionalities for our application, we implemented the IBM Watson API, Firebase, Aylien API, Easybib API, and Nature API.  We created a chrome-extension that would submit a get-request on hiroku server using a flask app that we created. Through the chrome-extension, we can send the URL of the active tab to the server where a reliability-score is calculated. This algorithm starts by breaking down the URL's HTML code to view the article's title and paragraphs in a JSON file.        Red Brick is built using HTML, CSS, python, django, javascript, jquery, and angular.js.  The currency itself is a smart contract that runs on the ethereum virtual machine. It was written in solidity and then transpiled to machine code.  Based on the new JIRA's project centric view, we have used some modules available on the Atlassian SDK to inject panels to display each entity managed by the plugin: Deals, Activities, Organizations and Contacts. Also, some other modules have been used to display reports on the JIRA's default report listing and a panel inside the issue view.Everything has been built using AngularJS, providing a faster using interaction to drag-n-drop or changing views.  We stacked GrovePi on top of RaspberryPi and connected Ultrasonic Ranger Sensor to GrovePi Digital pin 3. Camera module v2 is connected to RaspberryPi. Both camera and ultrasonic ranger are attached to the bin (see GitHub for more details on this). ** Azure IoT Edge modules** on Raspberry Pi detects when trash is thrown into the bin (with the help of ultrasonic ranger) and camera captures an image of bin contents. A Customized computer vision model analyses image and returns the items list present in the image. This list is compared with the contaminants list, which can be customized by the administrator, and if a contaminant is found, image is uploaded to blob storage and event details are stored to the SQL database via Azure Functions. Recycle.io web application displays bin details like type, location along with violation data to the user and uses Azure functions to get required information. It also allows the administrator to customize contaminants list which is forwarded to the edge device via Azure Service Bus and azure functions.  I used Firebase to save user info, flights info and Kin Android Client SDK for the android app and Django app powered by Kin Python SDK to verify account create and send Kin transactions from the backend.    Wheels was created using the most advanced technological tools on the market (digital maps, mechanism digital confirmation of the information, algorithms to match, etc.) to offer a service that aims to improve the welfare of the society. In that sense Wheels operates for each of the different existing operating systems on the market (Android, IOS (iPhone), and in Web version. Thanks to the Wheels design, you can create a route specifying how much are you willing to walk, how much time will you spend traveling and how much CO2 are being saved by the users thanks to the use of Wheels.  Using Uipath studio, API calls, Service Now        First we create a website where people share where they want to go, which days and what time. Then we make a city heat map and start to propose where the route should exist, the most voted route is the one officially opened. Finally the route is integrated to a very simple mobile application where users can see the Van in turn, check schedule and reserve a seat.    We built this with Node.js on the backend, React-Native on the frontend (app), MongoDB and Azue vision API for OCR.  The app is organised around a native messaging platform.   The app does not collect any personal information from the users so that full privacy is respected and people do not have to share their phone numbers, emails, names...    The front end was built using React.js and Ionic Framework and the backend api was built using Node.js and MongoDB.Built Withcssexpress.jsherokuhtmlionicjavascriptmongodbmongoosenode.jsnpmoauthreactTry it outgithub.com      Submitted to    Cybros Hackathon 2019Winner                GitHub Bagpacks                  Created by  I was the backend developer to the project, worked with node and mongoDB for the database. Developing a project for a hackathon is quite a challenging and a fun task to do. Really loved doing this one!!Akshat JainI worked on the front end of this app in React and Ionic.Subhajit NandiTackling one framework at a time...I developed the front end of the app using react and ionic framework. I also worked on the functionality of the QR  Code.Sourabh TripathiI worked on prorotyping and UI/UX designing.Designed all the elements starting from logo to cards.Abhimanyu Gupta  We used Lambda function to define our logic for question and answers. Then we linked the Lambda function with our Alexa skills.   The solution is developed on Predix platform and uses BMSs WIP APIs extensively to capture the plant floor data.Order Time Prediction: Part / Material availability is captured in the Predix based application and is monitored for consumption in operations. To capture consumption of Part/Material, the system traverse through all open work orders where the parts are used and captures the exact time at which the part inventory reaches Safety stock level.Order Validation- Sales visibility: Application uses the WIP API to capture the open work orders to get the earliest availability of work centers for the new work order. Also, applications Part inventory is validated to check the availability of Parts required for the work order based on current consumption rate. In addition, system also lists any work orders with low priority (based on promised date) to show any options available to swap the work order for earlier delivery.Order Tracking  Operation visibility: System uses WIP APIs to get all open work orders. A drill-down on specific work order, all Work order related information like, Time of operations, work centers , Part/Material availability (at time of operation) and current projected time of work order completion are captured from system and reported.The solution was originally proposed as three different ideas and on detailed study we understood the combined system offers better synergies.    Messenger platform components were used to build the frontend. For the backend, a RESTful Web Service was built using spring boot. The travel preferences from the users are sent as POST requests to this service. Since Spring  supports embedded Tomcat servlet container as the HTTP runtime, the packaged bootable 'fat jar' was deployed in openshift easily using the openJDK S2I image of JBoss Middleware. What I learnedI learnt to use JBoss Middleware like openJDK to deploy the backend services on OpenShift Online. I also learnt to design user-friendly interface for the frontend using the messenger platform components. Built Withmessengeropenjdkopenshift-onlinespring-boot      Submitted to    Hack your Travel with OpenShiftWinner                Best App by a Student Team                  Created by  deleted deleted        Using Google's Firebase database and storage, I was able to upload and download data so users can share recipes across devices. I also used AutoLayout in Xcode to set up my storyboard and design elements. Using UIKit and a few third party APIs, I was able to connect the backend and the frontend of my app.   When I heard about this incredible opportunity from Make School on May 1st, I started conceptualizing this app right away. For my first prototype, I made sketches of screens for the app and pretended to click through the paper. I created user stories to accompany my app. User stories are descriptions of who the user is, what the want, and why. They help define the usefulness of each feature in my app and help me plan through the development of the app. Shortly after, I began to build my app using Swift 3 and Xcode 8, the programming language and integrated development environment created by Apple.Accomplishments that I'm proud ofI loved the process of designing and coding the app.I have been teaching myself how to code for almost a year. I learned so much in such a short amount of time through tutorials and listening to friends who gave me advice.  We used a technology called Electrodialysis, where an electric field is used to separate salt ions from neutral water. This technology uses a series of water channels separated by ion exchange membranes which allow only a certain kind of ion to pass through them. We designed and built our own stack of these channels and membranes as well as all of the plumbing and circuitry that goes into controlling the flow of water. We also built circuits for introducing the electric field into the desalination stack, and switching the polarity of the stack in order to prolong the device's lifetime. We also integrated salinity and flow sensors to detect the state of the device.The Artik acted as the brains and communicator for our device. We utilized almost every GPIO pin of the Artik (it has a lot!). We used a C program with unix sockets to monitor the sensor readings, and we used Node.js to control the circuitry. We used Firebase to store and read our sensor readings and user settings in real time. We designed and built a custom web application for the EDDI. The application is entirely a frontend app made with React.js. The app is designed for ergonomic use and ease of understanding. We've provided an easy way for farmers to visualize and understand water salinity, which is not an immediately intuitive idea to grasp.    The Add-In was built using new JavaScript API for Office (Outlook), modern Office UI Fabric as a framework for UI, as well as Microsoft Graph API as a way to access users email data. We also use Docker and Java on the server-side and host everything using Microsoft Azure.Please, review below diagram to get a better idea about how the Molecula works and which technologies it uses:  There are three stages we do.   First, data collection, including Field research, collection of scientific papers and so on. We visit Ilan to observe the problem for over 5 times. Besides, we interview 2 professors who know transportation information industry throughly and read over 20 papers and articles.   Second, policy development. We discuss for over 10 solutions like lower interval, combination with OpenXC and so on. We discuss with more than 10 local residents to make sure find out a great solution.    Third, software construction. We write a "real time" app and website. In front-end, we first draw manuscripts and discuss UI/UX for 2 weeks. In the back-end, we host a server in Amazon, and run a NoSQL database.  We build a real time connection with web socket that can change information immediately when the database change. We also developed a web crawler to crawl information from official websites.   The back-end is written in Node.js and Express, and uses IBM's Watson to analyse the threat with deep learning.  I built it using the Alexa Skills kit and NodeJS service running on AWS environment. The nodeJs service uses Twitter REST API to look for most recent tweet about the bus number.  I used Swift 3.0 to build 9 view controllers, 5 tableviews within 5 view controllers, and 4 tab bar controllers. I stored and displayed information from the core data onto the personal history screen. I used NSDate, date formatter, segmented control, alert controllers, open-source audio files, delegation, datasource, pop-up view, keyboard dismissal function, and clickable cells that would open links in Safari and YouTube.  We've gathered a list of common words for several European languagesAmazon's Ivona Text-to-Speech SaaS, the most realistic TTS we could find, is used to say the words to the userMyScript's CDK is used to recognize the strokes written on the HTML canvasGame engine tracks the user's progressAll libraries/resources we used are specified at http://www.spellscript.xyz/credits  We decided that breveo should first learn to summarize the audio of a video, so we began looking into machine learned text summarization.  After researching different open source machine learning libraries, we began using Google's 2016 release TensorFlow.  However, after running many hours of machine training on our GPU and CPUs, we still weren't able to generate good enough models.  So we began looking for pre-trained models, that have been trained on multiple GPU's for many days.  We landed with OpenNMT (under Harvard University) which had pre-trained models that trained on GigaWord (the de-facto text data for machine learning).  ChallengesMachine learning for a long time was something only cutting-edge researchers could work on.  Now, thanks to open source libraries like TensorFlow and OpenNMT, machine learning is in the hands of anyone who knows how to run a python script.  However, this access to the public is still very new and there is not a whole lot of trial and error to learn from in the open source machine learning community.  Therefore, we spent lots of time just getting these APIs to work on our machines.Things we learnedAnaconda: Python data science platformTensorFlow: open source machine learning frameworkKeras: high level neural network api, can run on top of tensorflowAWS Elastic Beanstalk: service for deploying and scaling web applicationsTorchLuaOpennmtWindows command promptPandas: high performance data structures and data analysis tools for PythonTools we usedAnaconda: Python data science platformTensorFlow: open source machine learning frameworkKeras: high level neural network api, can run on top of tensorflowAWS Elastic Beanstalk: service for deploying and scaling web applicationsGithub/git: version control systemPython3TorchLuaOpennmtFutureBreveo's next step is to train with images to gain more context on the video.  After that, breveo will move into training with multiple frames.  The goal is to be able to create the most concise and effective summary of a video possible through audio, visuals, and text.Built WithluaopennmtpythontensorflowTry it outwww.abreveo.com      Submitted to    HackCU Episode IVWinner                Most Devpost Likes                  Created by  Intern @ Walkthrough. Student at University of Colorado at Colorado Springs. Developer experienced in front-end, mobile, VR, and game dev.Elijah SalbergJake JohnsonUndergraduate Electrical and Computer Engineering Student.  Experience with code optimization and iOS development.  We reviewed the requirements for the project, as well as the existing paper application, and began to sketch out ideas for the electronic version. We put these ideas into a process diagram, in order to view the flow of the application. From this process diagram, we began building the electronic application. The application is presented to the user in HTML5, and utilizes jQuery to add interactivity and enhance the user experience. The back end is written in PHP. Data entered by the user are stored in PHP variables, which are written to a database when the application is submitted.   The front end of the extension is built using react. There are 2 main views for the application, the broadcaster's live dashboard and the panel portion of the channel page. On the dashboard the view will execute the polling of the broadcaster's chat and will submit collected chat lines every 2 minutes. The analysis of the chat is centralized to a lambda endpoint that is hit with a POST request. This endpoint will first verify the JWT of the caller and then break off into the 3 major analysis threads. The sentiment analysis is an API call to Amazon Comprehend with an array of chat lines with emotes redacted. Additionally, there are 2 node functions that will complete the analysis for profanity and repeated lines. Once all 3 have completed, the lambda will broadcast a pub/sub message to all connected clients. Once the clients receive the messages, they will update the UI to reflect the information. Another note is that the dashboard will persist the information for the session, meaning if a client comes into the stream late, they will still receive the most recent information, thus keeping all clients in sync. The analysis data points are also kept in DynamoDB for future analysis and trending tools for streamers.     The solution is built using native windows form technology as a base, overriding its controls to fit design & automation featuresThe Designer is Form containing an MDI form where the controls are added/remove on runtime for design purpose and have their properties (the ones I decided to expose) editable via a property grid. The designer has its own file system where the form can be exported as JSON (based on a custom se/deserialization approach).Once a form is designed, it exportable and loadable by the Designer.The form Controls all have a property "Expose as argument" which will make them appear as an output (Of System.Windows.Form.Control) to be used within the UiPath Automation workflow.Certain controls even allow to directly invoke a XAML workflow when a specific event triggered (ex: Button_Click,Form_Onload,Combobox_SelectionChanged) by providing it file path as a property.The invoked workflow needs to have specific arguments to be invoked, The Designer has a feature within its menu to create a suitable workflow.  The mobile application is written in Flutter. The backend is built with Kotlin, Spring Framework and Mongo database    Team Secrets implements end-to-end encryption, which means the secret is always encrypted and decrypted in the browser of the creator or viewers instead of the server.Since the local machine is doing the work, we needed fast and lightweight crypto libraries.  We chose the xsalsa20-poly1305" encryption scheme because it provides great security with fast processing and TweetNaCl because its a small, auditable high-security cryptographic library.Decryption requires BOTH a master key (stored as a property of a JIRA issue) and individual keys that live on Team Secrets servers.  Since we dont have access to your JIRA server, we can never combine the keys to decrypt your files.  Only you and authorized secret viewers can access both keys.          our UI is based on AtlasKit - issues at the end of May 2017.technologies used include React, and React templating, auth0 for authentication.for the JIRA plugin we used atlassian-connect-express.    It is iOS mobile app built using Swift technology as front end and Backend services built on Node js SDK and integrated with Weather Insights and Twitter Insights.  We used Python to build our app backend and CSS, HTML and JavaScript for our UI aspect    We used the ren'py visual novel game engine and sourced character and background art from the public domain (mostly DeviantArt). The storyline is inspired by the many anime and visual novels that we have played.  We followed Design thinking Process taking into account of all possible scenerios and filtering out those data and insights which will give maximum benefit to the user.Keeping the fact that how important it would be for a RPA bot or the team to send error notifications and periodical report to the stakeholders. we have tried to implement such functionalities with in the dashboard itself so that lot of manual hours will be saved.The dashboard was built using a dummy data set.Dashboard link(It is published on Tableau public, thus anyone could access it. For enterprise level, it would be published in Tableau Server and users can be monitored.)Accomplishments that we're proud ofThe fact that the dashboard provides lot of forecast which when taken in to account by support team will reduce their Turn Around Time to a large extent.Sending Email notification to stakeholders if the RPA logs found to have any error.Reducing lot of manual hours for report generation.Implementing Design Thinking Process.Developing a working dashboard with all the UI/UX implementations.Sucessfuly submitting what we aimed to complete.What's next for Savvy InsightsWe have future scope of developing a phython script which reads the Bot logs from folder real time and stores in DB so that the dashboard is updated automatically in real time.We hope to get lot of constructive feedbacks and suggestions from the user's so that the dashboard's functionalities can be enhanced to a greater level.Also would love to hear from different users and their stories of how our dashboard is helpful in steering their business decisions.Built WithpythonsqltableauTry it outgithub.compublic.tableau.comgithub.comwww.google.co.in      Submitted to    Power Up AutomationWinner                Educational Prize              Serverless Apps for Social Good    Created by  I developed the Tableau dashboards and the data pipeline idea based on the discussions with my team on how RPA worksSwetha Srivarna SelvaGanesanI was involved in all stages of the solution building from ideation, design thinking, solution building etc. Suhadev Venkatesh Ravindran  Our Chrome extension works by detecting an image on a web page using JQuery and then sending that image through the Microsoft Computer Vision API. The Microsoft Computer Vision API uses artificial intelligence to identifies and returns the key elements of the JPEG or PNG image in a JSON file. We then convert the JSON object to a string where we send it through the Bing Text to Speech API that returns and plays an audio file. This allows the user to hear the visual and emotional elements of a photograph. We implemented this in Javascript. Making the Chrome extension required us to also use HTML.  we built the app using Google app engine PHP we created rest API for the back end  and ember.js in the front end, with the power of google infrastructure the application is salable and reliable.       Places were fetched by the point of interest API. Tags, categories and sub categories were used in classifying points and then matched up to generate the daily itinerary. Several flights APIs were used to better plan travel which are listed below Flight Cheapest Date Search - To display cheapest upcoming round trip dates Flight Low-fare Search  - Display prices for selected dates Airport & City Search - Find user's current location and nearby airport Flight Most Booked Destinations - Show most booked destinations in fare and cheapest dates list Flight Mose Traveled destinations - Show most traveled destinations in flight lists Most busy travel period - Show most busy months in a particular destination Here Maps SDK was integrated for the maps part of today's itinerary generator and Yandex Translate API were used to provide local language support.    Used dlib as the deep learning framework for face detection and recognition, along with Flask for the web API and plain JS on the front end. The front end uses AJAX to communicate with the back end server. All requests are encrypted using SSL (self-signed for the hackathon).    We built it using Ionic, AngularJS in our front end and we're using Ruby on Rails on our backend.    The city of Montreal provides open data for all of its current murals, found in this JSON file: http://donnees.ville.montreal.qc.ca/dataset/murales/resource/d325352b-1c06-4c3a-bf5e-1e4c98e0636b Java was used to parse the JSON open data and create a list of Mural objects. The methods were called in Android Studio to obtain the list of murals and populate them on the google map using the Google Maps API. Clicking on a tag on the map opens up a page with the address and some additional information about each mural.    we used a software called Ionic, where we used html and css to create the layouts of the application, in the meantime, one member of our team, worken on the BackEnd side of the app, where he used ruby on rails. throught these 3 last days, we have been giving roles to everyone, be it the layout, connecctivity, testing, design, debugs and documentation of the app.  Use a laser cutting model that I found in Thingiverse, I started to assemble the electronic circuit, joining the senses and actuators to the Grove Shield installed in the Arduino 101, to create the program, use a new firmware for IoT, created by MIT For its App Inventor platform.  Built using Front end -> HTML/CSS/JS and AngularJSBackend -> NodeJSMessaging Platform -> WhispirHosting/Server -> IBM Bluemix  We used Node.JS with Koa2 and Handlebars. PDFs are rendered client side using Mozilla's PDF.js. Communication between server and clients is done using socket.io.  This Pup is built with:node.jsexpress.jsangularsocket.ioTrello APIWhat's next for Planning Poker for TrelloThere's a long list of features I plan to deliver, if I win in one of the nominations.Built Withangular.jsngroknode.jssocket.iotrellovagrant      Submitted to    Atlassian Codegeist 2017Winner                Best Trello Power-Up                  Created by  Sacrificed my social life to make this Pup happenVitalii Zurian  The app was built in 4 stages:Integrating the Google Awareness API and creating "fences" for emergency situations.Building the speech recognition engine using Pocketsphinx (a library by CMU).Making REST calls to PlaceILove endpoint to get safety predictions about a particular place.Using the pre-set emergency contact to send information like location, nearby places, weather conditions, etc.ChallengesAs the awareness API uses multiple sensors, synchronizing the result from their callbacks was tricky to handle.The most challenging task was to create the speech recognition engine which can listen to a user 24*7 and precisly predict the emergency word. This aspect of the app still has a lot of scope for improvement.What it doesThe app uses the feedback from all the mobile sensors like GPS, accelerometer, pressure, etc. to predict the environment of a user. The app also continuously listens to the user for an emergency code-word ("help me").Using the sensor data as input to Google Awareness API, it predicts the nearby places. Using this as input to PlaceILove API, it predicts the safety level of the place.If the app predicts a vulnerable situation based on all the aforementioned conditions, it starts sending pulse notifications to an emergency contact.Future workThe speech recognition engine has a lot of scope for improvement.We can also integrate run-time adaption in the app i.e. if the user does not want to send alert notifications for some of the contexts (like if it is common for the user to walk during night) then they may chose to deselect such notifications. This would evolve into the app having different behavior for different users.While driving we can detect the speed limit in a particular area and use that feedback to improve our predictions.Domain.com - Best Domain Name Registered with Domain.comWe registered the domain unagi-tech.com thanks to the generous people at Domain.comBuilt with :heart: at HackNC by Unagi-TechBuilt Withandroidandroid-studiogoogle-awareness-apijavapocketsphinx-texttospeechTry it outgithub.com      Submitted to    HackNC    Created by  I worked on making the app context aware by making use of different sensors in the phone.Arjun SharmaWorked on the ux, messaging service and the neighborhood safety apiShrey SanghaviDeveloped Front end functionalities and worked on continuous speech recognition  service. Contributed to design - look and feel of the application.Ronak Ghadiya    UnityLeap Controller Motion SensorTears, sweat, and Awake chocolate (#Awoke)  We used Node.js as a server-side application to access Google APIs. We then applied the Google Natural Language API and Cloud Speech-to-Text to gain insight into emotions and tones expressed through a candidate's voice.   We build our front end via React-Native and Google Cloud's various API and SDK's such as Maps and places.Our backend is built through Nodejs. Our backend is supported by Google Cloud's FireStore, Google Maps, and Google Places.  After a lot of hard work,I succeed in making this amazing game for our Circle.There was many problems that I faced but I did not lose hope and completed this project alone.The main software used in making this game is Unity ,And scripts are written in C#Facebook ProductsTo make this game Facebook friendly, I have integrated Facebook Ads in this game to make this game more interesting and enjoyable.  LAMP stack, Adobe XD   The severity of the onset is measured by means of the accelerometer of a android device and reports are saved to a folder in the gallery . The gravity co-efficient of variance is removed and raw reading is taken .  We started learning and reading on how to build power-ups. Once we got some idea, we started building project on Glitch. The entire process of building the power-up was surprisingly very simple because of Glitch and trello apis. The server is built using node.js.  We made a rain sensor to check for the presence of rain and built the opening and closing mechanism using cardboard and other everyday materials and servos.     I used context.io example heavily to connect to the api        We first created configuration files using JSON language to specify all kind of parameters:one as a resource in order to list all algorithms and parameters handled by the toolone for the dataset, in which you can specify the path of the Excel file, the sheet to use, which columns are corresponding to the features, and which one is corresponding to the target, the algorithm to use and its parametersThen, we made the Python script to preprocess the data (dataset creation, label encoding) and train the model using pandas, numpy and scikit-learn libraries.Next, we created another Python script to use a saved model in order to make predictions on new data.Finally, we designed a user-friendly web application allowing users to follow all the steps from selecting the data to evaluate the accuracy of the trained model. There are 6 different steps:Where is your data?Select your featuresSelect your targetSelect your algorithm and specify its parametersReview all your configuration and start the trainingEvaluate your model by its score, some plots, a confusion matrix and generate a template to make predictions with new dataEventually, we created two UiPath robots:a first one to interact with users through Custom Input activity displaying the different pages of the web applicationa second one based allowing users to make predictions with their trained models    The Add-On is built with the following stack:node.jsexpress.jsangular.jssocket.iomongodbmongoose  We used C# for Serverside Code and Actionscript 3 for client-side,Also we used text-to-speech and facebook api's.What's next for UnmuteCurrently Unmute works with our server and internet connection, in the next version, we want to make it work in background without internet connection, and when someone calls mute person, keyboard pops up, and it will work like a charm. I don't think what I said is clear, but we'll explain this better in our presentationBuilt Withactionscriptc#facebook-graph      Submitted to    HackTBILISI Fall 2015Winner                MLH Prize                   Created by  Clientside,Serverside codes & Application Design, Head of TeamGiorgi TskhovrebadzeProgrammerI worked on the project as a designer. I have already worked on projects like thise, so when we started working i already knew what was my job in the team. I tried to make social diseng and i also tried to make accents on the people who will use this programm. We did our best so i think  we will manage to make our wish come true, help people who realy need this programm.nika jebirashviliI was making a school project about mute people and when I was researching I found out that they were not able to talk on the phone, despite of todays technology. I was very suprised that there was nothing that could help them to talk on the phones.They were able to message, but It is very slow and most of the times you don't get full information by messaging. So I was thinking and I got an idea to make Unmute. I told my friends about this and we decided to make this project on hackTbilisi.Giorgi Asanidze  We designed it in such a way that all of it could be run on AWS with a complete serverless architecture. Since real time data from the trucks was essential for the demo, we built a data generator that simulated a fleet of trucks each with a serial number and initialized them with mileage. To reduce costs of course we only put one truck per customer and had two customers/fleets and initialized the customer data.We  simulated a set of sensors providing the tread depth at sub-millimeter resolution that is inbuilt into every tire. Each tire on the truck has a unique identifier that allowed us to track the mileage, tread wear at the tire level. The tires also had a position indicator to indicate where the tire is installed on the truck (steer wheels need to have more tread than the rear wheels).Accrued Mileage was collected from the truck every second. We used Kinesis Analytics to calculate various features such as average velocity, acceleration, deceleration, braking events, rate of tread wear using rolling and tumbling windows.    We collect data from multiple sources such as FourSquare, Google Places, and we leverage the data we received to plan a trip based on various factors using our AI.      Context.IO APIAccountsAccount creation and deletion are included in the application, re-enabling sources that are disabled has also been added.SourcesAdding and removing sources via the source discovery api and managed UI process. Initial sync callbacks are used to trigger actions within the application.MessagesMessage body and metadata retrieval for user and contacts. Included message_body and additional folder information.  Used the to, from, subject, date before and date after to further sort relevant messages and contacts. Used message information to get from a users contact. WebhooksWebhooks are used to import new mail messages into the application and perform analysis without the need for user interaction.Watson APIWe clean the message body to ensure that only pertinent text are used  for further analysis. Cleaned messages are sent to Watsons Tone API so get the Tone Analysis per email.  All the messages sent by the user and all the messages sent by a users selected contact to the user are sent to Watsons Personality API to get the Big Personality Model for the user and his selected contacts.         We used meteor to enable real-time collaboration and a Javascript library called Wad that is a wrapper around HTML5 WebAudio API.  We focused on there important aspects:Beautiful Design and Easy Fun to Use UIPOLITE UI - since we are dealing with delicate topic of sizeNative iOS ApplicationNode.js / MongoDB backend powering and scaling the App and the AlgorithmOpen API for others to use our AlgorithmPitch WebsiteWe used:SketchZeplinBearSlackRadix TLDNO Alcohol =)  We used the React starter from Hootsuite. Then we rolled in a Node.js, Express, and Socket.io backend. The backend just statically serves the React app and exposes websocket endpoints. The application runs entirely over websockets, so all communication is real-time, and audience member screens will instantly sync changes.        Using Google Translate API associated with API.IA and a OCR api as the core mainframe. We can add any others apis to expand our solution.  We built Fireflies with Alohar as a native Android app from scratch for the very first time this August. We aimed to leapfrog off of other existing solutions, and accordingly we integrated several APIs and material design strategies to really vibe with our users. Additionally, we tinkered with delivery ideas in the past and created mockups and surveys to understand user behavior. However we never were able to build a comprehensive solution till now. Until we found Alohar, there was no good way to efficiently track our users and develop a context specific algorithm to enable delivery. We are excited to release this app, complete with designs based off actual user feedback, Alohar integration, and payment systems.  This service is built using Amazon Alexa skill with node.js as Lambda function. What's next for Voice Job SearchCurrently I'm only search for jobs using Github Jobs APIs. later I can add more job websites such as LinkedIn and IndeedAlso We can save search criteria to be used later without need to enter it everytimeBuilt Withamazon-alexagithub-jobsnode.js      Submitted to    Amazon Alexa Skills Challenge    Created by  Mohamed Hassan Abdulrahman  For detection of water level we used ultrasonic sensor, which gives the values in terms of voltage which was then calibrated in centi-meters. Arduino 101 collects data of this sensor and send it to Arduino UNO for further working. Then we used  Arduino UNO which collects data from methane gas sensor in voltage terms which we converted in terms of PPM. Arduino Uno analyses the values of both the sensors , if the values goes above the decided level it sends this information to municipality through GSM SIM800, it also pushes the data to thingspeak cloud through GSM.//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  Using the HipChat integrations API, Heroku API and a standard rails stack on Heroku  Magic and Fairy Dust!Kidding. The LEDs are individually programmable Neopixels (by Adafruit), and what we've done to dressify them is map them into different regions on the body-- so we control bust, waist, and sections of the skirt. We can customize per princess following that, but we assign colors to each section once the princess has been identified.To parse the song lyrics, we use an API called DeepGram.    Bitbucket REST API, 3D models visualization with http://threejs.org/, Play! 2, javascript, java, scala, View and Data API, Autodesk A360 Viewer APIChallenges and AccomplishmentsLearnt about 3d modelling and visualization in browser, worked on integration with Autodesk  360 APIWhat's next for File Viewer for Bitbucket CloudWe are looking forward to feedback and are considering ways of further add-on development at the moment.Try it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist: Add-on HackathonWinner                Best Bitbucket add-on                  Created by  Katerina KolinaAlexander KuznetsovDmitry Zagorovsky  We made a RESTful system where the app sent requests to our server where the questions and explanations are stored. This way a central database of questions and explanations can be maintained on our serverwithout much hassle.The app (i.e. the user end) is made in Android. While the backbone of the app is made in PHP using MySQL as the database solution.  We used Atlassian Connect and leveraged a lot of new features from ES2015. ReactJS takes care of rendering everything smoothly and everything is deployed to AWS.  We built this project using the Watson API, Node.js for the back-end and Twilio.  We found tweets with keywords indicating that the user was depressed - the individuals who published these tweets were potentially depressed. But the depressed tweets may be a one time event and might not prove that the individual is depressed. So, we looked at the latest 200 tweets of these users and found the possibility of those tweets being negative. If majority of the tweets were found to be depressing, the probability of the individual suffering from a mental condition would be high. Those tweets were further analysed individually to identify if they were related to relationship, money or education. We aggregated all the negative tweets for each of the categories for a specific user. This helped us identify the major cause of their depression.        At the sensor level, GPS sensor(GY-GPS6NV2), Infrared Temp sensor, Soil moisture Sensor(YL-38), Humidity Sensor (HL-01), pH meter , Focusrite Scarlett 2i2, Samson C01, Acoustica Mixcraft , Image-Line FL Studio, DC flow meter        I started out building it as a traditional Plugin SDK add-on, because as a result of data protection laws we have to host all of our Atlassian software on private servers at my company. When the product was more or less finished, we decided that it would be great to also support cloud instances and therefore developed an Atlassian Connect version with a Symfony backend.I used Hakim El Hattab's great reveal.js for displaying the presentation and jQuery to parse the Confluence page's DOM into a great looking presentation.      From the ground up. We developed this app completely from scratch with the help of Openshift, nodejs, and web technologies such as HTML, CSS, and Javascript.    The framework of this app is from the github repository android-sdk-googlevr. Code that relates to setting up the VR environment, initiating sensors on a Daydream ready phone, and enabling the controller handler events are all from Google Inc. Additional elements added such as extended color palettes, different brush types (opacity, depth, size), erasing, and saving are created by Varnika Sinha, Sarah Dogar, and Cameron Harvey.Android Studio was the app development environment we chose to work in. We coded the app in Java and C++.  I use some of services in AWS such as: Amazon-Lex, Lambda function, DynamoDB, API gateway, KMS,  S3 Bucket, and CloudFront. Logibot is deployed on Slack as a APP.DynamoDB is used for store session Lex integration, team installed credential, user profile, and job details. A session will be created for different team, channel, and intent name. KMS is used for encrypted token Slack APP, Google API key, and authorization Zendesk API. Lambda function is used for handle event API from slack, validation code hook, fulfilled amazon Lex, and connect to others APIs.    Using the Swift and Xcode skills I learned at Make School Summer Academy, I built Touch for my final project. From there, after several revisions and rewrites, the most recent version of Touch emerged.  We have used IBM Bluemix for our application.Then we used object storage for putting the files in the container.We uploaded the grocery and gourmet json file.We start with ipython notebook upload the data source,We used to data sets one is the review data and other is the meta data.We then set a hadoop config followed by using pyspark.We do analysis on tea products and then merge the data together.We aggregated the data and gave a complete overview.   First of all, we set sensors along the tunnel. The sensor can be IR, light sensor, radar, supersonic or others which can detect the passing vehicles. Second, we set LED array on the road. Lastly, a host computer calculates the car speed depend on the sensor signals and drives the corresponding LED array. The LED array can be a vision symbol, indicating the drivers respectively and reminding them of making some adjustments if they need.The LED indication will move with vehicles and change types depend on the driving condition: Green arrow - You are moving at the best speed (90~100km/h) with an appropriate safe distance (E.G.30~50m) range. Twinkle Green arrow  The speed is too slow, please speed up. Red arrow  The speed is too high, or safe distance is too short, please slow down.PS: the ideal speed and safe distance range can be modified by control center.(/IR///...)LEDPLCLEDLEDLED:(90~100km/h)(30~50m)PS:  AppInventor,MySql,PhpMyAdmin,Matlab  The infrastructure's information are being gathered through a Particle Electron board running of cellular network. The data are then sent to an Amazon's Web Services server. Finally, a Cisco Spark chat bot retrieves the data and outputs relevant queries according to the user's inputs. The intelligent bot is also capable of warning the user in case of an emergency.    I used Sketch to design the prototype and then used Xcode + Swift to execute. And used firebase for the database to save the users.  This Outlook module is built with React and Node.js. It makes use of the Microsoft Graph to work with the Planner tasks API and SharePoint Online.  On the transmission end, we used standard hardware to emulate a wall-gapped-system, potentially inside a secure facility. We wrote some low-profile code in C which hits the data bus at its maximumum frequency - on this computer, approximately 513Mhz [that's 513 million 128-bit transactions per second]. The code has a very low memory footprint [as it writes to the same location continually], and also has a very low CPU footprint - as the memory frequency is much lower than the CPU frequency.On the receiving end, we use a python script to process the RF data from the bladeRF. During the course of the hackathon we developed our own transmission protocol, and our own algorithms for automatic clock resynchronisation and noise rejection. In order to reject noise, we did have to maximise the transmission period, such that our data rate is approximately 2 bits per second. However, for an example use case of transmitting a 512 bit secure key from a remote air-gapped computer, this only takes 4 minutes to transmit, which we consider reasonable.  A transaction handler checks all modifications to relationships in each transaction before commit.For each node with more relationships than defined threshold it creates an explicit index where it indexes all relationships of that node.The index is local to that node.The lookup procedure then checks if a node local index exists, and if it does it uses the index to get the relationships,otherwise it iterates over all the neighbours.  Used IBM Data Science Experience for data analysis, model building...Built Dockerised Play! stateless web service exposing REST interfaceTested portability by running both in IBM Containers Service and Docker CloudTested performance and stability by running Gatling testsUsed Swagger to allow for interactive exploration and to document the REST API  Built using:-      1. Android Studio      2. Whispir API      3. Java      4. XML  We used a Raspberry Pi as a bluetooth receiver and wifi hotspot. Our app communicate with the RPi through Bluetooth serial communication. Anytime a user enter the bluetooth receiver range, the app pairs with the receiver and sends an ID (here a phone number) via serial communication. The receiver (here the RPi) then checks the online database (IBM Bluemix Cloudant API) that this ID is registered in the database (IBM Bluemix Cloudant) and then sends back the hotspot SSID and the WPA passphrase to the app. The app then initiates a connection to the WiFi with the acquired credentials. It all goes on in the background without user interaction. Except when the user opens the app during the event, he needs to accept Bluetooth activation first.    I built a C# program using HP OTA framework to co-ordinate tasks related to HP ALM ,developed a Bamboo plugin to utilise the C# program to drive the test execution and take care of Bamboo and JIRA integration.  A new task "ALM Task" appears in Bamboo task list.What's next for ALM TaskThis is a first step, a lot more ideas and requirements will flow in as teams starts using it.Integration of ALM Task with Hipchat is the next feature coming soon for ALM TaskBuilt Withatlassian-sdkc#javaTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist: Add-on Hackathon    Created by  Vikas BorseTest Automation Consultant  The application was built in Objective-C for iOS devices. We leveraged iOSs NotificationCenter, AudioToolbox, AVFoundation, CoreLocation, and CoreMotion frameworks to implement all the features for this application. We integrated Plaids API for payment processing.  We forked JMeter on GitHub and created the components for bolt. We built example load tests scripts to validate it was working fine.    Once we had a general idea of the solution to the problem we were trying to solve, we split-up our team into front-end and back-end sub-teams. We then established a list of required goals, as well as a list of reach goals that we worked on for the rest of the weekend. The result was a native IOS app made with Swift on XCode, utilizing Microsoft Azure's Custom Vision API for computer vision functionalities.  We built a contract called dDAI which is a token 1:1 backed by DAI and lends it out on Fulcrum. When the interest is claimed it gets send to the DeFi recipes the user set up. These convert the interest in DAI to the preferred assets.The UI is build in react.We integrated the following tech partners:FulcrumUsed for lending out DAI and in the PTokens are used in some recipes to open up leveraged positions.KyberExtensively used in multiple recipes to swap dai to other assets.SynthetixUsed in multiple recipes to convert dai to synthetic assets.        We started by observing and listing down the problems faced by pedestrians and cyclists. Lack of pedestrian and cycle lanes is one of the biggest issues. We started to build this application with the aim of solving this problem and the problem of pedestrian road accidents.We looked at various existing solutions and found that a very simple solution already exists. It was called Look Up. In our application we added that functionality and various other features.To gather pedestrian and cyclists data, we thought of using bluetooth beacon for higher accuracy. The authorities can also easily program the beacons to send awareness messages to all the pedestrians and cyclists.  Les donnes utilises, celle de l'emplacement des casernes et des feux de circulation sont fournies par la ville de Montral. Le programme a t implment avec python puis avec JavaScript afin d'utiliser une interface graphique issue de Google-Maps.     Using Uipath and standard windows function like mstsc.  There are 3 main components:Contracts : Written by solidity, deployed by truffle frameworkFrontend : Reactjs framework, game framework with createjsSwap module: Built with Kyberswap API    I started developing it using PHP and then did a deep insight study of the API usage in GITHUB.Then I registered for a key for Socrata ,511.org,Kontakt.io.For Beacon data I have used the api key mentioned in the resources tab.First of all for getting results of the API I used Ubuntu and terminal to use the Curl command and get the data.The Structure was found and I tabulated it to give it more  visibility and appeal.I use my PHP skills to fetch data from the APIs and then produced the result.    I built the application all by myself, I came about the design concept for the application and also developed the application myself  A static landing page (pwa too) made with Gatsby.org (A react static site generator)that takes you to the actual web app (A react progressive web app) that sends requests and receives responses from the rest api built with express js and mongoose. We're using mongodb as our database and the Google maps api.    I build it using Ruby on Rails during a two week Agile Sprint. I took advantage of both the Hootsuite APIs and Atlassian Connect.  Built from scratch on Python with a Tkinter module, ZOI integrates machine learning to provide coherent information and recommendations to users.  We used Ruby on Rails framework with React.js, Node.js and Stylus.    We built the app using MIT app inventor. This was minor challenge we faced during development of the app was learning the block language that we were unfamiliar to. To learn the language quickly, we spent some time looking through tutorials and creating the sample apps to learn the functions of the app inventor and the basics of how to code. Once we knew these, we began to program our app and used the guess and check method to overcome other programming hurdles.Most of out functions were programmed through complex decision structures that we created through the guess and check method and repetitive testing. An epitome was programming the sort function to work in all cases despite similarities in priority and expected time. We did this by adding multiple tasks with the same and different priority levels and programming the app to repeat the sort method between every 2 consecutive tasks until it is fully organized. The program uses one of our several algorithms, sort using an if...then case structure with variables programmed to change values for every run based on the given index.    We used an Arduino 101 (Genuino 101 here in Europe) as the core brain of Purrpurino, and different sensors of the Grove Starter Kit. Using the Arduino 101 was crucial because it made it easy for us to create an app to communicate an Android app with the board, using the board integrated Bluetooth.The Grove Starter Kit Plus sensor set was a perfect fit as it had the main components that we needed to make Purrpurino come alive:  Light Sensor : To control if it is dark and make Purrpurino go to sleep. 3 Axis Sensor : To know if Purrpurino is getting dizzy or if he is playing. Piezo Vibration Sensor : To track if someone is petting Purrpurino.  Stepper Motor : To make the purr effect. And from "standard" components not from the grove sensor kit we used : Push button : as a way to switch on / off lantern mode. ws2812b 5050 RGB Leds :  to display the emotions and states of Purrpurino through led lights. Instructions of usePurrpurino starts in a shy state where he gets to know his new owner. His cheeks are blushed in this state and he needs to be petted to get used to the new owner. After some caresses, the light of Purrpurino will turn green while purring delightfully as a cat, as that indicates the happy state. Now the functions of Purrpurino can be showed.The new owner can play with Purrpurino, shaking Purrpurino gently to have a good time, but if he gets shaken too much, he will become dizzy. Some petting will fix his dizziness and he will be happy again.A parent or guardian can modify the mood status of Purrpurino through an app designed to do so. This way a parent can reinforce positive behaviours in their children or make them realise of a bad behaviour and teach them not to do so.If Purrpurino get angry, his owner can caress him to make the rage go away.Purrpurino can go to sleep when no light is detected, purring softly for a set of time that will allow the kid to sleep while feeling protected.Purrpurino has a button in his right paw to enable "lantern mode" so no matter if he is sleeping you can use it to brighten the room.  We use Amazon AWS as our cloud server, building our model using CNN(Convolutional neural network).  We carried out research on different methods of conducting CPR and broke it down into 3 different groups: adults, children and infants. We found current applications that exist were very isolated, dry and outdated. We consulted an industry expert,   Trauma Nurse, and Police Officers. We received feedback from them about how vital an application like ours could be for them in the line of duty and how we could further improve Save A Life. We worked with talented developers to create a unique platform to connect individuals through healthcare - you will always remember the person who saved your life, or a family members life. We wanted to make it easily accessible in order to reach a wider audience, especially people in underserved communities and in developing countries. People in these regions may have limited access to electronics, and might only own a cellphone. That is why we have decided to develop our application as both a mobile friendly web app and a messenger chat bot. In future versions we plan to make an offline version for more remote areas.     For the LenderZap (see here), we have 3 Smart Contracts that deploy the investment received from the user into cDai and dLETH2x.  The primary reason to write 3 different Smart Contracts is to keep all the Smart Contract code modularised, keeping security as the prime goal.  The first contract with which our front-end interacts (see here) accepts ETH does the necessary allocation of ETH and sends it to the two(2) Sub Smart Contracts that make the investments.  Both our Sub Smart Contracts (see here and here) have a function which then makes the investment.  The Key Points to be noted over here are:-The investment functions take the address of the investor from the Main Smart Contract as the Address to which the assets are to be bought and sold;-For ETH to WETH to DAI, we use the Kyber Networks Proxy Contract using the swapEtherToToken function; and-For the investment in Fulcrum, we talk to the Fulcrum Interface and interact with the mintWithEther function.-Keeping Security in mind, our Main Smart Contract and the two Sub Smart Contracts have a fall back function which also makes the investment once an ETH is sent to their address.-Since both the Sub Smart Contracts do not do any allocation of ETH they are modular plug and play and can be used behind any smart contract which does a different allocation.Keeping Security in mind, our Main Smart Contract also has security toggles that can stop the function of making any investments.  Lua and brainpower.  It's powered by Unity and Vuforia. We have built a specific application for the Vuzix AR device, enabling them to experience the world handsfree!We have also built an AR application compatible with both iOS and Android devices. This application leverages the depth field of view in advanced cameras, rendering 3D text next to the speaker. This has been achieved through real-time image processing to help locate the anchor points for the text.Real-Time speech-to-text has been enabled by utilizing IBM Watson services and using the device's microphone to perform spectral analysis on the incoming audio.      We thought that adding machine learning to signature verification would be a good idea so we thought of how we could demo the product.  This lead to creating a couple of html templates and some javascript functions.  The app was built using Java programming and Android-Studio.  We decided with Meteor's reactiveness, we could re-create the famous mobile game Don't Tap White Tile, but with a little bit of twist by adding Multiplayer Features.    We have used Ethereum blockchain and integrated the Interface using truffle , web3js and react. The entire app is a DApp and contains implementations of the blockchain token transfer and device authentications. The framework for react is NextJs for making the app essentially a node app. We used Web3js for interaction with the blockchain and invoking transaction. Metamask has been used as a browser extension for signing transaction.       Uploading an image to a Flask application that pre-processes the image, sends it to Google Vision API to analyze face geometry and emotion, then locally manipulates the image using PIL before saving the images and redirecting the user.      The microcontroller we used for this project is the NodeMCU board which is an Internet of Things enabled board built on top of the Arduino framework. We then attached several sensors to the board (DHT11 Temperature and Humidity Sensor, CCS811 Air Quality Sensor, BME280 Atmospheric and Pressure Sensor, and VEML6070 UV Index Sensor) following the I2C (Inter-Integrated Circuit) protocol where we used a multi-master/multi-slave architecture to allow sensors to publish and receive data to the bus system. When the data is received by the NodeMCU microcontroller, the data is sent over WiFi to a real-time Google Firebase database where it is categorized by sensor and type of data. This information is then retrieved and displayed on a web app user interface updated in real-time. The web app is built using the Vue.js framework to process the real-time data. Other areas of the web app such as the front-end design are built using the HTML5, CSS3, and Javascript. Additional features were implemented using Bootstrap, and ApexGraphs.   Hack hack hack!!We based the project on Telescope, which provides a basic CMS, commenting and upvote system, and user account setup out of the box, as well as numerous extensions for community building, such as newsletters.We hacked Telescope "posts" and turned them into "product features". We added the concept of 2x2 planning grid and release management as our primary value adds.  we four friends are always keen in discussing such stuff with the teachers. when we came to know about KPApps challenge, this was the time for us to take advantage of it, it took weeks get the idea. When we got the idea there were many problems ahead but with the help of teachers and our determination we were able to finalize a tool that could model our idea as an application  The application uses 2 different dictionaries to first detect the category of the user input, then using the second dictionary, the application responds with the most appropriate reply. The application uses the Nuance Mix API to turn the vocal input into a text format that our code (done mostly in python) can process. An extensive list covering the most common input terms was made such that inputs could be fit into one of eight categories: Social life, family, relationships, school, political, health, technology, and self. The input terms are weighted (done through testing with example sentences) so that if two terms conflict in terms of categories they direct to, terms with higher weighting will take priority. Based on which category the input falls into, the output will be generated by searching for keywords in the input.       We worked with DocuSign's REST API and Business Development team to develop a full-service solution to embed the signing UX for an on-page document sending experience.  I, Sean Connolly, a Computer Science major from Northeastern University, and my friend from high school, Conor Boston, an Aerospace Engineering major from Pennsylvania State University teamed up to take on this project. This is our first time working with an Arduino (definitely not our last), there was a hefty learning curve for both of us.Conor handled the electronics and hardware components of the project: Arduino 101, Uno, Motorshield, Ultrasonic Sensor, Solar Panel and external power supplies. Conor has had experience working with robots in the past, this is why I recruited him to the Project.I worked on building the software for the Android application and both of the Arduinos. I had built android apps before this project, but mostly simple tic tac toe games or soundboard apps. So an augmented reality game was significantly more challenging, and rewarding. Also the allure of learning a thing or two about hardware was very appealing.  The server runs on node.js on AWS Lambda. I don't tend to use many frameworks or libraries so most of it is hand-written JavaScript. I make full use of the facilities provided by the Power-Up API, such as  data storage at the board and team level. I store other data on DynamoDB.  Thunkfluence is a pure Chrome extension. There's no server or back-end so all the data stays on your computer. This design is naturally secure, an innovative alternative to other third-party enterprise search engines.Thanks to the Confluence REST API, we didn't need a Confluence server add-on either, so it works out of the box for both Cloud and Server without your Confluence administrator doing anything.    We built a Google chrome extension using JavaScript (JQuery), HTML, and CSS. We also used regular expressions to detect and replace profanities on webpages. The UI was developed with Sketch.   Using Uipath studio and Excel activities.  Integrating multiple platforms through IoT        i built this application with : ionic, apache cordova,Angular JS,HTML5,CSS3,JAVASCRIPT,Jquery...PHP5   The robot chassis would be made with 3D printers, of a biocompatible and non magnetic material. It would also include microcontrollers and DC motors. For the physical interaction with patients and familiars, some cameras are included to recognize faces and emotions to give personalized answers. To give realism to the robot-face, light projection technology will be used (from the inside the head). Finally the robot would include a motion system based on marks recognition along the hospital.      We installed:PHP 5.5+CURL extensionMCrypt extensionCreated php projectCreated sample application in Ringcentral developer portalStarted the PHP built-in Web Server locallySetup Webhook URL for the BotStarted botOn-Boarded the Bot into GlipWe are done, started chatting with the Bot. How cool is that !!  We broke down the main tasks and each one of us chose the ones that they preferred to work on. We kept a Spreadsheet with all tasks and bug to track. We set up a BitBucket repository to be able to cooperate and speed up our work.Assets usedhttps://www.assetstore.unity3d.com/en/#!/content/44205https://www.assetstore.unity3d.com/en/#!/content/37457all sounds are from https://www.freesound.org/music track is from http://www.newgrounds.com/some particle textures were done beforehand by Sini (our team member)https://fonts.google.com/specimen/Satisfyhttps://fonts.google.com/specimen/Armata    Swordfish Essential is written in Python using Django and the Django REST Framework.Our backend stack consists of technologies including Redis, RabbitMQ and a PostgreSQL database and we integrate with third-party services to produce a product like no other.    We use Eclipse to create a web-prototype to show how the app will look like. The languages used are briefly javascript, css, php and html. We also use it with the database such as phpmyadmin and mysqli. The photos are edited with Adobe Photoshop.  We built the prototype with InVision. It will works only with graphics, but it WORKS!  we built it  on the screendy plateform and android studio  Native Android to access the functionality of the phone.Using Google Places for a list of locations (point of interest) based on the location of the user.    Basically, everything except of the external travel service runs inside of the IBM Bluemix. Check this blog for details.  I built this app in my free time over the last school quarter using Swift 3 and Firebase. I had barely any experience with iOS app development in the past (I have one game on the app store), but thanks to A LOT of googling and the wonderful help of the people on Stack Overflow, I slowly chipped away at each problem that arose.  The bot is a ciscospark bot that is hosted in heroku.com. The user's message is intercepted by the bot and sent to wit.ai in order to retrieve answer or to begin a dialog. In the wit.ai side, we have organized some questions and answer related to pregnancy and we train it in order to understand what intent and entities have to be extracted.  As user experience professionals, we did extensive benchmarking research with the current paper application, user interviews, user testing, a content audit, and information architecture audit. We iterated through paper prototypes, hi-fidelity mockups, and ultimately our coded solution to increase the usability of our product. The app is built with HTML, CSS, JS, and PHP on Bootstrap and hosted on Github.       We built it using C++.    Python 3Microsoft's Cognitive Services (We used their Computer Vision API to extract the text from the image)Google Translate API  There are 2 main components:Contracts : Written by solidity, deployed by truffle frameworkFrontend : Reactjs framework, game framework with createjs, Signin via Torus    A Pinch of IBM-Watson and a Dash of Amazon AWS & Lambda.    We've used Google's ARcore library combined with Kotlin to develop this android application that communicates with a PHP server for phone images and specifications.  The app will request a list of phones that match the user input and put them in autocomplete form for the user to select, the server then will call Fono API to get all the specifications for the phone and scrape Emag for pictures for the phone. When the app has those, it will generate a 3D model from the received images and display the phone alongside it's specifications.   Using the OpenPose library to extract the skeleton of the body of a person.Tracking right and left hands over frames.Using signal processing to detect events (position of the hands, note on, note off)Converting the events into MIDI signals.Connect it to a virtual intstrumentLet's play music !  Rapidly prototyped using RealityScript, a language I designed for hacks like this. - AR via AReality3D SLAMCross-platform compile via Unity 5.2.1f03mobile-optimized realtime Instagram effects using LUT hackWhy did I build this?Ive always wanted to take my avatar out with me everywhere as my pocket pet - and take Instagram photos (and videos)! This is one of few AR apps Ive built that I find myself totally obsessed with! <333No, seriously, how much cooler and more narcissistic can you get! XD!! Disclaimer: Im a former Second Life millionaire (and created a fashion empire and became a philanthropist, etc. Re: Years in Narnia)What's next for IMVU PhotoRealIts pretty polished alreadyI plan to add in-app payments and deploy to the App Store. :) Maybe this will make me an App Store millionaire ;) Known Bugs with current buildMixamo Unity animations + IMVU avatar leads to some odd twisted body parts - left knee etc Currently only tested with iPhone 6S etc.Check out my IMVU test feed: http://fw.to/cWI7YyLCheck out a clip of my avatar's piano recital in AR! https://youtu.be/3oN5unTKOf8 Built Withareality3dimvumixamoopencvopengl-es-3realityscriptunityTry it outbetas.to      Submitted to    Hack With MeWinner                First Place                  Created by  Yosun Changaugmented reality hacker-entrepreneur   We built the text-based game using Python in Trinkets.io, Visual Studio Code, and Brackets. After completing the game, we created a website to host the game using HTML5 and CSS3 in Brackets. The website includes introductory information about Millennial Money Madness, the team that created it, the game itself, and the resources we created to use in the game. That way, users can get all of the financial wellness information without needing to play the game. Additionally, if someone is confused about a specific topic after playing the game, they can go to the resources page to get more information about it.   I am using MeteorJS and mongoDB for databases. Using semantic-ui to give it a nice look and some fast rendering. Using google recaptcha for authentications where it does not need to sign in. Created the website to look and feel simple as possible which makes users happy.      GoforCar team provides you the easiest way to rent a car from Taipei to Yilan. There are hundreds of eCar stations in Yilan County so that you can get on a eCar in 5 minutes with the Reserve eCar function. We use QR code to rent, verify and return the eCar to simplify the whole process. By using Carpool function, you can share your eCar with others to save your rental. Besides, to avoid high-occupancy vehicle lane policy in peak hour, the Carpool is another alternative for drivers to choose.  CommitCoin consists of three parts: the blockchain to keep and sync records, the chrome extension for mining Monero and for the WebRTC blockchain, and the online dashboard to track important statistics.  Hardware - Rasberry-pi , TFT screen, Webcam (for its mic) and an external speakerSoftware - Python, Mapquest Geocoding API, Mapquest search API, Google Places API, Amazon web services (to send text sms to the contact) and Google voice API (both TTS and STT)    We used Amazon developer console as IDE, lambda function, Intents.Challenges I facedImplementing the lambda function.What I learned Lambda function Alexa app building skillsFuture ScopeCurrently we have limited rivers in our game we will include more rivers in our app.Testing InstructionsPlease use below provided data for Testing:Sample data : Say Alexa start World Rivers  or Alexa start Rivers of the Earth. It will ask Do you want to explore or play quiz?.  If you say explore it will say you the rivers of the world in descending order of their length along with their length and location. If you say quiz it will ask you a rivers location, its length, location, etc.  Built Withamazon-alexanode.js      Submitted to    Alexa Skills Challenge: Kids    Created by  Amit DandawatePlease like V-insurance and FB-surance by following project links below! Thanks a bunch! appramanik  We used the available JIRA SDK and specific IDEs for each mobile platform.      This project made extensive use of the WolframAPI and Mathematica, mainly to convert signals from the violin to midi files. We also use Mathematica to output the processed notes as various instruments in our improvisational pipeline. Finally, our Neural Pipeline took advantage of the variety and abundance of built-in instruments within Mathematica to generate distinct sound profiles for our orchestral compositions with the RNN.The Neural module was built in tensor-flow, making extensive use of the RNN modules that were available then. The char-rnn architecture used to compose some of the songs takes into input textualized MIDI files and processes them through 2 hidden layers. After the training process, the network is able to generate textualized MIDI data, which then can be converted into actual music through the Wolfram API.  The biaxial RNN consisted of two vertical stacks of LSTM-RNN nodes who were interconnected, the note and time networks, each with two hidden layers. The input would consist of a vector consisting a vectors MIDI note alongside some information on the notes vicinity and the history of notes played. The input would then be passed through a vanilla recurrent time axis, in order to learn some structure. Then, the output of the time network would be passed to the note axis, who makes recurrent connections alongside other Note nodes. This ensures that the network is able to learn connections between different notes and their relationship to each other throughout multiple musical piecesThe Improvisational pipeline was powered by digital signal processing. The fundamental technique in digital signal processing is the Fast Fourier Transform, a O(n log n) algorithm that is incredibly versatile in its scope and application. In our project, it allowed us to filter different notes just through their frequencies. Since FFTs are just something that the three Computer Science majors in the group had studied in theory until that point, it was incredible to see its power when implementing it to build our application.  The project consists of several parts:HardwareA Playmobil toy trainA Raspberry Pi 2An H-bridge to control the motor of the trainA 12V NiMH battery packA step-down voltage regulator to provide the 5V for Raspberry PiA WLAN adapter for the RaspberrySoftwareMeteor running on the Raspberrynode-wiringpi to access the GPIO pins from Node.jsA toy railroad engine from Playmobil was taken to start from. It provides the chassis and the motor.One team member created all the hardware stuff, soldered the H-bridge as well as other interfaces to the existing railroad engine. He had to drill holes in the plastic chassis for the cables and provide tons of experience to solve the voltage issues.In the meantime, a second team member took care of the Meteor application, which provides the user interface as well as a logic to allow only one engineer at the same time.The third team member was responsible for installing Meteor and the application on the Raspberry Pi hardware. He implemented the engine control, the control of the lights and parts of the UI.  We wrote 3 python scripts and used an IMAP library to create a running Python daemon to listen for bash commands.    We used the Amazon Echo - a high-tech fusion between furniture and AI - the new and improved Siri. Alexa runs on AWS using special Lambda functions to execute discrete tasks. We capture the user's voice input and parse it into consumable JSON objects that the game engine we wrote parses.   C# code captures the screen and does initial image processing. The data is passed through to a python script that passes the data through to Clarifai. The python script also does the programmatic clicking based on Clarifai's return value(s).    I used the HTM.java port of NuPIC's core algorithms to learn sequences and produce new ones. The frontend is handled in the browser in JavaScript and the two parts communicate over a REST API handled by Spring.    Meteor all the way!  The project is divided in 3 different implementations that work together: Hardware - I made an early prototype while waiting for the Artik board using an Arduino Uno, a neopixel led and a flow meter sensor. After receiving the Artik board, I moved everything to work there, using the advanced capabilities of the board that allowed me to connect with the Android App. The basic operation of the hardware here is to display different colors when the threshold set by the user is surpassed, and connect with the android app to update the data being collected. Android app - The android app uses a service that checks if one of the paired devices is on range, and if it is it connects to it, telling the hardware to start counting water, and updating it's threshold if needed. The app updates all the data collected and sends it to the server, among with the owner id, and this information gets saved in the web server. Website - The website is used to visualize all the data of the different devices installed. The user can set a range of dates to get the different data gathered between two days and can check the use of a device in comparison with the other users of the installation.   Using AI technologies and FOSSASIA 'stools.We take inputs from the user using susi.ai and make our database of the languages.  I built Slope Ninja using React and React Native, node as well as other open sources libraries.    The application consists of a react-native app that can be installed on both iOS and Android devices. The app communicates with our servers, running a complex neural network model, which analyzes the user's pupils. It is then passed through a computer vision algorithm written in opencv. Next, there is a reaction game built in react-native to test reflexes. A speech analysis tool that uses the Google Cloud Platform's API is also used. Finally, there is a questionnaire regarding the symptoms of a user's concussion.     The add-on was built with the 3REE stack - React, Redux, RethinkDB and Express. The reporting engine runs completely on the client-side, and uses React for the views and Redux for managing application state. The client fetches report data directly from JIRA using Atlassian Connect's AP library.To ensure that the client complied with Atlassian Design Guidelines, I ported a good chunk of the Atlassian UI library to reusable React components. I hope to further polish this library and release it open-source it in the future for the benefit of other add-on developers.The client needs to talk to the add-on backend in order to persist report configuration metadata. To accomplish this, I built a series of JSON API endpoints, exposed by Express and backed by RethinkDB. I leveraged Atlassian Connect Express to help with JWT auth, installation lifecycle, and a few other functions. The API runs on Heroku and the RethinkDB instance is hosted with Compose.io.     Un Arduino permet d'tablir une connexion avec les donnes quantifiables qui nous entourent. Cet Arduino communique ensuite avec un serveur en Node.Js qui permet de faire des requtes Mysql  un serveur web, rendant possible l'affichage des informations sur la proprit du client  travers les informations entourant cette ville.  Tandem is hosted on a HIPAA compliant cloud provider, Healthcare Blocks, and is built with PostgresSQL, Node.js/Express.js, and React.js. The initial design was developed with two doctors, who now act as advising partners. A prototype based off the initial design was completed in early January, and since then weve been collecting feedback from medical professionals, nurses, and doctors to iterate our product. Through Healthcare Blocks, were also using Mirth Connect to interface with EHR systems. We intend for our pilot version of the app to be fully HIPAA compliant and integrated with EHR, which should be completed by end of March.  Accomplishments that we're proud ofThere are numerous accomplishments that validate our efforts and keep us moving forward: Recruiting incredible advisors: the Chief Innovation Officer of EMA, two managing directors of hospitals in Virginia, the CTO of an EHR-integration company, and numerous prominent healthcare investors.Building a fully-functioning prototype and building a community of clinicians eager to implement our productWinning $11,000 from a series of entrepreneurship competitions.Moving insanely fast - we work on Tandem 80+ hours a week each - we've gone from an idea to a fantastic product in record time and are now moving even faster to iterate our product through user testing in preparation for our committed summer pilot. What we learnedHealthcare communications are complex -- "Slack for Healthcare" will never be enough to fix our broken systems.  Building an effective solution requires a cross-functional team of technology, business, and medical experts, and its requires a deep-dive into the specific needs and practices of hospital staff. What's next for Tandem MedicalWe are going to continue iterating our product through user testing over the next couple of months.  We have already completed HIPAA-compliance and made significant progress on the EHR-integration front. Built Withnode.jspostgresqlreactTry it outapp-4797.on-aptible.com      Submitted to    GE Health Cloud Innovation ChallengeWinner                Honorable Mention                  Created by  James WangKamran PirastehRipley Carroll  We are aware that are more smartphones connected to the internet than computers, so we decided to create a native app for Android. However this is not the final project. We build an API so in the future we could build an IOS app, web app or even a robotic arm to help companies to recycle.First we need the power of an artificial intelligence, we wanted this AI learn and become smarter. A great start point is Tensorflow, our server is build in python3 with django, great news cause Tensorflow run in python.We manage to create asynchronous tasks (redis) to let our AI self learning. The more you use our artificial intelligence, smarter it becomes. DemoYou can test the api by clicking on nasa2017.jorgechato.com. However if you would like to test the all functionality of our project we recommend you to install the last signed apk (only for Android). https://github.com/TheGreatGatsvim/NASA-April-2017-Android2.0/releases/download/1.0/app-release.apkYou can also see the Android application project, it is build on java. Ready to use in all android devices with lollipop or higher version installed. https://github.com/TheGreatGatsvim/NASA-April-2017-Android2.0 Built WithaiandroidapidjangodockerpythonredisTry it outgithub.comgithub.com      Submitted to    Make School's Student App Competition 2017Winner                10 Upvote Prize                  Created by  I worked in the backend, REST API and Android platform. Also I worked in the algorithm to improve the Artificial intelligence behavior Jorge ChatoI worked on training ai, async training and scrapping images.Samuel Ruiz de Gopegui MuozI worked on the Android platform. Also scraped images to train the AIAlex Oarga  conf.io reviewer is an open source mobile application that was built using React Native, taking from and back to the community, the repository can be found here: https://github.com/x-hub/confIO.reviewer.we are mostly using a fork from https://github.com/nicmarti/cfp-devoxx which can be found here: https://github.com/x-hub/confIO.cfp, we had to do a major UI improvement & added few screens such as the one that allows the user to scan the QR code & also few APIs that allows the API consumer to get the event details.we also worked on a messenger bot that will help the attendees get the most out of the event, which is also opensource and the repository can be found here: https://github.com/x-hub/confIO.messenger.    We built the application using the PHP programming language using the Code Igniter Framework and using the MySQL database as a data storage media. The data that we need for this application is the location (latitude and longitude) of tourism or UMKM as well as a description of the place, to be displayed on the website and using google maps technology as a medium to show and provide directions for the tour, by taking the last location user (latitude and longitude).  We have leveraged power of following technologies for overall solution.   Amazon Lex: We have used Amazon Lex for building conversational chatbots with natural language understanding (NLU) to recognize the intent of text. Lex enabled us to build applications with highly engaging user experience and lifelike conversational interactions. Lexs deep learning allowed us to quickly and easily build sophisticated, natural language, conversational bots (chatbots)  Amazon Lambda: We have used Amazon Lambda as the backend for validating user inputs to Lex during conversation and also for fulfilling user request to update task when all valid inputs are provided  Enterprise connectors like (Rally/Version One): These enterprise connectors are integrated with Lambda to fetch & update data from Scrum tool residing at enterprise backend. In this case, validation Lambda via enterprise connector would be fetching user stories & tasks details from Rally to validate user inputs. Lambda will update the task status and other info once they are available during the concluding fulfillment phase.  Slack Bot App: We have leveraged the Slack app to enable it as front end of our chat bot. Slack is one of the ideal channel with its collaboration and team communication capabilities. It is perfect fit for a team (software project team in this use case). We have created Slack app virtual scrum master app and added its bot to software project team slack channel. In our case, software project is virtual scrum master as well  Facebook Messenger Platform: We have leveraged the Facebook messenger platform to enable Facebook Messenger as front end of our chat bot. Facebook messenger will provide maximum reach as it is one of the most popular social messaging channel in the world. We have hooked-up Facebook messenger with Virtual Scrum Master Lex bot by defining it as one of the channel under Lex bot.    We looked through the project description, did a brainstorm about how the project would go and distributed each part to a single or pair of responsibles.  I built it in swift, and used sprite builder. I used my knowledge from Make School when I attended in summer 2015!  Tossed up a private repo. Used sketch to make Hi-Fi designs. Met with the android team to structure the database and then just started coding out our pages like the Hi-Fi designs looked.  We incorporated Amazons Alexa voice service into a Raspberry Pi 2 and attached it to the helmet. We built the Alexa skill to communicate with the rider, using node.js and firebase for the backend. The Raspberry Pi is powered by an external USB battery pack and the electroluminescent lights are controlled using a relay which is controlled by the Raspberry Pi.      Just with node.js and react.    We've used the brand new Firebase released in Google I/O 2016 alongside Swift in XCode 7        Using Spark AR Studio, we tracked facial movements through the facetracker, and connected them into interaction. For example, a simple wink is a continuous action of, one eye closing and opening/ while the other eye is opened. Then we added a music script, so that when these facial actions are performed in the precise spot, the score would go up. We designed the Graphic elements and layout with sketch, and modeled the instrumental assets with cinema4d. In order to import 3D objects, we used AR Library in Spark AR Studio after uploading them to sketchfab.    We used the NetObjex platform for Digital Assets management and its ability to link to HyperledgerWe built the front end IOS App in SwiftThe dashboard was built using Kibana Accomplishments that we're proud ofThe solution we have is end to end and conforms to the various types of barcode tags currently used in the Thai Seafood industry.What we learnedWe learned how the fishing industry functions at an indepth levelWhat's next for TraceItWe are looking for industry partners to adopt the solutionBuilt Withelasticsearchhyperledgerjavakibanaphppostgresqlreactswift4Try it outdrive.google.comdrive.google.comdrive.google.com      Submitted to    Seafood Trackathon    Created by  netobjex  Created extensive, randomized data for a population of locations in Los Angeles, formatted with ExcelParsed the data on Jupyter Notebook to run various clustering algorithmsCreated visual plots to represent data in clusters and predictions for nursesConverted output of clustering algorithm to a JSON fileCreated an object-oriented structure in Android Studio to represent contents of JSON fileUsed various data structures (Collections, HashMaps, Lists) to group Patient objects in their clustersUser Interface implements the google maps API for nurses to see markers for prospective patients          Created a PyGame environment from scratch, consisting of the following entities: Chosen One (agent), Gun (generator) and Bullets (projectiles). Built and trained a multi-agent reinforcement learning model, consisting of a minimax game between Chosen One (agent) versus Generator (Gun).Main contributionsTraditional reinforcement learning face the difficulty of non-convergence of the agent due to difficult-to-plan curriculum. Using a generative model to play the Gun, the Generator progressively improves, allowing buffer for the Chosen One to improve. This results in a progressive autocurriculum that enables smooth learning of both the Chosen One and the Generator.Accomplishments that I'm proud ofCreated Proof-of-Concept of Multi-Agent Reinforcement Learning enabling smooth learning.First time using PyGame to build a game.What's next for The Chosen One - Multi-Agent Reinforcement LearningIncreasing environment complexity - More weapons for Generator to choose and learn, more fine-grained movement mechanics for the Chosen One to master, possible attacks made BY the Chose One.Final OutcomeThis project shows that multi-agent reinforcement learning and progressive autocurricula is useful to enable smooth training of AI in complex environments.ReferencesDDQN - Deep Reinforcement Learning with Double Q-learningChainerRL - Deep Reinforcement Learning LibraryPyGame - Python Game LibraryBuilt Withgenerative-modelmachine-learningmulti-agentpygamepythonreinforcement-learningTry it outgithub.com      Submitted to    iNTUition v6.0Winner                Best AI Hack                  Created by  Jet NewComputer Science at NUS      we tried using asp.net. Had alot of problem uploading images within a SQL database and the site wouldn't look good without it. Concurrently we tried developing on ruby but content and site design wise, still isnt up to standard. So we decided to concurrently work on plan C to build the prototype using WordPress - albeit with reduced functionality.  Our idea consists of a conceptual network, a 3D printed model of a prototype train, and a programmed Lego Mindstorm vehicle representation of the maglev.  The application was built using Android as the preferred mobile OS. Lots of Open Data was fed for training the application. Weather APIs were used to leverage the accuracy of the application.The sensors used are part of BreezoMeters' Sensor Management Service. Using these sensors, Real-time data for Air Quality is collected. The data is fetched through the Rest API and thereafter it is used to make further recommendations. Moreover, the Satellite data is also used for determining road conditions etc.  We used SoundCloud API to stream the music and embedded HTML5 player. Every user has a private playlist which stores all the songs played. We rank the songs based on the number of plays.Accomplishments that I'm proud ofWe built a nice lightweight favourite music player.What's next for LifeLong PlaylistImprove the algorithm which ranks the favourite songs and release mobile versions.Packages usedaccounts-baseaccounts-passwordaldeed:collection2audit-argument-checkscheckdigilord:fakerecmascriptfourseven:scssjquerykadira:blaze-layoutkadira:flow-routermeteor-basemeteorhacks:fast-rendermeteorhacks:npmmeteorhacks:ssrmeteortoys:allthingsmrt:accounts-soundcloudnpm-containerreactive-dictreactive-varservice-configurationsessionstandard-minifiersstevezhu:lodashtemplatingthemeteorchef:bertthemeteorchef:jquery-validation trackertwbs:bootstrap                  Built Withbootstrapes6html5jquerymeteor.jsoauthsoundcloudTry it outlifelongplaylist.meteor.comgithub.com      Submitted to    Meteor Global Distributed Hackathon     Created by  Worked on Soundcloud integration service together with OAuth authorization. Designed login landing page. Worked on playlist view front/back end.Pawel MordaszewskiI'm moving from a stark Java EE world into the candy JS universe. Looking forward to work on the world-changing projects!Project management. Helped with the UI development, design concept. Performed QA activities.Mansur SWorked on most part of the Front-end code and UI, set up the initial project boiler plate. Also involved in some back-end and integration tasks.Rajasegar Chandranalex davisLooking for other hackers to develop mobile products using latest technologies  The module is conceived as a building block, compatible with Drupal's massive ecosystem of modules. Since it's a module that implements the Feeds architecture, it allows site builders to configure:A fetcher that uploads emails that fulfil a set of search parameters A parser that extracts information from inside the email into separate fields, to store structured informationA processor that maps the information into native Drupal objects and their fields (e.g. nodes, users, comments)  We built the frontend using html, css and bootstrap while the backend was with node.js and mongodb    After getting the MVP working, I exclusively used asdfghjkl for navigation and input while developing the app.It's built in Swift 2.0 (for the easy C interoperability) and partially Obj-C for certain functions.What's next for asdfghjklApple partnership is in the works. NASA partnership is going smoothly; soon the inhabitants of the ISS will be able to get more done, easier, thanks to mandatory asdfghjkl usage.InfoThe correct way to pronounce asdfghjkl is "asdfghjkl". Please don't get it wrong. Additionally, the only way to type asdfghjkl is by sliding your finger across the entire home row. Just don't hold Control, or your mouse will fly to the right!Built Withobjcobjective-cosxswiftswift-2.0Try it outasdfghjkl.co      Submitted to    PennApps XII    Created by  asdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfdsafasdfadsfasdfasdfasdfsadfasdfasdfasdfasdfdasfsadBenjamin Zweigwhat up`1234567890-=qwertyuiop[]\asdfghjkl;'zxcvbnm,./Matt CondonSandile KeswaI'm a serial hacker with a gig appetite for problem solving. Currently organizing http://owlhacks.com.Daniel Cadden        We built Automation for JIRA on a serverless stack using AWS Lambda, API Gateway and Cloudfront for static resources.  Dynamic parts of the UI are served directly via API Gateway, which makes calls to Lambda to render the initial connect page contents and provides REST resources to fetch and update additional data.  When automation rules execute, they are added to a AWS Kineses queue that is also processed by AWS Lambda.The front-end was built using React and Redux and can be run completely standalone. In fact most of our front-end development was done on a local webpack dev-server using mock REST data.    With a lot of help from caffeine, we used Laravel's PHP framework to build a full stack web application that can analyze and display hypothetical data from developing country farmland potentially using FreeWave radios. We used n3 charts to create a consumer friendly data analysis page where one can see varying farm data to keep track and understand what exactly is going on.      Empathy Match is built based on JavaScript framework called Meteor.js and IBM Watson Personality Insights API to analyze the emotions and experience of the participants through semantic text analysis.    Once after the idea was selected the team cooperated in the best possible way to design and develop not only a MVP but a complete finished product. Meteor which always had been at the top was just the right framework to make our idea tick.         We used swift to develop our front-end on IOS, Node.js as the back-end, using AWS hosting the service, and Microsoft Bing API for Image Search.  This was built using Clarifai API to do the machine learning and image recognition, imgur api for image uploading, mandrill for emails, and BlueMix to deploy it all.    I build it using html, css and javascript. I tested it locally by running it on localhost , I started the server using gulp.js      *we will use Google API . *    A Domain was given to us by our facilitator of which my team mate and i utilize the opportunity fully by building a website that perform 80% of the features needed and an android application that performances 80% which together perform 100%. i.e. the application is dependent on the website and some features  of the website only run on the mobile app to optimize both platform fully. Which makes it easy to reduce the barrier between languages in Nigeria to the barest minimum if not fully eradicate.      We parsed https://paikat.te-palvelut.fi/tpt/ jobs and built an online recommender system on top of this data: we extracted keywords from raw job descriptions and matched them to job categories and jobs. This was done with a freshly baked neural net (hashtag deeplearning :). The keywords are used in the flashcards in the app.We also extensively studied The Organisation for Economic Co-operation and Development datasets (http://www.oecd.org/, quite an interesting read!) to create an ability-to-occupation mapping model.Courses data are taken from: https://www.hel.fi/sto/fi/opiskelu/maahanmuuttajat-immigrants/immigrants.Service backend is rock solid AWS/php/MySQL.      The solution was built on top of MS bot framework. The Bot service is created and connected to the Business Layer of SharePoint client-side object model. Thus, this acts as a separate UI for your SharePoint site. All the conversations are intelligent with the NLP background.   We worked as team with each member focussing on their strengths and stronger areas of knowledge. We decided to write our code using as many of the newest features as possible with Meteor. This meant choosing Angular for the front end dashboards which now has excellent support in Meteor. In addition we made a conscious effort to write all code in as much of an 'es6/ecmascript2015' style as possible with the new support in the framework. For our first proof of concept we used a Particle (previously known as Spark) Core IoT device which was in turn connected up to a temperature sensor for real time data capture. The intention was to add more devices and sensors as quickly as possible. Our application design implemented a user flow for adding new devices to their accounts and profiles. Once a device and it's authentication details had been added to the platform our IoThingies application can immediately start querying this device for any methods, variables or data sets which may be available for collection and inspection. So in our first iteration we collected sensor temperature data using the Thermocouple connected to the Particle Core microprocessor.Packages Used:meteor-base             # Packages every Meteor app needs to havemobile-experience       # Packages for a great mobile UXmongo                   # The database Meteor supports right nowblaze-html-templates    # Compile .html files into Meteor Blaze viewssession                 # Client-side reactive dictionary for your appjquery                  # Helpful client-side librarytracker                 # Meteor's client-side reactive programming librarystandard-minifiers      # JS/CSS minifiers run for production modees5-shim                # ECMAScript 5 compatibility for older browsers.ecmascript              # Enable ECMAScript2015+ syntax in app codeautopublish             # Publish all data to the clients (for prototyping)insecure                # Allow all DB writes from clients (for prototyping)angularmeteorhacks:npmnpm-containerernestorocha:sparkjsangular:angular-materialangularui:angular-ui-routerfourseven:scssmquandalle:bowermomentjs:momentstevezhu:lodashdotansimha:angular-google-chartspark/particlejs (npm)  Java android app development    We collected datasets from various sources and also created our own dataset for the training of our model. The Frontend part was created with React.js and was connected with our model using Flask.  The solution is built with the Microsoft Bot Framework, that supports both web chat dialogs and integration with Microsoft Teams. It uses Azure Services such as Service Bus and Azure Tables to keep track of the messages and the embedded web chat and which Microsoft Teams channels they should post to.   The application first renders an interface webpage built on Bootstrap (HTML, CSS) to take the ticker input. Control in the program is passed to a central flask code (running on a local flask server) that receives data by POST. Using modules in file structure, the code calls Google News API and runs article URLs through BeautifulSoup and Diffbot Article API to find the aforementioned list of main phrases. Our database handling downloads keyword dictionary by SQL and then uses a matching algorithm in O(n2) to compare each phrase to the entire dictionary. The frequency-of-occurence for each tag determines the final relevant category for the article, which the flask server then passes to our ML neural network. Using multi-layer perceptron that trains using backpropagation and LBFGS, we use the Cross-Entropy loss function and in-built regularization functions to create a 'usefulness matrix' of binary values. The output of relevance binaries is then mapped to the article list to dump our relevant reports onto, and render, the final HTML page. The user's submission of "helpful"/"not helpful" on the final page passes control back to the Flask server to run SQL queries that add said new entries to the the investor's specific training dataset on the database for future use.   Using Java and its Processing & Swing library, we created a GUI that provides user interactions with different objects through keyboards and mouse.    We tried to combine two futuristic technologies and bring it in the pockets of industry leaders.First being Augmented Reality and OCR which bridges the process of users   Android Studio IDE.Android Design Support Library for Material Design.Aloha Technology for location, time and user behavior data.Parse Backend with Push Notification.Facebook Login.Google Maps.Picasso, Retrofit y OkHttp Http Client.Dagger2 y Butterknife injection dependency.What's next for BiCity Social Cyclist Map & AlertsAdd offers with In-App Purchase.Update user profile.Forgot Password for Email Authentication.More types of place.And More!Built Withalohar-mobileandroid-studiojavamaterial-designparseTry it outplay.google.com      Submitted to    Create Context-Aware Applications with Alohar Mobile's SDK    Created by  I worked in an app for cyclists, I integrated the Parse backend, Aloha Mobile SDK and the new Material Design support libraries.Lesther VegaEntrepreneur passionate about of Android Developer, Augmented Reality and Virtual Reality.One of the first apps I designed from the ground up and with terrible time constraints to boot! Fun though, and I did learn a lot.Fernando MartnezIllustrator, UX and UI designer from Rafael Landivar University. Been designing apps & websites since then and loving every moment.        Using python models  Android ApplicationWe used the  concept of Google Maps, Google Api's, accessing data in Json format with the help of Volley library, maintaining proper database of the user and the emergency contacts.Arduino UnoWe designed the circuit using Arduino Uno where we used piezo electric sensor to calculate the frequency of how hard the sensor was hit.  We started out with creating a facebook page and messenger bot with it. We separated into two teams - one team focused on the bot, and another focused on the native application. For the bot, we used wit.ai for the natural language processing. we created the content for interactions, and stored the data in firebase. The React application used data from the bot and focused on the interface for organizers, and organizer discovery for volunteers. We included Facebook authentication to make it easy to pull data from both sides, and Graph API.  The process of building this application was quite tedious and challenging, but it can be broken up into various modules.Load Unity and Vuforia and create models of all the planets with an ImageTarget that maps the planets to each other and a reference frame.Create an Alexa Skill in the Skill Builder portal and define the interaction models as well as the intent schema. This sets up the framework for the behavior of our skill.Create a Node.JS server that interfaces with the Amazon Alexa and translates the various intents into requests that can be passed on.Create another intermediary server that interfaces with both the Unity application and the Alexa skill by using HTTP requests to pass data between the phone and Alexa. Deploy the Node.JS servers onto a cloud based platform service such as Heroku that can run web apps from a stable and fast server.Step 1: Create a Unity application using VuforiaThe first step is to install Unity with the Vuforia package which allows for the creation of augmented and virtual reality apps. On the Vuforia website, you can generate a database with an image target that renders your content on top of a specified image, which it uses as a reference point. In this case, we used a picture of a rocketship to go along with the space theme.  Next, we had to download 3D models of all planets and position them accurately relative to each other. Step 2: Building the Alexa SkillThe next step was to create the skill in the Alexa Skill builder. We had to write out all the utterances for our skill and define the intent schema in order to create the backbone of our skill. This was fairly straightforward as it takes you step by step through the process. Step 3: The NodeJS Alexa Facing SkillWe decided not to use Lambda and instead create our own custom HTTP server that interfaces with the Amazon Echo. To do so, we used Node.JS and Express as well as a variety of other modules to make this task simpler. The server handles a variety of intent requests and returns a response after sending an HTTP request to the middle server which will be discussed in the next step.Step 4: The Intermediary ServerIn order to link the Alexa skill to the Unity application, we created another HTTP server that acted as a bridge between the phone and Unity application. The server sent HTTP requests to the Unity Application, which then responded with the nearest planet relative to the camera using a custom C# script. The server also responds to the Alexa Skill Server.Step 5: Deploying to HerokuWe decided to deploy our Node.JS servers on heroku in order to give it a static url as well as ensure that our servers stayed up. You have to create a file called Procfile.txt, and inside type:web server.jsThen, its as simple as posting the project on a Github repository and setting Heroku up to follow any commits made in that repository.How to use itEnable the Alexa Skill by asking Alexa to enable Reality Planets.Download the companion app from the link below or from the link Alexa will send to your phone.Ask Alexa to pair your phone and enter the code it gives you in the app you downloaded.Open the image down below on another device or print it out.Use the app to point at the picture and display the planets.Ask Reality Planets about the planet to get information about it.  Tech Stack: Wordpress and Divi style (for site), Google Map APIs, Grindr Data exported to JSON, Javascript, Refuge Restrooms API for data, geohash js library    TMux, the terminal multiplexer, can be scripted to automatically set up terminal layouts. Command line ad scripts provide the ads themselves.          Our inspiration came from an academic paper on Computer Vision and Pattern Recognition. Starting from training data, we build our own deep style service with Python on our cloud hosting including the restful API. Then we embed the service and build a mobile friendly website with Javascript and Python.         Atlassian SDK, a REST API, javascript, a new template for the email notification.Challenges and AccomplishmentsConfluence pages restrictions. It was a challenge to make it possible to show the content of a Confluence page (including attachments) to users who have no permissions to view it. It was needed so that a person taking a quiz could view questions while completing a test, but wouldn't be able to view actual question pages in the space in Confluence. Creating pages via blueprints with applied viewing restrictions and page labels.What's next for Quizzes for ConfluenceWe have lots of ideas and will be looking forward to feedback and feature requests to decide on the further development.Try it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist: Add-on Hackathon    Created by  Katerina KolinaAndrey KhaneevAlexander KuznetsovMaxim Kuzmich [StiltSoft]  This project is build with the Amazon Sumerian platform, which is very user friendly and build applications even without expertise in coding. Sumerian Hosts makes it easy to create and add animated 3D characters called Hosts.   The mobile app was build using javascript, while the simulation platform, which proves the usability of the application, was built using a combination of QT creator with C++, Android Studio, bash commands and a lot of creativity :)  The add-in itself just uses plain old JavaScript with the Office.js library. The UI components are a mix of custom css and a framework named skeleton.css. The add-in is hosted on a nodejs server. The supporting back end is also hosted with node.    This is built on top of the Atlassian SDK using Java and HTML.What's next for iFramedFix licensing and make it public on the Marketplace.Fix whatever bugs I overlooked in my 2AM coding. ;)Listen to user feedback and improve the add-on to meet their needs.  Built WithhtmljavaTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist 2016    Created by  Boris Berenberg      We added three main new components to Brave:a geth binary, which is run as a subprocess of Brave. It is downloaded during the Brave build process.a slightly-modified version of the wallet.ethereum.org front-end (which is also used in Mist): https://github.com/ethereum/meteor-dapp-wallet. Our fork is published as an open-source NPM package from https://github.com/brave/meteor-dapp-wallet-prebuilt and downloaded during the Brave build process.a Brave extension which implements the local ETH wallet UI and the toolbar shortcuts.This is all implemented in https://github.com/brave/browser-laptop/pull/13177.       After architecting a high level conceptual solution, I started investigating the elements of the solution I didnt have previous exposure too. I started with Cognitive Services and used PowerShell and Invoke-RestMethod to formulate my web requests to integrate with the services and perform the necessary functions. I then designed my custom LUIS Model to take phrases and identify the entity and the entitlement. When I had working solutions for Speech to Text, Text to Speech, and LUIS I translated the necessary initiation piece to Python (Speech to Text) and got it working from my IoT Device. I then configured Azure API Management to front end the Lithnet Microsoft Identity Manager REST API Service. I translated the LUIS and Microsoft Identity Manager (via API Mgmt) queries into a Webhook Azure PowerShell Function. The Azure Function utilises Managed Service Identity and Azure Key Vault for the necessary credentials. Next I converted the Text to Speech PowerShell script to Python to return an audio stream of the query response that is then spoken back to the requester.The final piece was to log each request and response to Azure Table Storage and Power BI via IoT Hub for Auditing and Reporting functions.   Claire Li wrote the Android application that communicates the binary light signals, and created the hardware portion of the hack that recieves the magstripe data into the device. We send the photoresistor signals into an analog input pin in an Arduino Uno, then route it to a Raspberry Pi that interprets the information. Ben Roytenberg created the hardware portion of the hack that broadcasts the data to POS-terminals for real purchases, and interpreted the stored magstripe data. He generates the electromagnetic field with this information via stepper motor driver.   Meteor of course! With the Cordova build tools and a lot of packages :)  Eat an entire tin of Altoids in one sittingRinse the tin outDry the tinWrite some Python codeCover everything on the inside of the tin with electrical tape???Profit!    The idea was to build some small but useful feature for JIRA Server and JIRA Cloud simultaneously, trying to share as much code as possible. I used Node, Webpack and React to build the front-end.  Our EDI Connector is built using the following steps:First, a code was developed to make a secure connection to the EDI gateway, where the credentials such as username and passwords are authenticated. The session is created with the help of tokens to ensure valid users and correct information is fetched.Second, we wrote a REST API code that fetches invoice headers and their details. This API was called within the DLL to fetch raw JSON data.Third, the fetched JSON data is then deserialized to invoice class and finally, the invoice class is returned as the output to the Uipath activity (returned as data type List).Finally, the DLL developed above is packed in NuGet explorer and imported to Uipath Studio to be used as an Activity.      We built it using Kyber integration through solidity and web3, compound protocol and react app.  With tears, suffering, despair, and very basic coding skills.        I am a big Android fan so I decided to build this as an Android application.  The data set behind this app is supplied by the USDA in the ARMS - Irrigation and Technology Water Use report.  Available @ http://innovationchallenge.azurewebsites.net/#ArmsTab    First we implemented multiple web-crawling algorithms to take data from GoogleAPI elevation and geocoding, and then we also created programs to extract useful information from the EPA and NOAA, such as the pollutant concentration level each month and wind direction & magnitude respectively.Afterwards, we developed an algorithm base on the Gaussian Dispersion model in python and made a dataframe to run through the whole algorithm to generate a final csv file with the analyzed information in the csvWe exported everything to json files for data representation.For current data, we implemented a backend script in python where as an HTTP request is sent, backend.py would run through the same algorithm as before, but using the concentrations generated by a machine learning code, to give real time pollution and pollution source data.  We learned about the Arduino and NodeMCU. We connected the NodeMCU to a breadboard, and since the voltage is 3.3V and we need it to be on a scale of 0 to 1V, we added two resistors of 220 ohms and 470 ohms. We connected the 3.3V, the Grounded, and the A0 pins to the circuit. We used a potentiometer to replace the lead detecting electrode. This is to verify that the NodeMCU picks up voltage information, which it does.Then, we created a model website to display this information to the public and to the user of the device. It shows the live feed of voltage. This would be useful when implemented in a pipe, so that the government can consistently check the lead levels. People can also make sure the water supply is clean for their children and safe for consumption.  We used html,css,bootstrap,php google books           This application supports Android based smartphone and built using native Android Development Environment    I built you meet world user interface first and one for an approach of user driven development.  I worked closely with a travel blog that is pretty well-known. I also did a ton of user  tests to validate my assumptions.  My you are is written in react JS  im utilizing a UI library that I also wrote concurrent to the project called feuxworks.  On the backend Im using node.js, mysql, vert.x middleware, and a openshift3 middleware for caching, for my auth and api layer I'm using graphql (written by Facebook).  I wanted this application to be platform agnostic so my backend and is just rest API Sitting on openplatform3.             Firstly, We interfaced all the sensors to Arduino 101. The sensors included arduino 101, GSM Module, GPS Module, Pulse sensor and a GSM Module.Using arduino IDE, we programmed arduino 101. At last the, whole system was integrated to form a unit which was then waterproofed and then attached on the Life jacket.  The Alexa skill has been built with Node.js and deployed to AWS lambda. On skill startup a JSON file is requested and downloaded from the GIT repository. On each consecutive question, it retrieves a random question from the JSON array. The JSON array holds all questions as well as the URL to the sound on the GIT repository, once a question has been selected, two other answers are randomly selected.The application has been built with a GIT repository (https://github.com/robbalmbra1/AnimalSounds) in mind so that additional sound clips and animal information can be added to the repo without tweaking the code within the lambda instance, this allowing the Alexa skill to support more than 10 questions to the user playing.Packages and Software usedThe alexa-sdk was used as a development framework and allowed me to use other packages within various intents, such as: ssml-builder, request-promise, random-int and lower-case.  We went through various design iterations/paper prototypes - more than 5 - and decided on the simplest, best idea and design and worked on it.We used machine learning, natural language processing and sentiment analysis to reinforce and further validate the idea as a proof-of-concept (seen in the final output).    We first established what we wanted to do, then we had to figure out how we were going to retrieve the answers. Then we split up and used google and stack overflow to figure it out. We got a working project before we went to bed, then in the morning we ran through and improved it the best we could.  We used AWS (EC2, RDS, Route 53, Recognition S3). For our graphs we used d3.js. We collected the Twitter data by creating a script that would collect data and analyze the sentiment of the data to use for our graphs and what the user should do with their cryptocurrency.      Built it with love from scratch in junction.With  pen and paper I made the SQL map and got ideas and started coding! Using JS,JQuery, Knockout.js for frontend and codeigniter framework and mysql for backend.       TeXTFLIX is more than just a Netflix client; it's actually a window into your browser. Using the Chrome screen-capture API, a custom Chrome Extension pipes frames to the server (a python script) through a WebSocket, where they are rendered as full ansi-colored text and sent out to all connected telnet clients. Technically it could be used for any website, but Netflix is a perfect use-case.    For the Mobile App:SympMD mobile app was built using Xamarin Forms. For the purpose of this hackathon/demo, you can only use the android version and you can try it here: http://bit.ly/SympMD-AndroidFor the Back-end Service:Azure Function App was used to act as the API service of the mobile application, it is responsible to secure different the API keys and fetch data from different azure cognitive services.Azure Cognitive Computer Vision Service was used to identify human features (face landmarks, gender and etc...).Azure Search was used to index and search all possible symptoms and health conditions data. All the data were gathered from different royalty free sources (books and websites). Azure Cognitive Search Service (Bing Web Search) was used to get the different health condition know-hows and alternative medicine medicinal usage.Azure Cognitive Search Service (Bing Image Search) was used to collect herbs/plants image and used it to training of the model in Custom Vision AI.Azure Cognitive Custom Vision Service was used to create and train the model of the plants/herbs images and identify plants/herbs images.     We use a flask backend to implement the game logic. We use a javascript frontend (no frameworks) to paint an html canvas based on the game state and to display the scores of players. We communicate between the backend and frontend using websockets using socket.io.We are currently in the process of hosting on AWS EC2 with Apache (its a struggle)  iTrack is built using Raspberry Pis with an external WiFi dongle. For the sake of the demo, 2 of the Raspberry Pis will be used as monitoring node to triangulate and track the lost device. A central server serves as a user interface to start the tracking service. It will also provide real-time updates to alert the user of the whereabouts of their lost device.  This was a super simple hack: Download a video and extract audio (using -x flag) with youtube-dl.Upload the extracted audio (mp3) to S3Write a simple TwiML file hosted on Twimlbin that plays the mp3 & hangs up: <?xml version="1.0" encoding="UTF-8"?><Response>  <Play>http://gam.s3.amazonaws.com/cdn/you_suck.mp3</Play>  <Hangup/></Response>Register a new Twilio number and point it to your Twimlbin.Transfer aggro callers to your new Twilio number.Are you actually using this?!Nah, Stef's way too professional for that -- but it'll remain a nuclear option.Built Withmp3s3twiliotwimlbinyoutube-dl    Created by  If you see something, hack something.Neal ShyamI write Devpost's weekly newsletter. I also write code.Stefanie MaccaroneHello!        Using react-native, we simultaneously built one app for IOS and Android devices. It connects to our simulated GE Opera API, allowing us to display realtime status updates.          The skill is built using Node.js, Alexa Skills Kit (ASK), Alexa Presentation Language (APL), Visual Studio Code, Lambda, Git and AWS.APL is very fun to play with. The authoring tool is really helpful. I used to mock up the screen and show it to my daughter to get her feedback. She always provides honest feedback. The decoupling of the user interface with APL documents and data binding is really cool. I am amazed at how cool and fast the ASK CLI is when trying to make updates and publish it to lambda. It just takes few seconds to deploy my updated code to lambda using ASK/AWS CLI.     We used programs such as Azure, Firebase, Flask, HTML, CSS, as well as Javascript to help bring it to life.Accomplishments that we're proud ofWe were able to successfully create a web application that analyses facial recognition as well as incorporate well designed user-interface.What's next for EmoceanSeeing as how we created this in under 36 hours, there are a couple more features could possibly benefit the application: allow users to save their results to a unique account and implement dynamic emotion analysis that would give unique explanations for each user. Built Withazurecssfirebaseflaskhtmljavascriptmicrosoft      Submitted to    HackUCI 2019Winner                Hacker's Choice              Winner                Best UI/UX Hack (by Sketch)                  Created by  I focused mostly on front-end elements and worked on the user interface for the webpage. I structured and stylize the HTML/CSS files.jwang0110I worked on the backend connecting the database to the front end and API calls. I also worked on major styling elements throughout the website.Veeral BhagatI worked on the front end logic for the main interface and analytics page. I also helped out on styling other pages.Steve  NgI designed the UI/UX and drew the graphics and illustrations. I also helped out on various front-end elements.Angela Li  We researched the available APIs and libraries available for each platform and see which one suits our needs. We then combined a network interface driver with the libraries to send packets back and forth. Libraries used: pytun, yowsup and PyQt4    We've taken what we learnt from Workflows on Confluence server to build a brand new simplified interface with React.  Our team already had experience building Atlassian connect add-ons on top of Node.js and the ACE framework and we have continued that with this new add on.We've also devoted time on this build to improve the performance, using things like Redis caches, a CDN for static resources and client side tweaks to improve the user experience.        Android, NLU, Naunce Mix and so many other random sh*ts !  We follow the same approach as Atlassian Connect framework, that is, showing plugin's contents inside iframes. This project consists of two components - a plugin template, and a generator.The generator simply inserts into the JIRA Server plugin's descriptor locations of web fragments declared in Atlassian Connect add-on descriptor. It also replace various placeholders with real information, like artifact ID and package name for Java source code.For plugin template, the descriptor contains the declaration of all web fragments. When a web fragment being displayed, the renderers consult information provided in the Atlassian Connect add-on's descriptor to inject the corresponding iframe. We leverage Atlassian Connect's JavaScript to provide the same API for communication between the iframe and the host UI, so that developers do not have to update their client-side logic. The plugin template also provide a servlet that forward REST API calls to /rest/api/2 endpoint. This servlet act as a proxy that authenticates the requests using JWT token, and authenticate itself with Java using a JIRA administrator account.  You probably have a group chat for your projectSo why not use Telegram?We've built a bot for you to easily manage your schedules with your groupmatesMessage @nussync_bot on telegram and try it!Built Withtelegramtelegram's-bot-apiTry it outtelegram.me      Submitted to    Hack&Roll 2016    Created by  hacknroll2016Joseph TangNicholas Lui      Messenger platform components were used to build the frontend. For the backend, a RESTful Web Service was built using spring boot. The options shown to the users are retrieved from GET requests to this service. Since Spring  supports embedded Tomcat servlet container as the HTTP runtime, the packaged bootable 'fat jar' was deployed in openshift easily using the openJDK S2I image of JBoss Middleware. What I learnedI learnt to use JBoss Middleware like openJDK to deploy the backend services on OpenShift Online. I also learnt to design user-friendly interface for the frontend using the messenger platform components. What's next for Airport AssistWith the helps of beacons, the bot can be extended to show localised offers and discounts from the nearby shops, restaurants etc..Built Withmessengeropenjdk-middlewareopenshiftspring-boot      Submitted to    Hack your Travel with OpenShiftWinner                Best New Travel Guide App                  Created by  deleted deleted  We first started by focusing on our backend, we set up an amazon server and formatted it to store the questions and answers. Then we researched the java automation class and decided to use the robot object to interact with the online quiz. Finally, we were able to leverage the complex object-oriented programming style of the Java programming  language to combine all of these parts into a cohesive program.  We built a front end with React.js and OpenCV with Python was used for backend, used with Numpy.  Our app used a flask app as a backend and facebook messenger to communicate with the user. The facebook bot was powered by api.ai and the ML was done on the backend with sklearn's Naive Bayes Classifier.  We built a mobile Android app to continuously monitor network and GPS information in the background of the Android operating system. The app scans all available wireless hotspots every 5 seconds to retrieve access point IP addresses, MAC addresses and RSSI/frequency. The RSSI (signal strength) and frequency allows us to calculate  distance from the phone to a wireless hotspot. In addition, we monitor GPS location changes to pair network hotspots with a Latitude/Longitude coordinate. We use Firebase as our data management service, sending the real time data outlined above to the Linode Server. This Linode server hosts our java server, which analyzes and processes over a hundred wifi/gps combinations a second to identify the exact location of scanned wifi hotspots. We accomplish this by using a process called trilateration. Each non-linear trilateration calculation use all of the previous data points for a given hotspot, and custom calculations to compensate for the spherical coordinates of the earth. Finally, we have a node server running on our Linode host to gather IP connection information from the network. This node server contains an ARP table which allows us to look at all connected devices on the University campus and aggregate IP addresses. Once we have all available IP addresses, we can figure out how many connections each wireless hotspot has and link it to the location data collected before.    The ethereum-based, private network is secured by validator nodes run by hospitals, physicians, EMSs, fire departments, etc: organizations we know have a vested interest in preserving human life (even the pessimistic must agree they are incentivized because they rely on living humans for revenue). They validate transactions, approve new validators, and authorize first responders.We use IoT devices (phone accelerometers/gyroscopes, wearables, etc) to detect potential emergencies and send a notification asking the recipient to confirm they are OK. If they don't, we immediately contact the closest official emergency responder service (911) and additionally launch a transaction on the blockchain. This component  the accident detection service  can either be run on a private cloud server paid for by the potential patient or run by a validator node or external third party as a SaaS.Once an emergency is in process, first responders will be notified via a mobile app (either through a push notification sent by the validator who authorized the first responder to join the network, or by running a custom mobile light client) if the emergency is within their configured range. They can choose to respond, which involves publishing their location, intent, public key, and optional contact data. Once a responder is confirmed, the patient's emergency medical data will be encrypted with the responders public key by the accident detection service and delivered to the responder as they approach the patient.  Meteor, react, radium, cordova, and location API.        Our app is based on a multi-modal routing algorithm developed in Java, that combines public and private transport options and also considers the user preferences regarding time, money, environment and convenience.We are already in near 100 cities all over the world, including Mexico City. We were already using the Mexico City GTFS data and the EcoBici real-time data, and therefore, suggesting mixed trips to our users, encouriging the use of public transport and man-powered means of transports.For this challenge we have worked on incorporating to our list of suggested trips in Mexico City, the following:BlaBlaCar carsharing trips, by using the v2 web service and suggesting possible published rides given the user requested trip,Public transport ticket costs, to better inform the user about the expected cost of the trip,Carrot shared cars pickups, by offering our users to pick up a car and drive.These new features added to our app facilitate the use of car-sharing options to our users.What's next for TripGo in Mexico CityWe are working on including park and ride places to our system, whenever available, which will allow our app to suggest our users to drive to them, park their cars in there, and then travel to the city center on public transportation, therefore reducing urban traffic congestions.Our app is also capable of delivering results in real-time: Up-to-the minute information on predicted departure/arrival times, GPS locations and service alerts for public transport. We plan to incorporate this feature for Mexico City as soon as a valid source of information becomes available. Built WithandroidgwtiosjavatomcatTry it outskedgo.comitunes.apple.complay.google.comtripgo.skedgo.com      Submitted to    Mexico City Mobility Challenge    Created by  Mariano Tucat          We decided to split our work into several parts:1) Scanning twitter ( and other sources but not today :) ) and looking for tweets (we call them Mention that may contain interesting information, e.g. terrorist attack, strike, weather condition, etc 2) Extract from the tweets information about city, country and classify tweet itself (strike, terrorist attack, etc). Try to get the coordinates of the place 3) There could be many tweets about the same event. During this step we group them and create Event entity. 4)  Get weather information from openweathermap.org 5) Analyse on which flights our events influence. For this reason we check whether we have flights from or to particular airports where event happened. 6) Create a simple frontend to show that it works. We have 2 main pages:page with event, its type, how many flights are influenced page with flights, where in we can see information about following flights, their status and events, connected to this flights. We think that that it is very important to know this information in order to make a decision.  We started building this add-on because some of our colleagues had a serious need to manage their relations in JIRA. After some research, we noticed that many people were looking for some sort of CRM functionality in JIRA. We weren't sure what people were looking for exactly though... This is why we built an MVP Server version of Relations and shipped it internally and to a dozen customers for feedback. And boy, we learned a lot from that! The #1 feedback we got is that people definitely needed this add-on.Even though we were using Server ourselves, like most of our clients, we decided to focus our efforts on JIRA Cloud at first. Why? Because we realized that the challenges for the Cloud version were an order of magnitude more complex, that decisions would be harder and that the technology would likely slow us down. But going through this tougher road would ultimately help us later when going back to Server and it would allow us to reach way more users right from the start.Looking back, we're glad we went down this road. It's not that we have an exciting feature list, but we know that we have a very strong core and that what we have, we can build on.We chose technology that would allows us to build on an amazing stack, but also port as much code as possible back to the Server version. All of the front-end code is therefore written in ClojureScript. The backend is written in Clojure with a strong dependency on the Datomic transactor. For the Server version we will match that with a triple/quad store implementation of a Sesame RDF store that will use Active Objects for storage. But that's enough buzzword-bingo.    This solution uses Java for backend and Hyperledger Blockchain to enable transaction security and guard against data loss.This system also Integrates into the NIMC's (National Identity Management Commission) Data Verification API for Customer Verification.To be sure of authenticity of Receiver, this System uses Biometrics Verification (Checked Against the NIMC's Database).This system has a web portal and a mobile app for easy accessibilityThis System also gives room for employment and self empowerment as agents can subscribe to this service to deliver these services at their locations and make some charges thereby earning a living.      Created astar  paths for the planes to follow on the runway, and for takeoff/landing. Used Alexa to change values in a Firebase database. Unity listens for the changes and acts appropriately.      I used CreateJS for manipulating the canvas and vector graphics. I used CodeMirror for the in-browser code editor. I'm using NodeJS with Socket.io for the networking. The game runs the Javascript code you supply it in an isolated container so you can't interfere with the game itself. All you can do is return which key to press next. You have access to some data about the world, such as your position and the positions of other players.     We re-implemented the backend using Ratpack based on the Server REST APIs. Data is stored in a dedicated database, accessed by the REST API.The frontend Angular application is the same for Server and Cloud. Feature flags for the features that are in server but not yet implemented in cloud were added so features can be enabled as soon as the backend supports them. Some adjustments were required to make it compatible with the Cloud environment, such as changing the APIs used for opening a JIRA issue dialog.    I started from Raspberry and Arduino technologies for the electronic part as easy to program and configure. It was very easy for me to use these technologies as an amateur.Once the control part of the flat was done I then developed the web portal to simulate my flat in a virtual way.I used the Python language and Javascript for the back office. I also developed a websocket and API (in flask library) in order to make the simulation in real-time. I used lambda functions to control my API and manage my devices.Lexus is connected to the platform via a Slack token to share its services with multi-channel and secure channels. Its hosting is a simple docker container that can be easily moved to a typing or an EC2 instance in the cloud. Currently, it can be deployed on multiple EC2 clouds with load balancing. This architecture reinforces its resilience and performance as a distributed system.Its development took place through 4 major stages:Architecture In a first step, Lexus uses the arduino and raspberryPi configuration with USB communication and a Light REST API for management by the Radio Frequency 433 module of the home appliances.Second, I developed the back office in python Django, Redis Cache, Websocket, Slack, Blueprint3D and Blender. During the third step, I developed and settup Lex and the meaning intelligence for real user talking.Finally, I developed the Front Office, meaning the request emulation into the 3D view with Blender software and the chat functions with Lexus.    It is built using NodeJS, Solc-js, Express, React, etc. It is forked of the 0x monorepo as a starter-kit, but honestly it does not use much of it and can easily be turned into an independent project.It works as follows:The user submits a contract address.The contract source code and compilation options is fetched from Etherscan.The correct historic version of Solidity is instantiated and the contract is recompiled with sourcemaps.The bytecode, sourcemap and sourcecode are parsed to create a mapping from program counter to line of code.Execution traces for historical transactions on the contract are fetched using debug_trace.Traces are converted to mappings from program counter to gas consumption and aggregated.Program counters are mapped to line numbers and aggregated.The contract source code is presented with syntax highlighting.The gas cost is presented in customizable formats in the margin.  A web application was built using HTML/CSS and JavaScript for planning and organising the program, whilst a mobile app was built using Swift for assisting participants with learning and practicing new vocabulary in preparation cooking meetups.What's next for OpenDoorPartnering up with non-profit organisations to launch a trial program.Built WithcsshtmljavascriptswiftTry it outappetize.ioopendoor.esuu-creatives.com      Submitted to    Techfugees Melbourne    Created by  We have started the project using User Experience methodology; Understanding the situation from refugee subject expert, Masod. As a team we conducted a workshop empathizing the users, identifying the problem statement, solution statement and design workshop for ideation stage.I have spearheaded (and work on) the paper prototype (wireframe) for OpenDoor admin platform that consist of the homepage (call to action for volunteers or users (refugees) and download of OpenDoor app) , volunteers login page, dashboard of default calendar and next cooking demo event listing, create new cooking demo event and add participant search criteria (listing). Created the persona 1 (women refugees), person 2 (volunteers), tasks and later the storyboard of the user journey which can be seen within the presentation slides. Lastly, I have conducted the usability testing for OpenDoor app (for persona 1) where it helps identify areas for improvements and recommendations for next step. The usability testing video can be seen within the presentation.Evon TanI am an entrepreneur/Fashion/UX Designer. My passion is in connecting creativity and logic thinking.Project management, pitching, cost+benefits analysis, research and contribution to the problem definition and ideation process.Rebecca MajorI did all the front end dev for our web interface.James WangDeveloper, front-end and back-endUser Experience Design, logo, affinity diagramming and enjoying the lovely coffee by wide open road coffee roasters.Tung Van Truongdigital escape artist, deep mind diver, incurable optimistsamantha wAspiring iOS Developer from beautiful Melbourne AustraliaDoris HoogeveenCumputer science PhD student at Melbourne Uni working on language technology    IoT Device:We are using MXChip IoT DevKit (AZ3166) as the device. The source code for this Arduino based development board is written in C/C++. It has many sensors embedded on the board, so we didn't have to solder the sensors manually and we were able to focus more on the functionality.Web App, API, Database & Function App:*The Web App & API are built using (C#) ASP.NET Core to handle the business logic on the server side. *We used C# for developing Functions on Azure Function App. *We are using JavaScript to get data from the API to the Dashboard and also to handle several client-side components, including the Maps (using Azure Maps), Alert, etc.*We also used Power BI for showing the historical status graph on the dashboard. Power BI really helped us to speed up the dashboard development process, as it allows us to design professional looking graphs very quickly.We are using Azure SQL Database for storing all the application data.  I created a Microsoft Azure LUIS app and hit the endpoint of the LUIS app in my web application.  We used a button on an Arduino board to emulate the action of closing the fridge door. The signal created by the button is sent to a PC through a serial COM port. When PC receives that signal, the kinect camera is triggered to capture a photo of the current status in the fridge. The photo is then compressed and sent to our web server. Our web server is coded on Python+Flask and deployed on Google App Engine Flexible Environment. This web server also contains some logics for responding to Twilio messages, which will be mentioned later. When the web server receives that photo, it puts the photo in Google Cloud Storage. It also keeps some basic image metadata in Google Cloud Datastore database. Then the Google Cloud Vision API is called to analyze the photo and label it by what the item is and which category it belongs to. The labels (coming out of cloud vision api) are then passed to Google KnowledgeGraph API to be further narrowed down to things people would normally put in a fridge. The results coming out of Google KnowledgeGraph are then stored in Google Cloud Datastore database. Now the fridge basically identifies the items that were put in it by automatically capturing and analyzing photos. Every time new items are added to the fridge, Twilio would send a notification through SMS to inform userUsers are also able to text Twilio some basic commands to:Check what is currently in the fridgeCheck which item is about to pass its expiration dateCheck the nutrition of the food storedSearch for recipes related to some items  The first matter to be decided what framework I was going to use, the decision was not hard, I picked Ruby on Rails for two main reasons: I already have plenty of experience working with it, and it's also the framework in which I am also hosting my other Twitch Extension. Once that was done, I needed to decide what I would do to handle the speech recognition, while many services offer Speech to Text, most of them are meant for recognition of text from recordings, not a stream of audio, so I went with the Web Speech API, which allowed real time STT conversion, which I could later send to the RoR server for re-distribution to all other connected clients, and therefore all clients would receive the same content, in real time.What I learnedDuring the process of making this extension, I learned a lot about both the client and server side of WebSockets and about the Web Speech API, which I found rather interesting. Mostly about the way the WS subscriptions work and how Ruby on Rail's ActionCable handles them.What's next for Closed Captions for Twitch StreamsNext up on the list:  Option to use multiple speakers(Not confirmed yet, need research).  Support to automatically add Moderators and VIPs from chat.  Setting default color options for text on a per-streamer basis.  Color, Font and Placement options for text on a per-viewer basis.Built Withamazon-ec2google-web-speech-apiruby-on-railswebsockets      Submitted to    Twitch AWS Extensions Challenge    Created by  Alejo Pereyra  Android and MS SQL Server 2014 (ASP.NET Web API 2.0)          Moving through several iterations of the robot throughout the night, we settled on our design due to time and material restrictions. The robot was mainly built with hand tools and scraps from Harvard's machine shop.  We 3d printed a case to hold a Raspberry Pi and a battery that could connect to the bike frame.  This was for the brains of the project.The bike light was opened using a splitter tool and we used an oscilloscope and function generator to figure out how the circuit works and determine signal timing necessary to interface with the device.We also built an op-amp comparator circuit (there were no transistors left!) to be able to signal the bike light to turn on, turn off, and switch between the four modes.  A python script was written to be able to interface with the lightBelow is a link to the documentation of how the device operateshttps://docs.google.com/document/d/1RgW_YeAuNCNQEm03j-cdHYmJ41WvquLUqxvsqUrFhA8/edit?usp=sharing  Using java, AWS and caffeine:)   There are various components involved in transferring streamers gaze on their screen to a stream on Twitch. The easiest way is, of course, to burn the gaze bubble into the stream. However, this won't offer the best user experience for the stream viewers as they don't have any say in the whole setup.We started to develop the Twitch extension based on the Tobii streaming gaze application which we already had. It now needed to push the streamer's gaze data to the viewers, luckily Twitch's PubSub API fitted that purpose. From the viewers perspective, they would get streamers gaze data and could customize it according to their preferences or turn it off altogether.Tobii Ghost windows application passes the gaze point data to Twitch using products offered by Amazon Web Services, then Tobii Ghost extension on Twitch will receive the gaze point data and will try to visualize it with the help of WebGL.What we learnedThat eye tracking is a great addition to streaming for 3 main reasons:a) To allow viewers to understand the streamer decision-making process while they play. b) To allow viewers to see streamers reaction time, been able to measure Streamer eye-hand coordination with their own. c) An extra tool for educational streamers to showcase their game awareness and what they focus is in the different stages of the game. in addition, being able to have access to all the data that streamer generates with AWS has made us learn a lot about our users. What we initially thought was a great product for the battle royale and overall shooter genre, has given us an amazing response in some other communities. That has been the case with  OZU!, Tetris 99, Guitar Hero, Chess and Poker and IRL. We have also learned how to create a community out of this specific extension. What started as a support channel for new users, today is a place where streamers are constantly discovering new ways to use their eye tracker to connect deeply with their viewers. We are very aware that we have only just started and there is a long path to follow in terms of development, but for now, we have achieved great things and one of them is celebrating user content. We have done that not only by placing them into the spotlight.What's next for Tobii Ghost - Eye Tracking Twitch ExtensionThere is a lot cooking, so going forward we intend to make sure we continue the development of this tool by listening to our community feedback since we believe that user-centered design is the best way to explore the future of eye tracking in streaming.Based on that we are looking forward to working together with Twitch in order to fix time sync. Right now that is a process that needs to be done manually for every streamer. We believe that the outcome of this will be a big revolution since we will be able to provide the social aspect of eye tracking allowing viewers to make clips including Tobii Ghost.We are also looking forward to extending our experience to more devices, with mobile as a big target.Lastly to be able to give streamers way more control and data over how their viewers interact with the extension, so they can see what decisions boost engagement.Try it outgaming.tobii.com      Submitted to    Twitch AWS Extensions Challenge    Created by  Back-end DeveloperMohammad MousaviContent strategy, promo. Ignacio della MaggioraEye Tracking Enthusiast - Working for Tobii Eye Tracking. Software architecture, client application, webgl renderingAlexey BezuglyDesign & UXJuha_S  My time on this project was split 50-50. The first half of my time was learning react-native and developing the cross platform interface. Next, I had to learn a bit of machine learning using Clarifai's Image recognition API. I then spent time training my own custom model using predictive analysis to recognise common physical wounds .  We built an iOS app, a Node.JS server to run Twilio and manage requests between the phone and twilio, used neura to handle predictions for what the user is doing. Together, these things make up Butlr.  We used a similar approach to how we built ERC20 Sets and just extended it to the ERC721 interface to support the new use cases and methods.  The extension was built using Javascript and uses the MathJax library.Our challengesWe werent able to figure out how to bypass the Messenger file upload system to automatically send PNG files or get the live preview to load on the main page. The image processing script is currently only able to convert the MathJax SVG output to PNG and allow the user to download the file. We will work on resolving these issues in the future.Accomplishments that we're proud ofActually implementing the idea, the live preview and learning about MathJax and Chrome.What we learnedWe learned a lot about the web stack and, in particular, about how Chrome extensions work.What's next?Adding live preview to the main page and allowing users to send PNG files automatically over Messenger.Built WithjavascriptmathjaxTry it outgithub.com      Submitted to    MHacks NanoWinner                1st Place              Winner                Top 12              Winner                All Submissions                  Created by  Implemented the live preview code and the click to copy LaTeX markup from renderings. Edwin ZhangUniversity of Waterloo Software Engineering '22Worked on the message passing and detection of network events.Brett SelbyI worked on the rasterization of the equation to PNG for those who don't have the extension.Mathieu GodinUniversity of Waterloo Software Engineering '22I came up with the idea, worked on the icon, initial MathJax setup, message passing and preview hiding.Dmytro ShynkevychUniversity of Waterloo Computer Science '22      The mobile app is built in Flutter with Dart. The website portal is written in CSS, HTML, and Javascript.The backend systems (login authentication, databases, etc) are written in Ruby and hosted on Google Cloud.  The extension front end was built using standard web UI technologies with ReactJS. The Companion site is a standard Ruby On Rails application hosted on Elastic Beanstalk. When a broadcaster signs in using a Chrome browser (Firefox support for Speech To Text is on the Mozilla roadmap) and they click "On". The browser will start listening to broadcasters mic and convert speech to text at near real time speeds. From there I send that information to the Server over a websocket when it passes though a simple profanity filter and then relays the payload to the broadcaster channel using the Twitch PubSub API endpoint. Now all viewers of the channel will receive the the Closed Captioning text to display. Since all viewers have a different latency to the broadcaster before displaying the text, I check the users HLS video latency and delay the display of the text for the amount of time. This helps to keep the text as in sync as possible with the broadcasters voice.      Using Meteor.    Having the problem clearly defined, our first priority was to brainstorm a backlog of features and figure out which were part of our minimal viable product (MVP) and which were nice to have that we could squeeze in at the end to wow the crowd. We then went on an epic journey to find the brand we would give our bot.  This would in turn give us a better idea how to design, market and bring it some appeal. On a technical note, a Slack command sends data to our Azure web API endpoint.  This endpoint then crunches your Slack data and figures out the unread messages and direct/undirect mentions.  This formatted data is sent as a beautiful digest in the taz bot instant messaging channel.  We use Leap Motion to read the finger motions such as numbers and letters where it translates the motions into a speech through Nuance's SDK. We also use Myo Armband to read more complicated motions that involves big movements to translate them into speech.     Used MEAN stack for the development of the entire project. The text summarizer, chat system, video calling and video search were built on Node.js (The only real dev language) with MongoDB as the back end. Socket.io was used to establish socket connections between the devices for broadcasting mobile screen sizes to larger screens.HPE's Audio to text API was used to convert video files into text for videos without captions to search for tags for highlighting on the timeline. Finally built a clickable wireframe using Socket.io for turning these stand alone things into a working app model.              The browser uses Vue.js to render responses from the API, which is made with Node.js (using Express.js). Authentication is Github OAuth2 (login with Github) and all the post & user data is stored in RethinkDB.  Using Html and the google maps Api and much more! We worked on a script to automate the snapshot taking and the analysis of each photo taken. This is the core of the program since it uses the security cameras to detect if the users are present during their reservation. If you are curious come talk to us!Accomplishments that we're proud ofHaving a functioning systemBuilt Withgoogle-mapshtml5javascriptpythonyoloTry it outgithub.com      Submitted to    Hackatown 2018    Created by  I worked on the script to automate the snapshot taking and the analysis of each photo taken. This is the core of the program since it uses the security cameras to detect if the users are present during their reservation.Humberto VillarinoEliane BarbeauQuantum TellPOComtois  We built it on several technologies, primarily using on the Arduino board.     Central Hub (acting as server for the interweb)We created a system as a server which will act as a server for the entire interweb. Any content to be shared with the interweb will be present in this system. This is the only device(system) that needs to be connected to internet whenever something new is to be downloaded. The entire architecture will still be up even if this system loses internet access, just that it won't be able to download any new material. It can still provide all the inter-connected devices data for consumption without internet connection.Client Side applicationWe created a web application that's accessible from literally any device that supports a browser. The web application accesses the data available on the local server and consumes it as if it's available on it's own machine only. Client web application also enables users to communicate with the admin as well as other registered users. Users don't need to register in order to consume videos / cached websites.  I wrote the voice app in Java and host it as a lambda function. It uses DynamoDB to store session data and information about the app. Each time a user loads the app, it looks to see if the user has a prior session saved. Returning users have different options than new users. Once connected with our skill, a returning user experiences a personalized greeting and the ability to redo searches or continue those that may have crashed. Then, I built a server to store user info and return graphical responses in ruby on rails. The server is used for account linking and finding all of the housing data.  It was built upon Actions on Google and Dialogflow.  Mistakes and regrets. Lots of them.  Enthusiasm / Node.js / React / MongoDB / kiwi.com API / OPENSHIFT / JBOSS MIDDLEWARE  We builded it with the Microsoft Graph API and followed all the ISV guidelines from Microsoft.  I started with the JavaScript library for image editing. Next, I built the JIRA plugin that wraps it in the JIRA user interface. I used frontend technologies mainly, the only one Java class so far is responsible for handling license.    A combination of C#, Unity, and CInema4D to create 3D assets.  Agile Estimate is built using React.What we learnedJIRA Software REST APIWhat's next for Agile Estimate for JIRA(update) version for JIRA Server has been already shippedBuilt WithjavascriptreactTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist 2016    Created by  Vasiliy KrokhaRoman Semenenko  Appian serves as an interaction between employees, process, data, and content. Unify these interactions for better, faster, smarter decisions.  Chat Heroes was built using several technologies and Amazon Web Services infrastructure. The characters themselves are created in Adobe Illustrator. An animator animates them using Esoteric Software's Spine. The animation is exported from Spine into images and JSON data hosted on AWS S3 and AWS CloudFront CDN. The images and JSON data are put together in the browser using several JavaScript libraries hosted on Twitch's CDN for the extension. The animations are controlled by an IRC chat bot written in Node.Js, which runs on an AWS EC2 instance and the GUI JSON endpoints are hosted on a separate AWS EC2 instance running Amazon Linux with Apache. JSON Web Tokens are exchanged through Twitch and the endpoints to ensure the authenticity of the transactions.        This project was created using several different technologies.  It primarily uses open-source and Amazon platforms.  The key technology is Amazon's APL.  The APL is designed using several layers of APL Components, including APL Video, that are sequenced with SendEvent Commands and Document Directives.  Document transforms, game logic and speech response is managed using the ASK SDK for Node.js running on Lamba.  Skill configuration for invocations, utterances, intents, lambda, etc, are maintained and deployed through a Skill Manifest and the ASK CLI.  Resources and Assets come from two different sources.  Static files are hosted on S3 with a Cloudfront instance on top.  Dynamic assets, like the dynamically generated 3D scene, are requested from a Lambda service with API Gateway routing requests.  This dynamic asset service uses a Headless Chrome Node API called Puppeteer to load a website, take a screen capture, and send it back to the Alexa device with a "image/png" header.  The captured website uses Three.js to dynamically construct a scene based on the variables in the request and composes elements such as extruded text.  Animations were also scripted in Three.js and then captured, cropped and embedded in the APL. The 3D scene was first composed in Unity to get the look and feel correct and then exported.  Finally, the fork's sound effect was composed in Sony Acid Pro using a couple of layered sounds and exported to the appropriate MP3 specs using Audacity.Enlarge ImageEnlarge Image              Microsoft Cognitive Services:Face APIEmotion APILinguistic AnalysisText AnalysisSentiment AnalysisYouTube API:Used intelligent tagging to play music based on moodPHP  MySQL backendAndroid SDKHttpRequest Library    The laptop stand was printed by Cubify Cube. The voice transcription is done by IBM BlueMix. The database is done with Back&. The app is written in HTML5/JavaScript as a Chrome Extension.    This is an Office 365 Excel Add-in using the Excel API to parse the current active worksheet. It uses angularjs in the Task Pane and Polymer in the viewer. This allows for maximum portability and device compatibility.        Using Azure's SignalR, we were able to instantiate real time communication between the controllers and the display, providing a smooth gaming experience.    We used a smart ring with two circuits (RFID and NFC). Our office access badge was cloned to the RFID, while we used the information stored on the NFC, along with a custom developed Credential Provider for Windows, to create an easy system to unlock the computer. The Credential Provider receives the NFC information from an Arduino controller, which passes the signal from the NFC reader to a USB port. Also, we developed a Windows client application (with Electron) that captures the unlock events and converts them in donations, towards an NGO.  Lots of tech! Its built on node.js.Frontend:Angular 2 (love this beast!) running on TypeScript,WebPack configured.Backend:Express.js (also on TypeScript), OrientDB (Graph Databases FTW)          We only used for this project Ethereum Smart Contracts (written in Solidity), and a little web server serving static files which are actually being executed at client side, granting 100% credibility because you're directly interacting with the Blockchain.    We spent initial hours discussing various interesting ideas we came up with and wrote designs for few of them. This was followed by feasibility discussion of these ideas :D . Finally we decided to work on weather Alert system. After finalizing the design we decided to make a mobile application. It was getting difficult to decide platforms to work and therefore we came up with an idea of a web-based application.  We wrote the code on HTML and PHP and used LIFX and OpenWeatherMap APIs . We have implemented following functionalities.Switching Lights On/OffSetting lights to desired colors.Control brightness of the light.Implemented Special Effects - Pulse effectsWeather Alert - This is the most vital functionality. Using current user location (Longitude and Latitude ) we fetch data from OpenWeatherMap. This returns JSON data with different details about weather. There are three major categories of weather status returned - Clouds , Rain and Clear. When status is "Rain" we invoke a pulse notification with Blue color that notifies user of Rain. If the weather status is Clouds we give a mild purple color pulse. This indications help user to stay updated without looking at their digital devices. As this is a visual stimulus it quickly grabs user attention and saves them from missing any update (SMS or alarms can be overlooked if they are silent or in other rooms). These alerts then help users in planning their schedules such as - meetings or travels. As, this is a web based application so it can be easily modified into an i-OS or android based mobile application. Demos - https://youtu.be/nSY-xDraPCEhttps://youtu.be/JAF_W9bzmV4  We built a simple Android app that is sending data through a HTTP POST to the API that we built in C#. The API then manages the calls to the IBM Watson API and send a MMS if it's applicable.  Android Studio.    Java/Android-Studio to build the Android version and Swift/XCode to build the iOS versionGoogle Maps API to display the Map and Firebase for cloud integration and user authenticationGoogle's Material Design guidelines for beautiful User Interface and User Experience  Our projects include different advanced technology such as Microsoft BabylonJS for VR experience, Deep Learning for training brain images and Google Cloud Platform with Compute Engine.     the WeLearn project was built with React360 only on 20 days  Neural network architecture was constructed, measuring various criteria and evaluating various mathematical operations based on these findings to build a profile of the phone's owner.    We built it using Android studio and Parse as the backend for content and activity .  We built an Android app using Java and XML. We worked together as a team and faced every problem that came across us. We kept motivating each other to do better and didn't give up even when we were on the verge of losing all hope of completing the app.  We used the Particle Photon with a pressure sensor, and LED strips to create the gloves. Then used C++, Particle's back-end,   We used Flask for running a local server because TensorFlow is easily managed from Python. We trained the machine with over 1000 midi files that we got from public libraries. Midi (that's why we are called MIDIocre) is a perfect format because it only saves information about duration and the tone, so it is ideal for training. We used a sequence learning model and a recurrent neural network (first time we heard this terms) because usually neural networks accept a fixed size input, but here we accept a sequence of information (notes).   The iOS mobile app is built using Swift while the back-end is a Django server, communicated via JSON APIs  We built the entire product using Javascript, with a frontend written in React.JS and a backend written in Node.JS. We use a mongoDB to store users and items. In the backend we implement and talk to both Zalando's API, as well as Twitter's API. In the frontend we use a GoogleMap to display the user's current position and your closest borrowable items.  With the help of the nuance api, Android studio, python and an arduino.  We build a layer on top of google maps using places and direction API along with our own webservice providing the exit numbers for the destination. The data collection model is based on wikipedia, where users help key in data. We also use some big data techniques to fill the gap using local government agencies and business.  Node.js service that connects to Confluence via Atlassian Connect. That service tracks content changes in a Confluence Space and when it detects a change it converts the raw storage to static HTML using metalsmith (http://www.metalsmith.io/). That content is then published to Amazon.    The product is a Chrome extension which uploads images to a Node.js server hosted on AWS.Facial recognition is performed on the images using Python and OpenCV. For facial recognition we trained a logistical regression model on a dataset of eigenfaces generated from the AT&T Database of Faces [3] as well as normalized images of Matt's face.When Visage verifies your face, a new tab opens for each of your social media sites. Login is done automatically by selecting text inputs and buttons with jQuery.We use Twilio to send an alert to the user if an intruder attempts to sign into their browser.Accomplishments that we're proud ofIt works![1] https://techcrunch.com/2011/10/28/facebook-sees-600000-comprised-logins-per-day/[2] http://www.reuters.com/article/us-cyber-passwords-idUSKCN0XV1I6[3] http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.htmlBuilt Withamazon-web-servicesexpress.jsjavascriptnode.jsopencvpythontwilioTry it outgithub.com      Submitted to    VandyHacks IIIWinner                2nd Place              Winner                Hacker's Choice                  Created by  Backend: image upload server, face recognition, twilio. Helped out with frontend designRidoy MajumdarI helped with the Frontend. I focused on the automatic logins into social media sites and cleaning up the overall product.Matthew Violet  Tiger Event is an iOS app built with swift and objective-c. In the backend, a relational database runs on a SQL-server on Google Cloud Platform. A C# restful API is used to connect the iOS app with the relational database.    I took the help of YouTube videos and with the help of eclipse and firebase I have built it.  Knuckles and his footsteps were rendered with Unity and the app was built with Android Studio using Java. The landing page website for our new startup was created using basic HTML, CSS, and Javascript.    We built a NLP algorithm that parses the natural query and converts it into JIRA API calls to fetch the data. Bolted on top of our algorithm is an intelligent visualization layer that renders data in various charts,tabular,graphs etc.  I have used php, javascript, bootstrap, jquery, mysql, ajax and some opensource libraries to develop this application. Whenhub API has been used to create, delete, upload and organize users' albums and photos. Picast asks user's "whenhub access token" for creating a schedule named Picast-Albums.  Users can create multiple albums(events) in that created schedule of whenhub and can add photos(media in whenhub events)  to their albums. Picast is made user friendly. Update, delete, edit and sharing facilities have been included along with the options of following other users' photos and albums.How whenhub API has been usedCreate a schedule: create a directory to preserve all the picast albums of a user with 'name', 'when', 'description' and 'createdBy'Create an event: create picast album with 'name', 'description', 'location', 'when' and 'createdBy' Add an image to a event: add images to picast album with 'name', 'description', 'createdAt' and 'url' Update an Event: to update the album's' name', 'description', 'location', 'when', 'updatedAt' and also to remove and update image's info in media Remove an event: delete the albumSchedules with events and media:  get the albums' and photos' info     The retrieval of similar questions and answers is done with the help of Gensim, a topic modelling and document indexing Python library. Gensim allows us to analyze semantic similarities between words and sentences. We used Wayfair's main PHP-MVC framework along with Redux for the webpage. We also built a search via Solr for better search performance. The website is structured similarly to Stack Overflow. The home page has a search bar where users can search for a question, and then select from a list of questions to find their answers. Once a user clicks on a question, they get redirected to a page that has a list of answers that have been associated with the question. It also allows for other users to add in follow-up responses and vote on which answers have helped them as well. The user can also post questions from the website and optionally associate a help channel with the question.  E2S is a cloud based Office add-in for Outlook. Using Microsoft Graph, Fabric UI and the authentication and consent model    We used an open-souce 1000-class convolutional neural network with some custom libraries to make it work on mobile phones, and a few filters to make it work especially well with objects. The neural net is using Torch as a framework and works through an iOS (Swift) app. The data for images used in challenges are stored in an AWS server.  The team utilized Java, Android Studio, Firebase, Pyrebase, Flask, Python, Heroku, Google Maps API, Google Single Signon, Bootstrap, CSS, HTML, Javascript, and Geopy to create a fully functioning android and web app.   It was built using excel vba and existing set of uipath activitiesAccomplishments that I'm proud ofThis solutions helps us realize that different facets across tools can be used as an addition to RPA to solve numerous problem statements. What's next for Excel_Summary_Snapshot_in_mailWe can add a feature to merge screenshots wherein we can view or validate details side by sideIt must take the excel range, sheets , no of images as an input and then place them into a single image Built Withemailuipath-studiovbaTry it outgithub.com      Submitted to    Power Up Automation    Created by  Jasnain Singh                We used Github to work as a team and achieve the goal of version control. Each one of us has a task: Siyuan is responsible for UI of the app, Yuren is responsible for the icon design, string generator and image player, Harry is responsible for icon design, link images to string and view management. We started with simple tasks and tried to add features to our app after debugging the basic version. We always make a plot graph before moving on the next objective. Therefore, we communicate effectively and maximized our learning experience in 24 hours.      We take in images of the item using a Mini Spy Camera from the MLH Hardware Lab and send it to a web server where it is forwarded to our Google Cloud machine learning model for identification. Google Cloud then responds with a classification of recyclable or non-recyclable, which is then transformed to a message to a servo controlling an arduino to move either left or right.  With built it using Android Studio, Java, Google Places API, and Google Location Services.    SMSiri is built on a flask backend, using a myriad of APIs. We used the wolfram|alpha API for general information retrieval, the Bing Maps API for step by step navigation between an origin and destination, the Microsoft Translator API to translate phrases between two languages, the Open Weather Map API to get the present weather at any location in the world, the Twitter Python library to retrieve the latest tweet of a certain user, the Yahoo Finance API to look up the prices of stocks, the Expedia Activities API to discover interesting activities near a location, and lastly the Microsoft Azure Bing News API, which allowed users of SMSiri to view headlines pertaining to any topic or location around the world through text messaging.  The Indeed API was used to return a basic list of jobs for a given position such as "software engineer".The app scrapes information about companies from news media sites like CNN Money using a from-scratch Python crawler. Over 800 companies were ranked by analyzing over 60,000 news articles from the past 5 years.This information is analyzed using two forms of sentiment analysis: Wolfram Cloud API sentiment analysis and a from-scratch Python sentiment analysis based on Bill McDonald's (University of Notre Dame) master dictionaries for financial sentiment analysis. The sentiment analysis is used to rank the companies to help you decide which employers you would actually like to work for. A numerical score is returned for each company and you can view how a company's media reputation has changed over time.        With <3What's next for SmartCabFord's OpenXC platform: This tool will have more reliable data for:The development of a route from the geolocationThe distance traveled by the taxi (for calculating the price of the race).For that, an apache cordova plugin for android and ios must be created.Rate calculation tool for assessing know the price of a race.Voice assistant for taxis to facilitate the application use.And everything in real time (we use real time databases). So users don't waste time.Built Withangular2apachefirebasegulp.jsionic2sasstypescript      Submitted to    Ford Code for Taxicabs Mobility Challenge    Created by  I built it all.Bacary Bruno BODIAN18 years old, 2nd year IT student in Morocco.      With MeteorJS of course. We used the following standard Meteor packages:meteor-basemobile-experiencemongoblaze-html-templatessessionjquerytrackerstandard-minifiersOther Meteor packages:randomreactive-varlessaccounts-passwordaccounts-facebookemailCommunity packages:iron:routersemantic:ui-cssanti:helpersanti:modalsfortawesome:fontawesomematb33:collection-hooksmeteorhacks:npmchuangbo:cookieand application specific ones:npm-containercustom:postal // postaljs for message buscustom:midijs // MIDI playback based on MIDI.js    We built our app on top of MoClu (*), that way we can add hardware on demand to deal with tons of data for realtime flights.On the UI side we are using AngularJS that integrates with Lift 3 and Comet actors, a google maps angular implementation called angular-google-maps and D3 charts.The backend is built using 2 different web servers. One of them is used to connect to the models cluster, get and push new flights data coming from external sources also it saves results into MongoDB.The second web server is used to serve this data to the client. Both web servers can be scaled horizontally.  With laughter and rounds of failure :D The game is essentially built using pyGame library and the computer vision tasks are handled using OpenCV. We are also using a python library called keyboard to run the game successfully without concurrency issues.  We built a ANTLR grammar rule that checks a mapping of the translation and then replaces the reserved words. We have a Java program that loops into the class file and then brings it to the standard library.    A merchant or business owner requests for payment of products or services purchased by a consumer by entering the consumer's mobile number or ESR number, the cost of the purchased products or services. The consumer receives a notification with details of the request (information about the merchant, amount to be deducted). The consumer then confirms the transaction by entering his/her transaction pin.The merchant's account balance is updated and they receive a message on their phone about the transaction.Users fund their accounts by using their debit/credit cards, bank deposit or Gift Tokens.Users can also send money to other accounts by entering recipient's mobile number or ESR number, the amount they want to transfer and their transaction pin for secure transaction.All accounts are linked to users mobile number      PROS was built with the gaming platform Unity as it provides the ability to render and simulate work order operations in an immersive 3D environment. Using this platform will enable us to port this application to other platforms (i.e., browser, mobile, etc.) or run in Virtual and/or Augmented Reality.  I made a php script which is connected to a mysql database. The account linking script checks the login data against the voicemail system and if everything is ok, stores the login in data inside the database. The login is encrypted by the generated authentication token and two hashes (with different salt) are used as search id. The skill script gets the authentication token from Alexa, generates the search id to look up the login data from the database and uses the authentication token to decrypt the login credentials. Then it can query the voicemail system. The voicemessage is provided by a third script as MP3 to the audio player. This script shares the server session with the skill script. The skill script codes the session parameter to the url, which is provided to the audio player. So the MP3 PHP script gets the access token and id of the message to play from the server session.  The device consists of 2 parts - hardware and software. For the hardware, we wired up an Arduino 101 with a basic breadboard, which consisted of a photo resistor, a 10k ohm resistor, and an accelerometer (adxl345). Whenever the patient would cover the photo resistor its voltage would change and send the data to the serial monitor. Whenever a change in the voltage was detected (below 850 in the case of the photo resistor), the accelerometer within the arduino was triggered. For the software side, the arduino would run a simple .ino file and receive the (x,y,z) coordinates. It would then wait for 6 seconds and then remeasure the (x,y,z) coordinates. This data was all sent to the serial monitor. From here, a python script using the serial library would use a baud rate of 9600 to connect to the serial monitor. It would save the data points in an array (of size 7, because 2 sets of (x,y,z) coordinates) and transfer this data into a .txt file (called 'output.txt' in this case). From here, it would also automatically run another python script that would analyze the delta change in the coordinates (see above) and based upon the changes form the closest gesture/desire. This gesture/desire would then be printed onto another .txt file (called 'input.txt' in this case). From here, PHP code was called that would transfer the .txt data to static web page. This was then modified a little bit using HTML5. The patient's desire was then printed to this web page and would be available for a person to view from anywhere (our's is limited to localhost for the demo).     With a lot of motors and battery packs...        Our system consists of 2 parts: a mobile application and a web service. The mobile application is for customers. It registers the device with Klarna, scans QR codes and requests purchases via our web service. The web service contacts Klarna to perform requested purchases and provides interfaces and APIs for merchants to generate the magic QRCodes and manage payments.The mobile application was built with Android SDK and klarna-on-demand-android library. The web service was developed with Ruby on rails, materialize-css, and Klarna's payment APIs.    The Web Page is made on React, and other tools like Redux. Using the FB Login as Authenticator.It will connects to an API made on Hapi.js and mongoDB as database, where will be stored and gathered the users data and the kinds of knowledge.Then the way to connect on Messenger is through a chatbot made on Yalochat framework, than will notify you when an mentor/mentoree wants to approach you, and the chatbot give you tips about good practices.  We used the Google Cloud Speech-to-text API to parse what the user says and the pynput package to control the host machine's mouse and keyboard.    We built this web application using the technologies of MAMP, SequelPro, and Visual Studio. The languages that we used are HTML5, CSS3, Javascript, PHP and MySQL.    We have primarily used react, plotly charts and graph-app-kit to build this application.It's built as a neo4j graph application that can run inside neo4j Desktop.  The frontend is built in React. The backend is built in Express, Node, and Firebase. We used Google Cloud APIs for our NLP synthesis and Capital One APIs for our marketplace.    Physical: The hand is built from 3D printed parts and is controlled by several servos and pulleys. Those are in turn controlled by Arduinos, housing all the calculations that allow for finger control and semi-spherical XYZ movement in the arm. The entire setup is enclosed and protected by a wooden frame.Software: The bulk of the movement control is written in NodeJS, using the Johnny-Five library for servo control. Voice to text is process using the Nuance API, and text to sign is created with our own database of sign movements.  The app was divided into three parts Client, Server and Hardware. The Client part was done by Paul and the rest was worked on by me. The server and hardware was created first and then the client architecture was decided.  Pixel Kanvas is built on the Firebase platform. We initially intended to use node.js and socket.io to send information about users actions back and forth, but quickly settled up Firebase following its presentation. We were blown away by how powerful it was, especially considering the low barrier to entry. We created our canvas in HTML5, the markup in CSS, and everything else in JavaScript. We used BitBucket to collaborate together and share our work as a team.        The MVP is made as an android application.   We use ARTIK10 as the main processor in our Artik System, we integrated it to the air conditioning control board (got as much info as we could, but this is an ongoing process) to capture some data like temperature, we also incorporated sensors of our own. We then push this data to our server, that has a webpage on http://econditioner.me/ , using the MQTT protocol that is pretty common for IoT applications we get and receive data from our secure server. We can get useful information as temperature, humidity, water saved and the purity of the water and can send from our server to our Econditioner information about turn on and off signals and many features we are currently implementing (You AC unit would be able to turn itself on 30 minutes before you arrive home to greet you with a nice weather).  I started off using the MongoDB/Node.js template on Openshift. I spent quite a long time messing around with this and learning about how it worked, and how the network link between the containers was created. I also learned how the Openshift online platform worked, and how webhooks worked.Once I had all of this figured out, I deleted my sample / test project, and started fresh.I developed a very basic game menu and simple game prototype that just had a flying ship in Unity3D and got that up and running.Next, I developed a base node.js solution that was able to read from and insert values into my mongoDB instance. This part took quite a bit of skilling up, and I spent a significant amount of time learning about node.js and mongoDB here.Never-the-less, I found myself making good progress and just kept iterating on the game client and the online platform, adding an API manager to the game that was able to talk to my Node.js API. I built-in a way for the platform to register and login users, and authenticate them when API calls were made from the game. I plumbed in methods for the platform to talk to the Fitbit API and store any synchronized fitbit data for the player's on their accounts. I then built a way for the platform to communicate this to the game client.Once the core mechanic was in place, I began resolving all the game's logic. This includes things such as: player control and input, enemy spawning patterns / behaviors, attacks, player weapons, as well as the movement of random space objects and mining mechanics to collect space crystal with the mining laser.From there I added live leaderboards (total fitbit steps) for players to compete on and a way for the game platform's API to work with these, and added a message of the day service too. The leaderboards can be viewed online on the site, as well as through the game client.    We built visionOS with the help of Unity technologies and prototyping tools like Sketch. Along with the project we created the sample use cases for engaging ad campaign.  We used meteor.js to build both the front and back end of the web application. Bootstrap is also used to improve the aesthetics of the website and integrate it to be mobile-friendly. Finally, we made use of the Telegram API to create our bot to communicate with the user.      The main iOS app frontend is built in Objective-C, and the backend is written in Node.js. We realized very early on, after several attempts, that writing our own image classifier to recognize a variety of foods would be too expensive in terms of time and computational power. Instead, we decided to use the CloudSight API to help make this process a lot easier. We then used the context provided by the CloudSight API, and used the Nutritionix API to grab information about the food's nutrition data. The architecture of the app is very simple. The user takes a picture of some food with the iPhone app, which then sends a POST request to our Node.js backend (running on a Microsoft Azure VM). The Node.js backend then handles all of the communication with the various image recognition APIs and nutrition APIs. In addition, the backend also manages all user data and stores all past queries locally. The backend also aids the iPhone app in processing nutrition requests in the background, by allowing the iPhone app to send multiple images, and then receive the nutrition data the next time the user opens the up.      We simply coded in react-native using expo.io and Visual Studio Code.     I started with the Chrome extension which was built with Javascript, HTML, and CSS. The backend is a set of Lambda functions written in python. The Chrome extension sends the video ID from the browser through an API gateway which triggers the first Lambda function. The Lambda function gets the download URL from YouTube using a Python library called Pytube, the Lambda function will also send the video directly to an S3 bucket. The Chrome extension will call another API gateway to start the transcription process on the video using Amazon Transcribe. The transcription process can take a while, so the Chrome extension will check to see if the transcription process is complete or still in progress. When the transcription process completes the Chrome extension will call an API gateway which will trigger another Lambda function to get the transcription data. The function will also perform sentiment analysis on the video and return the data back to the Chrome extension. The video file gets automatically deleted after the process completes.  We used html and css to design the website. We used php and JavaScript to code the back-end.     I built the environment in Unity and viewed it through Oculus rift.  Hardware:Equipment: Coax Cable, Pringles can, RF connectors, Raspberry pi, WiFi Dongles.A high gain antenna was built using an empty Pringles can to directionalize a small custom WiFi antenna. This was then connected to the WiFi Dongle using the Coaxial Cable.Software:Used: Python, TerminalDeveloped software to collect and analyze signal strength of WiFi in order to give lines of bearing toward the strongest signal source.   A website that tracks and detects objects on the play field, and calculates the geometry of the objectsA Node.js server that gathers the data calculated by the website, calculates the movement needed of the bots, and dispatches commands to the botsAn EV3 bot with Debian installed, running a Python socket server that listens for movement commands    I used Android Studio to make the whole Android app. I used the Google Cloud Platform to do Optical Character Recognition on every picture I receive so I know what text is in the picture. I then run this text through a keyword/importance analysis algorithm to see what parts of the text are most important. Finally, I show this to the user and give them multiple options to share, etc.  We did the machining at TechShop SF and we did apps in angular.js (consumer) and ionic (bartender). The robot has a raspberry pi and everything on the server is running on docker on AWS.    We took an open source code editor and integrated and extended it into Excel using vanilla JavaScript, written in ES6 and transpiled with Babel and Webpack.  The create the central platform in mobile application.    We used Python and a github repo to use the element data.Accomplishments that we're proud ofThousands of students across the globe can now reference the element data easily.What's next for ElementsAn iOS or Android app to go along with this.Built Withalexaamazon-alexapythonTry it outgithub.com      Submitted to    Amazon Alexa Skills Challenge    Created by  Bhagat SinghAkshay Bawejahttp://akshaybaweja.comJasmine Sodhi      Our stack consisted of a JavaScript backend built on General Motors' 'Next Generation Infotainment' standard development kit with Firebase for database storage and real-time updates. Our mapping system utilizes various Google Maps APIs.   First, we came up with an algorithm for deciding how dangerous each combination of drugs will be based on the  reported untoward occurrences. After we finished the front-end with Jade, we worked on implementing the algorithm onto the Node.js server with MongoDB. We then deployed it to Amazon Web Service EC2 and linked it to our domain sponsored by domain.com.  Through Xcode  Semi-meticulous software architecture on the fly for a client-server app that I/Os to server, and on the clientside, is aware of the users face pose tensor to be able to virtually apply the products on the users face.  Stack below: * App: Unity C#/.NET 3.5 Equiv @ .NET 2.0 Subset. AReality3D RealityScript framework, primarily just RS.IO, RS.TrackingBindings, RS.Misc, RS.Rendering (shaders for makeup blending, face mask, LUT for color matching and post processing filters, etc)Tracking: OpenCV 3.4.1/Dlib 19.7 (Free, open source) Webstack: LAMP (because I learned it all starting in 1997)Cloud Storage: AWS S3iOS, Android, etc via cross platform compile WebGL via asm.js / Emscripten or Unity cross-platform compile* Gitlab for storing giant Unity projectsProducts featured from Amazon, Zenni, Shopify and other fine ecommence retailers.  Our extension is comprised of 3 parts. The front-end is vanilla (yet modern) JavaScript and is very lightweight, we have plans to upgrade this with a framework (React/Angular/Svelte). Our back-end is a Node.js server hosted on a BaaS for simple scaling and links the game to the extension. Finally and maybe the most important component is the game mod, in our case hosted on Steam Workshop, which queries our back-end for any commands and executes them within the game.  We used FlirOne iOS SDK to fetch thermal heat images and used image processing algorithm to accomplish our features.       It first starts off with all the help from 4 major stakeholders of which all currently work in some capacity to help Hawaii's  homeless problem. All allowed us time to interview them and really figure out what their pain-points are. We came to the conclusion that building a solution that would 1. store all homeless data in a centralized database owned by the state, 2. allow workers to assess and survey homeless in the field, and 3. render that information in a practical and meaningful way. We are building to solve all 3 needs. The rest just took some laughs, coffee and LOTS of programming!    We used the Shopify API to get the information about the shops and Google Maps API for the map. We thought that it had to be simple and clean. With that in mind we the user can get the information that he needs faster and can use it to improve their commerce.  We built the REST API using the LTS version of Laravel (5.1 LTS). We defined the required endpoints for the backend and split them among the team members.We don't live near each other, so we used google docs to brainstorm and to write the documentation, skype to communicate and bitbucket/git to version the code.  We simultaneously developed both android and iOS mobile apps, using Kotlin and Swift 3 respectively. Another member of our team built our web platform using Meteor.js.  We have a PHP backend with MySQL database that keeps track of the tickets, persons and rules in the company. It also receives the data from the devices and locations of the personnel with the Tieto API and uses that data along with the rules crafted by your company to automatically assign the tickets. We also built an android application for maintenance personnel, where they can see their tasks to do, specific information, location and error codes from the machine that is broken. They can also report a problem regarding the broken machine using the app. The foreman can control the flow of tickets manually, if needed, using the web dashboard. We also constructed a simple Arduino-based sensor and simulated the data generation and posting it to the backend.    Meteor from Scratch. Autoform was a great help for us. For the UI we used semantic-ui  We assembled a core team with healthcare IT, EMR, enterprise computing, clinical process, UI/UX, and full-stack software development experiences.We researched for the disconnects between current BPM and care processes.We hypothesized the shape and form of solution that HCOs would adopt.We looked at systems that have similarities to the solution we were envisioning.We spoke with more than 40 people in the healthcare industry, including front-line clinicians, Medical Directors, CMOs, CMIOs, healthcare IT senior managers and workers.We synthesized and prioritized from our experiences and learning a set of user stories / requirements.We designed and built our current demonstration system.  We used the Twitter API to retrieve Tweets for six top presidential candidates. After aggregating the Tweets by day, we create a semantic SDR representation for each day's Tweets using the Cortical.io API. These SDRs are then input into an HTM for each candidate, which learns the semantic patterns contained in candidate's posts and computes anomaly scores. We built a frontend in JavaScript that communicates with the HTM via a REST API implemented in Python to display the data to the user in an interactive web application.Built WithjavascriptnupicpythonTry it outwww.cortical.io      Submitted to    Numenta HTM Challenge    Created by  Sren Tjagvad MadsenTaylor PeerSoftware engineer and data scientistPablo Gonzalvez Garcia  Apache Spark provides a Python binding, PySpark, which is ideal for scientific data analysis, given Python's strengths in data analysis, statistics, and scientific data libraries.Data from the US National Centers for Environmental Information's (NCEI) 19812010 Climate Normals were imported to the provided Object Storage. Spark Resilient Distributed Dataset's (RDD's) are created from these files and joined, then organized through a series of map-reduce operations, resulting in an RDD containing key-value pairs of location/month key and climate normal values.PySpark's cartesian operator comes in handy to create a new RDD that is a cartesian product of the original RDD with itself, with all possible pairs (195,105,024 pairs) of keys now represented and ready to be distributed.A method was created using the numpy library to compute the Euclidean distance between each vector pair and mapped to each pair in the RDD. Apache Spark held true to it's reputation with over 195 million vector operations taking well less than 1 hour.The RDD containing the distance between each pair of vectors, or similarity between the two, was then organized through a series of map-reduce operations, with the final RDD containing key-value pairs of location/month key and an array of 13,968 similarity values.Using the geographic coordinates of each location, maps representing each calendar month of the year were created. A green/yellow/red color-scale was used with green indicating very similar (go!) conditions and red indicating very different (don't go!) conditions. Total images created are 12 images per location per month, a total of 167,616 images. Each image answers a question such as "How does the month of August across the US compare to my selected location in February?"    We used Unity and Vuforia to create the augmented reality environment. We used Flask as our back-end framework to handle all of the business logic. We integrated Interac's E-Transfer API to handle payments and Twilio's API to handle SMS receipts. Finally, we hosted our data store in Azure.      We built it using Polymer and Google APIs with resources and data from various websites    The front end of our app is done mainly in android with java/xml and the backend is done with javaspring with a database implement with SQLite. In order to provide an interactive interface for the user, we integrated the google maps API to our project; we have a map view of the nearby restaurants and venues.  We started with some sketches first, on what components we need. There was a lot of experimentation, but we decided on laser beams, lava pits, and laser shooting turrets with horse head masks.Then we started designing the concepts on the Unity platform to finally port the game on the Hololens.  The AR Watch is built in Unity with Vuforia and Google Cardboard SDKs. Unity is the main platform where everything is put together. Vuforia is used to detect a image which determines where Unity displays the AR Watch. In order to display data across the watch faces, AR Watch made REST calls to various APIs. The map data came from Bing while the weather data came from OpenWeatherMap. Want to try?You can download the .apk below for android. It will work with this image:https://drive.google.com/file/d/0B-LMvLFomXHHNGJNUFRGX2ZGT2s/view?usp=sharingBuilt Withc#google-cardboardunityvuforiaTry it outdrive.google.com      Submitted to    WearHacks Kitchener Waterloo 2016Winner                Third Place              Winner                Top 5 Projects                  Created by  Attempted integrating Vuforia, Unity, and Cardboard. Used REST APIs and parsed JSON to get data for AR Watch. Worked on UnityIvan YungComputer Engineering student at Western UniversityI was the secondary programmer, participated in pairing on using REST APIs and system commands to get data for the faces, did some unity UI creation and management.Devin OwensVuforia Unity integration, and implementing an interactive user interfaceCara YungI worked on Vuforia\Unity integration and good target detection.Dustin Firman  Our Virtual Reality simulation is built in C# in Unity for the HTC Vive. We used Firebase as our backend service, as well as the IBM Watson API and Unity SDK for emotional analysis. We used the Nexmo API to send user feedback directly to their therapists, so a medical professional is always being updated.            Backend: Ruby on Rails framework,  ruby gem, Frontend: React, Design: Sketch.app .    , i.e  notification-   email delivery, weekly subscribe  .Accomplishments that we're proud of        :)What's next for Bundle          .Built Withreactrubyruby-on-railssketch      Submitted to    HackTBILISI Fall 2015Winner                VereLoft 1 month  membership              Winner                3rd place                  Created by  Back-end DevDavit KhaburdzaniaUI/UX Designer. I used SketchApp to design Mobile and Web Interface.Giorgi TabidzeBack-end DevelopmentGeorge ChkhvirkiaFrontend Devlevan gulisashvili    Built on Ubuntu using NetBeans and Atlassian SDK.    Our landing page was developed with the following libraries and technologies: Knockoutjs Bootstrap VelocityJS jQuery Python NodeJS AWS Google Cloud PlatformLink for AWS: linkLink for GCS: link      Hard...Please, see detailed description in our main repo.https://github.com/HaySayCheese/EthSC_GEO_ETHBerlinIs it works?Screenshots of nodes operation: https://drive.google.com/open?id=1eEy4VK7TaiqKc1DvKqFQKwFr3-MI4gvqAlso, please, check real-live demo (access rules are descried in the main repo).Where is your UI?Oh, common! We provide low-level protocol solution.Accomplishments that we are proud ofBi-directional fast non-lockable state-channel for the eth, that supports efficient, and predictable assets exchange with ethereum network.Tiny, but strong and predictable communicator for GEO node, that is able to mirror node's states to the Eth. network.Ability to perform fast p2p payments between up to several hundreds of participants involved into one operation, in atomic and time predictable manner.Routing algorithm;What's next for Crosschain transfers through general-purpose L2 (eth)The next goal is to add ERC20 support and to create also a BTC-connector and to perform maybe first eth<->btc atomic payment through L2 network.Built With.netbashc++javascriptpythonsolidityTry it outgithub.com      Submitted to    ETHBerlin    Created by  I worked on State channel smart-contracts and middleware for connectionKoroqeDima Chizhevskiy  Front End:We used ReactJS for the front end.The dashboard was designed with with p5.js and JQuery was used to make the animations.We deployed the website using AWS Amplify. Real-time communication was established using Pusher Channels. Back End:We used Google's Speech to Text and Text to Speech API to map accents. We used LSTM to do perform Speech to Speech mapping. Google Speech APIs provided us with information about the timestamp for each word in the audio. This helped us to determine the duration and the location of pauses in speech, and the speaking rate to generate a Speech Synthesis Markup Language(SSML) file. The SSML file was converted to the target accented speech.We then synced the target audio and the video and sent it back to be displayed.  Foo Blox was built in C++.  It uses OpenCV to process a webcam feed.  The blox are parsed into Python which is run using the Python/C API. The UI is written in HTML using the Awesomium C++ SDK.What's next for Foo BloxThe first step will be launching a kickstarter campaign to get Foo Blox out to the world. The next step will be setting up a web app where users can add their own level packs.Built Withawesomiumc++opencvpython      Submitted to    HackingEDU    Created by  Dan PoindexterRetired.  We build this progressive Web App using various technologies, such as FOSSASIA Susi AI, react.js, node.js, Amazon-DynamoDB, html5, css3, javascript, NoSQL Database, PWA, and open data!  At its core, our Dank Meme Generator is powered by an ML model created using Google Tensorflow. Our meme-generating model uses a convolutional neural network to extract features from images, and long short term memory (LSTM) units to model and generate captions. To train this model, we crawled the internet for hundreds of thousands of memes, using a python-based web crawler as well as integrating existing datasets. Tesseract OCR was used (with limited success) to derive the text from meme images. For users, this model is exposed by our webpage, built using Flask.  We designed the basic structure, based on the CubeSat specs provided by California Polytechnic State University and used by NASA to send low cost satellites.We printed the structure by means of a couple 3d printers.We handcrafted all electronics by using a combination of 3 Arduinos, which required us to search for low consuming components, in order to maximize the battery power, we also work on minimize the energy consumption for the whole satellite.We opted to use recycled components, like solar panels, cables, battery, converter...We worked a lot on the data transfer part, so it allows the Sat to be sleeping by the most part, on an effort to increase even more the battery life.And almost 24hours of nonstop work and a lot of enthusiasm!!  The app interface was built using Swift and Cocoa, and theres a great deal of state-of-art sound tech underlying this simplicity: it is powered by using Fast Fourier Transform, Harmonic Analysis, and Natural Language Processing, and we rendered the score transcript using LaTeX and Lilypond.The voice recognition is done through Google's API, and we ported an algorithm for breaking words into individual syllables, Hyphenator from Python to Swift2, and used it to generate lyrics with articulation like 'Lon-don bri-dge is ~~~~'  Data from the insulin pump and CGM is read using a third party android app which stores the data in a mongoDB. I wrote a python script to read the mongoDB every 2 mins and extract the data and send it to the Azure IoT Hub. An Azure function listens to the Azure IoT Event Hub and stores the data in an Azure SQL Database. Every 10 mins another Azure function reads all the data in the Azure SQL server and executes a stored procedure to create the BG forecast. If the predicted BG is too high or low the Azure function sends a message to a Azure Storage Queue which in turn triggers another Azure Function which sends a text message via Twilio service. Bulk of my time was spent in building the stored procedure which creates the BG Forecast which is currently rules based but could be expanded to leverage machine learning.    We designed EyePhone using a bottom-up approach with two separate modules, one targeting the Machine Learning and the other Android Application development. After performing unit testing of these individual components we did a system integration to launch an android application that detects cataract. Since the machine learning component required the data to be in a better quality, we used some image enhancement techniques to preprocess our data. We could not get any annotated data online, so we collected the data ourselves from Google Images. We took a dataset of 60 patients (30 cataract infected and 30 normal healthy people). We divided this dataset into 40, 10 and 10 patients for Training, Development and Testing purposes. The most important feature for this study was the whiteness of the small ring of the eye. In Cataract images, the inner surface of the cornea is more whitish as compared to that of normal images. We also took into account the Big Ring Area of the eye. In cataract images, the outer surface of the cornea images is bright in color as compared to that of the normal patients' image. Also, we took into account the Edge Pixel Count. Canny method is the most powerful edge detection method among them. It uses two different thresholds (to detect strong and weak edges), and it includes the weak edges in the output only if they are connected to strong edges. In the computation of EPC, we counted the number of white pixels in the output of the edge detection. Now, with all these features, we chose the Unsupervised K-means clustering algorithm as out baseline Machine Learning algorithm. The reason behind that was having access to only non-annotated data. The algorithm divides the datapoints into two sections (or clusters) which are separable. The test datapoint can be incorporated with the cluster to which its closer (Euclidean distance). From the Android side, we used Material Design concepts to bring together a clean and sleek UI that helps to minimize stress when dealing with diseases. We provide educational materials along with out ML backed algorithms to deliver information in a straightforward manner. Users would get to know more about how to deal with cataracts. To ease users into the app, plenty of animations were implemented to guide users around the app, making sure that buttons are reused and minimal learning is required.    We used the Adafruit PN532 RFID reader breakout board with an Arduino to read the unique UID code associated with a RFID tag. We transfer the ID# over serial to a python script that identifies and opens the corresponding medical record.  Super Creature Showdown was truly a family effort. My wonderful wife, Katie, teaches 2nd grade, and her insight into education was a huge benefit in helping make the game fun, educational and age appropriate. Certainly Jack was a huge inspiration, and even our 2-year-old, Luke, got involved.We built the game by starting on paper and working out the game logic and pacing. I then moved into coding using the Alexa Skills Kit SDK for Node.js. The SDK made it really fun to develop for Alexa, and leveraging Amazon Web Services made everything fast and efficient. I opted to have an external config file hosted on Amazon S3. This allows me to easily add new Showdowns without having to republish the skill.          We mostly used python for the backend with the fiscal notes' API  and django/html/css for the frontend.  Blood, sweat, tears, and solder. Shout-out to the HackIllinois crew for letting us use one of their Nerf blasters for the proof-of-concept.  We've been working on this project during this hackathon and made some initial functionality working. There's plenty left to do and there are a lot of ideas here.A list of packages we used:jqueryunderscoreaccounts-passwordaldeed:collection2iron:routerraix:handlebar-helpersmatteodem:easy-securityfastclickzimme:iron-router-activemanuelschoebel:ms-seobrowser-policynatestrauser:animate-cssdburles:collection-helpersmeteorhacks:fast-rendertwbs:bootstrapian:accounts-ui-bootstrap-3ecmascriptstandard-minifiersmeteor-basemobile-experiencemongoblaze-html-templatessessiontrackerloggingreloadrandomejsonspacebarscheckmomentjs:momentaldeed:autoformunderscorestring:underscore.stringBuilt Withjavascriptmeteor.jsnode.jsTry it outpelorus.meteor.comgithub.com      Submitted to    Meteor Global Distributed Hackathon     Created by  General idea and developmentAlex PletnovYara MorozovaMikhailo Ostroviy    Confucius is composed of three primary components: WEB SCRAPER, CLOUD BACKEND, iOS USER INTERFACE.WEB SCRAPER:     In order to populate our cloud implementation backend with text articles, we had to create a web scraper using Kimono Labs technology. Written in JavaScript and Python, we were able to crawl multiple websites ranging from BBC news, The New York Times, to the Onion and National Geographic. Using Heroku, and cloud logic, we are able to automate this web scraping process to grab the latest articles at regular time intervals. That way, our cloud database is always up to date with the top most recent articles.CLOUD BACKEND:     We implemented our backend data storage with the Parse API, a cloud database implementation. Following web scraping, we interfaced this database with iOS querying methods to easily write to and write from our cloud database. Most importantly, this database enabled us to populate our front end user interface with all the most up to date articles, hottest news article playlists, etc.iOS UI/UX:     In order to take advantage of our solid backend, we had to build a front end capable of reflecting all the information in the cloud to the user in a simple and intuitive way. Every part of our UI/UX has been thought about, and we have taken validated User Interface tips from other top mobile applications.     We decided that a Tab bar is the most straightforward and intuitive way to show all the pages in our application, taking only one tap to change from page to page. On each page, useful indicators enable users to intuitively navigate our application with satisfying swipe and tap gestures..  Using HTM.java, and Swarming to find optimal configurations.    We built it using a Haskell web framework called Yesod.  We took a survey to determine the correlation between skills, hobbies, talents and preferred careers.Using this data we built a platform that uses an algorithmic method to assist people in choosing their careers without much confusion or bias.  We have used UiPath Orchestrator APIs to build the framework. A new transaction for a process is triggered by creating a transaction item in the master queue. A master robot runs non-stop as back-end process checking for requests in master queue. The master bot check the back-end configuration to find out robots which are configured for the corresponding process and dispatch the transaction and wakes up a robot based on its availability.  We depended a lot on Google APIs related to maps and directions, notably Places API and Maps JavaScript API. By first parsing the JSON obtained from using Google's Distance Matrix API, which allowed us to estimate the time needed to get from the origin to the desired destination. Following that, we implemented a map and an autocomplete search query in order to get a visual feed of the directions. The complicated part was to use the PlaceID for both the origin and the destination generated by the search and using it for our Distance Matrix functions. Implementing geolocation was the next step to properly centre the map when it loaded. We then added a timer, which was the main focus of our project. It was necessary to start the timer when the directions were submitted, and to send out an alert via Twilio if the user failed to arrive in time at their destination.   The backend was built on AWS lambda with NodeJS and the schedule data gets stored in AWS DynamoDB. The frontend was built with HTML and Query and a CSS library called Bulma.  We started off with a simple elastic search cluster that was built out for the labs group, we built out some sample queries pulling from our orders, customers and loaded those in with kafka into logstash, and then into elastic. We also wanted to prove out the pipeline to move data from our big data HDFS clusters into elastic and built out a system that could geo-tag search IP's, track the products they clicked on, build up rudimentary search analysis, and then loaded the data directly into elastic search using python, and the elastic hadoop plugin.    Before rendering the images on the web page, we take its URL and check with Clarifai's API for the tags it can be associated with. Then we match the image tags with a pre formed list of bad-words indicating adult content. If flagged, we redirect this image load request to a pre set safe image.   Mercury is an intelligent IoE application which used vehicle sensor data and Alohar API features like as an input o Arrival and departure events for any place o Address of place o POI name and category of place o Change of location or movement within a location o User's current motion state (fast, slow, stationary) o Arrival time, departure time, and User-Stay duration o Places near user's current locationAnalyze these data to generate different emergency events on the road and trigger SMS, email, push, voice, chat, web, social, & rich messages to near entities on the different emergency conditions.It introduces a revolution to search near field entities (Geo-spacial search) and collaborates service providers, insurance companies, hospitals, res-ponders, victim using real time communication as a service. Our Call control mechanism route the call to connect the entities anywhere anytime. If needed, the archive of the multimedia live recording can be accessed later for reference in investigation and predictive analysis. It also provides social media sharing and interlinking to improve the helping nature of humans psychologically to build a better, safe planet.  c9.io + git for developmentjquery and bootstrap for frontendnode,js, express, multer, passport, NeDB, SSSA-js, file-encryption, keygenerator and many other modulesAdobe photoshop and Illustrator for design  We wrote the bot with Node.js and Telegram-bot API, with the help of Firebase SDK.  We built Telehope's app using Marvel, Adobe Creative Suite, and Brandquiz. For the website, we used HTML/CSS and Bootstrap.    I built game with Unity which is a popular game engine for 2D and 3D. This game is 2D and all the codes are written in C#.  Trevor Wilkin and Jon Fuller/Director of Customer Support were the driving force of the first two iterations of the TruClinic platform. The first version was built on a LAMP stack using Javascript. Version 1.0 used a Flash based video codec. TruClinic was an early adopter of WebRTC for the video codec which was rolled out in Version 2, along with moving away from Javascript and into Node.js and Angular. Version 3 (code name: Olympus) is built on Node.js, Angular, React and Typescript.     We created bot commands using Workato recipes, which is a tool for writing scripting logic for the Cisco Spark commands.Accomplishments that we're proud ofBeing able to consolidate data from Salesforce and turn them into charts within the Cisco Spark chat console.The ability to handle ALL Salesforce customizations including all custom objects and fields.The ability to perform useful actions in Salesforce within the chat console itself so context is not lost.What's next for Forcebot for Cisco Spark?Right now, any actions taken with Forcebot would be attributed to the person who installed the bot. We'd like to enable a feature called Verified User Access, which allows the ability to attribute user actions based on each individual person, instead of attributing all to the person who installed the bot. When a user wants to take an action via the bot, he/she will be required to authenticate his app credentials and the bot will attribute the action to the person.Built Withcisco-spark-apisalesforceworkato-platformTry it outwww.workato.com      Submitted to    Cisco Spark the Industry Challenge    Created by  Zann Yapryankohycrishidebnath  With love, Photoshop, a printer, and some scissor/tape skillz.Accomplishments that we're proud ofAttaching it to the cart went pretty smoothly.What we learnedHack day projects don't have to involve code!What's next for Brian Koles Memorial Sandwich CartThe sandwich cart will continue to be an integral part of lunch time activities, and live on forever in our hearts.Built Withbrianlovephotoshopscissorstape      Submitted to    Devpost Hackathon 2015    Created by  Holly TiwariProduct Manager & Designer at DevpostStefanie MaccaroneHello!  Backend server:opencv: for images analysis and faces manipulation and extraction.nodejs: the backend is written in nodejsmongodb: database where we store all the datasentiment: npm package to analyse sentiment of a string, in our case of a tweettwitter api: to get the data, and post the replies of the botsocketio: to comunicate in realtime with the frontend client.frontend:angularjs: the logicsocketio: realtime communication with th servervisjs: for the nodes network mapgoogle mapsBuilt Withangular.jselectronjavascriptmongodbnode.jsopencvsocket.iotwitter      Submitted to    HackUPC Winter 2017Winner                HackUPC - 1st prize                  Created by  arnau codearnaucode@gmail.comMax Max    During the development, this application is designed using MIT App Inventor supported by google map to detect other users location in the circle.In the future, we hope this application can keep being innovated and having high selling point. Again! This application can be used by everyone both personally and in community.    Guru is a native iOS app backed with a nodeJS powered Parse Server, a popular open source deployment framework. We host our server on Herokus powerful infrastructure which allows us to easily deploy new server side changes via a git command line and manage performance in an integrated environment. For the client side languages, most of the app is written in Apples Swift 3.0 with Objective C libraries as well. Cocoapods was our dependency manager and proved to be very useful in organizing the external packages we used. For live video streaming/conferencing, we used Twilio to deliver fast and reliable content streams connecting the Guru and student. There is native Twilio code running in the app, as well as a server component hosted on our backend for authenticating users with the service before granting them access to a session. The real time whiteboard that is provided during a call is powered by the open source Live Query server project which facilitates web socket connectivity for efficiently sharing coordinate points across devices to create the effect a continuous line is being drawn live. A separate portion of our Heroku instance (dyno) manages this live websocket system. We use a MongoDB database hosted on MLab which has proven to be fast and very adaptable to our needs.  IBM blumix Node-RED starter (Including node.js, cloudant NoSQL Database)Twilio (TEL/SMS etc.) API    Incorporating Adobe After Effects and the Unity game engine, we used C# scripting to combine the best of 360 degree imagery and speech feedback.  We created sensors by ourselves - they consist of tube, photo-resistors and diods. Measuring the resistance on the resistors we can determine the angle of the person's joint.Since we don't know exact physical formulas we used Machine Learning and learned our computer to properly interpret the sensor reading.        FourPly uses custom OpenCV image processing for our flagship feature - virtual AR graffiti.  The rest of the app uses Parse for data storage and Firebase for real-time chat.Challenges and learningQuality AR in bathrooms is challenging - everything from different lighting conditions to different stall door materials to complex tile patterns makes pattern recognition challenging and computationally taxing. This is an area we'd like to learn more about to improve the performance of FourPly and future machine vision projects we embark on.What's next for FourPlyBathroom checkins, badges and achievements, improved AR functionality, who knows. Don't forget to flush! Built Withandroidfirebaseopencvparsepostmates      Submitted to    PennApps XIIIWinner                PennApps XIII Humor Route Prize                  Created by  I built our AR Graffiti feature using C++ with OpenCV and a JNI interface to access the AR functionality from within our Android appVinnie MagroHi, my name is Vinnie. I am a junior at the University of Southern California studying Computer Engineering & Computer Science.Vincente CiancioJames CarrDavid CarrSoftware engineer at Periscope/Twitter. Previously at Twitch. USC alum.  I built everything in Docker containers hosted on my own digital ocean server. The frontend is a static site that interacts with a GraphQL backend that is also hosted on my DO server.TechnologiesNode.js React.jsGraphQLmySQL Prisma      I build the code using Node.js where I used REST API services and a self generating code body for handling follow-up questions. I built the code through Jenkins and Deployed it to AWS Lambda. I used a simple Atom Editor to code and my testing environment was EchoSim.io and the App on Android Playstore called Reverb.    with html5 and web technologies, and technics learned with the facebook scholarship for web development.  I used ASK-CLI and Node.js to develop the skill.  We used Node.JS and Angular.JS to create a server that serves UnicornLabs. Additionally the server has the capability to generate based on a set of instructions the necessary server to output and provide it as a download.    Sumerian, Amazon Polly, AR Core    The front-end was created with angular, typescript, and bootstrap; the back-end was developed using C#. ConnectOnCommute is hosted on an Azure server.  We created a Node.JS server that runs on Heroku that serves APIs and dynamically loads webpages. We then built a custom SMS Gateway (using Bash and Python) on Google Cloud App Engine to send mass daily announcements and receive subscription texts  The facial recognition software is based on an open source python library called Openfaces. In order to successfully build and use it, we ran the software in a virtual machine called docker. The web interface backend interacts with Python through Tornado.We also used CockroachDB to store metadata for each painting, and how similar each painting was (for reinforcement learning).  The client side uses React.js and support libraries, such as Material-UI and styled-components, alongside pure CSS. We use Reacts routing to handle requests. We also used Axios to asynchronously fetch data from our API. Adroit is deployed on Firebase.The server side, hosted with PythonEverywhere, uses Python Flask to respond to requests for content analysis, which indicate the query target of the analysis and the amount of content to be analyzed. Query results are filtered using Google sentiment analysis and passed through an entity extraction step to determine a set of potential user concerns, which are returned alongside the analyzed tweets. Major challengesAs new hackers, we faced difficulties with environment management across the team, particularly in managing different Python and React setups. We also got the chance to work with some tools which were completely new to us, including complex APIs such as Google Cloud Language. And finally, we learned some new things about familiar concepts - for instance, we learned the hard fun way that single quotes arent accepted in JSON strings to JavaScript.Major accomplishmentsWith little experience and all being first-time MLH hackers, we're proud that we were able to integrate the frontend with the backend, gain an understanding of Google and Twitter APIs, and create a fully-working demo. With only a small amount of starting experience, we hacked together a cohesive and useful product. What we learnedUsing and maintaining web APIs. Adroit's backend is its own small API, and uses both Twitter and Google's own developer APIs.Principles of NLP in context. For instance, we considered how to group text in order to get the most effective sentiment and entity analysis.Design and adaptability as a team. Our team members were nearly complete strangers to each other two days ago, and Adroits development was tied to the process of learning to work with a brand new team.What's next for AdroitImproved insight and summarization In addition to the stream of complaints, we'd like to add better capabilities to analyze the complaints as a whole.Better concern parsing. The determination of major concerns is a beta feature and does sometimes return less-than-helpful terms.Broader data sources. Adroit, as a demo, only draws from Twitter. However, its structure is such that functionality for Facebook, Instagram, or even more exotic sources, like Youtube captions or academic articles, could be easily added.Built Withaxiosbootstrapflaskgoogle-cloud-languagejavascriptlanguagematerial-uimaterialuinatural-language-processingpythonreacttwythonTry it outgithub.comgithub.com      Submitted to    YHack 2019    Created by  I worked with Paul on the front end. I used ReactJS with HooksAPI to manage state and make API calls. API call was done with axios to work asynchronously. Components were created using pure CSS and styled-component .Hung NguyenI worked on the frontend and reading JSON from API calls, project management, and coordination between teams. Made sure we were on time and on shedule, getting enough sleep, delivering, and thinking through things properly, making sure everyone expressed what he or she believed the best actions to take were. Love this team and so proud of our first hackathon demo.Paul RinaldiI worked with Nikita on the backend, including the API which interfaces with the frontend, in order to the gap between Adroit's NLP processing and the user experience. I also developed the subsystem to pull data from Twitter.Miles KrusniakI'm a CS student at Yale interested in machine learning, technical writing, and intelligent software use. I worked on predicting the sentiments for the given tweets (reviews/statements), filtering the negative sentiments, use them to extract the entities that might have caused the grudges and find out the top 20 concern areas.I used Python and Google cloud language APIs - specifically Sentiment analysis and Entity analysis.Nikita Soni  Lots of googling and tutorials    I'm working on the 3d printed case, and it'll house a Particle unit that's either connected to lots of leds or a few lcds.  With Visual Studio as a Office App    Using GULP to automate tasks, I've created all using browserify so I could have a more structured code to maintain.   ** Client Layer **This is a slack client, It is hosted on a server and listens to chat messages in the #general channel of Slack. It looks for certain keywords being spoken. When it finds keywords it is looking for it takes an action*AI Layer - HTMS *For this layer I have built and HTM server, the server is always on an allows for state being maintained in between interactions from the client layer. There is also a HTM client , that connects with the HTM server . This way the client may start and stop any number of times while the server in always on. The server is has a model defined via xml. This specifies the design of the HTM model. It has encoder, spatial pooler and temporal memory . There is also a new generic encoder which was built. The generic encoder is able to extract topological information from text. This allows for a robust representation of natural language which can be high in noise and error rates. Data is fed in the form of CSV files, The model trains itself on each line of data in the CSV file and tries to make synapses with information it seen in each frame of data represented in a line. All of this data is stored in the HTM model which maintains the state in the form of synaptic connections between columns of cells in the HTM memory.Below is a picture of the training process. See the Raw value, Encoded value, and the predictions .Technology StackNupic, Python, Django , Custom Encoder, Json etc*New Resources Developed *New encoder that captures information about textual dataNupic server and client that can maintain state in between calls from the userHollybot for interaction and visualize the knowledge graphIntegration with data from IMDB and OMDB about movies and movie related information    We used .NET to scrape the data from Hounston's 311 and 911 data, inserted it in a database, and used Node.js and MongoDB Geospacial queries to search along the route we get from Google's Directions API. All of this is then displayed on a map in the browser.      CAD Viewer is a mashup of a three.js STL loader demo, the tracking.js color tracking demo, 2 STL models (finger wrench & TSA master key 001), and some very hacked together JavaScript. Yes, the model on the right is a TSA Master Key. And no, you're no longer in control.InitializationFirst, I used dat gui to create a small interface for selecting a model and setting it's initial position, rotation, and scale. This allows you to make sure the models looks good on screen before you start playing with it. Three.js & some custom loaders (not written by me) take care of all the shading, mesh generation, and 3D stuff. Dat GUI tho!Color trackingUsing tracking.js, I setup a color tracker that targeted any magenta objects seen by my webcam. Using the tracker's x/y position, I continually adjusted the camera position:// objX & objY are coordinates set by the color tracker// this camera positioning occurs within the render() function, which is in turn called by animate(), which is a continuously updates.camera.position.x = (objX - camera.position.x) * 0.001;camera.position.y = (- objY - camera.position.y) * 0.001;                  Bedtime Hero is built on Voiceflow and uses the APL authoring tool.     We built using the following technologies;Language: Html, Css, JavascriptFramework: Vue Js, BootstrapDatabase: FirebaseDesign tool: Figma  using nodejs and cloud technologies.    This project was built using .NET, Angular 5, and Bot Framework. It connects to Microsoft Graph for Azure Activity Directory data and uses Bot Framework to connect to Microsoft Teams.  To-Do Bot is a Node JS app using the Express framework. Data is stored in Mongodb and encrypted using AWS KMS service. I use Bitbucket as my repository, Codeship as my CI along with AWS Beanstalk to set up my servers.  I used Node.js (with the help of Passport.js, Express.js, and RethinkDB) to create Notus.    The current setup of the simulation is an Okuma OSP-P300 on a LAN with the NC.js server.The Okuma feeds the MTConnect data to the server which then parses and uses it to drive thesimulation. MTConnects PathPosition tag is used to drive the position of the tool duringruntime, while the GCode block numbers allow us to determine the active working step anddisplay toolpaths relevant to the current operation. We also use MTConnect to determine thefeedrate of the tool. In the future we plan to compare it to the expected feedrate values forerror analysis. The backend uses Node.js and the frontend uses React.js in order to parse the MTConnect and STEP data that is attached to the live server.    We decided we wanted to use Google's Polymer. The HackerCard itself is defined in it's own HTML file that a client can load in and then a custom web component becomes available on the page. You can embed a hacker card in just 3 lines of code. This makes it portable and flexible.<script src="components/bower_components/webcomponentsjs/webcomponents-lite.min.js"></script><link rel="import" href="hacker-card.html"><hacker-tile user-name="MGerrior"></hacker-tile>    I started from atlassian-connect-express template and reused some concepts from our other JIRA Cloud add-on - Agile Poker for JIRA. Application is now hosted on Heroku and templates are stored in Mongodb (Heroku app). I use Vuejs library and webpack to build frontend.  NEATVIBEwear is an integration between a personal noise capture device built using an Arduino 101 with the Grove Loudness Sensor (with the potentiometer for adjusting gain and a wide frequency range) and Android Smartphone running the gaming platform Unity as it provides the ability to render a representation of the real-time sound in a 3D color coded simulation. The Arduino 101 and the Smartphone are integrated through the Arduino 101's on-board Curie Bluetooth Low Energy (BLE) capability.We use the standard formula (20*log10(V/Vo) to convert the electrical signal from the loudness sensor to dBs and then use additional mapping and smoothing calculations to calibrate the dB readings to external digital sound level meter (SLM) readings. This approach is ideal and practical for showing both NIOSH and "Health Risk" scales of noise levels of exposure.  Turnstyle UnitA magnetic contact switch is used in conjunction with the Arduino 101s internal motion sensing to determine when the door opens.The precise angle that the door makes with the wall is computed using the Madgwick Filter.  When the door is opened beyond a threshold angle, the ultrasonic sensors on each side of the door start firing.  Depending on which sensor detects motion first, the directionality of people travelling through can be determined.The Arduino 101's internal BLE capabilities communicate with the mobile application.The Serial output of the Turnstyle is sent into a Node.js server to plot the results in real time.Any of these variables-- the threshold angle, the distance at which the ultrasonic sensors maximally operate, the directionality of traffic, can all be tuned to match the specific needs of each door. Real Time PlottingA Node.js server uses streams and plotly's streaming API to generate the plots in real time.  It parses the population data from Turnstyle's serial output and creates the plots on the fly.Mobile ApplicationOur mobile application is built using Evothings, a platform designed for creating IoT applications.  Evothings allowed us to write applications that could be run on Android, iOS, and Windows Phone in pure HTML/CSS/JavaScript.  We used Evothings's BLE libraries to allow the application to send and receive messages with the Arduino.  Using a raspberry pi B+, we created a script in python that would recognize all keyboard characters (inputted as a string) and output the corresponding Braille code. The raspberry pi is connected to 4 circuits with transistors, diodes and solenoids/servo motor.  These circuits control the how the paper is punctured (printed) and moved.The hardware we used was: 4x 1n4004 diodes, 3 ROB-11015 solenoids, 4 TIP102 transistors, a Raspberry Pi B+, Solarbotic's GM4 servo motor, its wheel attachment, a cork board, and a bunch of Lego.    Unity, our tears  The team leveraged years of experience in areas like NoSQL distributed databases and infrastructure and webapp development. The platform's MVP was originally envisioned in AWS but implemented using Kafka/Spark/Cassandra due to our familiarity with that stack. We finally ported it to AWS for scale, availability and as a stretch goal.  The application is developed using Android Studio IDE and the backend services are hosted in  Amazon Web Services.Service Oriented Architecture is employed while developing this application. Each component in the app is loosely coupled which makes the  maintenance of the app very easy. Following is the architecture of the project.ArchitectureImageREST APIs to communicate with server:The app makes the REST API calls to the server in AWS to perform various operations like to get free food events. to get the all the dishes available, to upvote the event, to post the free food events on campus.Database Schema:The database server is also deployed in AWS cloud. The main tables that are present in the database are user_table, items_list_table, freefood_events_table, upvotes_table and places_table.  Used Technologies and Software:Java, Android Studio, PHP, XAMP, Mysql, HTML, CSS, JavaScript  I built it using MIT App Inventor for frontend and PHP-MYSQL (Silex framework) for backend.I also build a testing tool to test interaction between taxi drivers and passengers, and to simulate gps positioning      We used Android Studio and programmed the application in java. The application accesses multiple data sources from the list provided for the event. -SmarterRoads was used to access yearly crash data as well as toll pricing on I-66. -WMATA API was used to access bus and metro routing and fares.-Google API was used to find alternatives using different modes of transportation and road segments that would bring the user to their destination. -Open Street Maps was used as a reference and source for locating bikeshare and parking locations.   We have used Office AddIn, Microsoft Graph API for Groups,Outlook,Planner,SharePoint and Teams to integrate Outlook, Office 365 Groups conversation, Planner/SPO/Outlook Tasks, Teams. What's next for Office 365 CardsExcel,PowerPoint,Word Add-ins to show document's related card informationChrome,Edge and Firefox Extensions to show website's related card informationSupport activity logs in the boardSupport quick actions from the board such as Send email templateSend approval requestBuilt Withangular.jsdotnet-core-2.0microsoft-graphsharepoint      Submitted to    Hack Productivity 3Hack Productivity 4    Created by  NarasimaPerumal Chandramohan  Inspired by iGlobe CRM Outlook Add-in, we designed the Team app in a similar design. It is importnat for us that the user gets a UI the can recognize and feel confident with. Using the Microsoft Office 365 Security model and handeling the access controlled by the SharePoint site where the CRM users are granted access.  The Add-in is using the Authentication model and make sure the user has a valid token. Using the Microsoft Graph API the Add-in is interacting with several services on Office 365. Due to limitation in Fabric UI we had to combine bootstrap.     The iOS application was built using swift in Xcode, and the ramen cooker was built using the NodeMCU wireless micro controller, Arduino IDE, servo motors, cardboard, duct tape, innovation, grit, and twelve cups of coffee.    We started with farmer interviews and data exploration.  We tried to see what questions actual farmers are asking that we could answer from the data provided by the USDA.  The number one issue that all of the farmers mentioned was water.  We decided to combine the USDA data with a dataset from National Oceanic and Atmospheric Administration (NOAA) that contains precipitation and temperature data. Architecturally the application is built using using Javascript, Node.js, React.js, Dc.js, and the google-map API.  We use a combination of Bower, Node.js, and Gulp to test package and deploy the application.  We tried to keep the entire application client side to simplify testing and development.  We used github.com to host the source code during development.  The project is public for anyone who would like to look under the covers.Challenges We ran intoConstrained DataWe were constrained by the limited data in the datasets. For example, yield data was only available from the surveys that occur every five years. The activities data was not available in the granularity that we wanted and it is in the form of percentage of land on which each activity was performed over a year, rather than area-specific data about where herbicide was applied to specific plots of land.TimeGiven more time, we would have been able to implement visual details we had built into the planned experience using hi-fidelity wireframes that added an element of fun and polish, such as using a raindrop and leaf for the rainfall and yield graph.We would like to explore more external datasets and pull data out of them to make meaningful connections.  We would like to compare the temperature data from the existing NOAA dataset to yield data in the application.Behavior ChangeIn terms of behavior, farmers dont have the time to create or maintain online presence, so despite what we learned about the need to share experiences more efficiently in a more targeted manner online, we would have had to tread carefully with such a solution. Additionally, many farmers initially decided to farm in order to get away from the computer. Therefore, any technological application we build has to distill the data into digestible and actionable chunks that require minimal time to analyze. Where possible, we analyze for the farmer and point out information of interest.ScopeSome of our most compelling ideas were out of scope for this project because they would have required us to use machine learning techniques to identify and recognize patterns from existing data and make recommendations for useful courses of action for the farmers. Other ideas, such as creating a social platform, would require building an active user base overnight.Accomplishments that we're proud ofWe reached a wide spectrum of farmers and began to really understand their workflow, motivations, and challenges.We addressed both the hackathons challenge and farmers needs:An important part of sustainability is keeping food production and distribution local. Our map-based application appeals to farmers by showing them exactly what is happening on their land and in the surrounding local community. The Farmed application helps them understand their land holistically, using history as a teaching tool, so that they can make decisions that support increased productivity.Weather is the number one challenge cited by farmers. Our application arms them with historical weather data that they can use to plan crops for projected drought years. It is flexible enough for us to add real time weather predictions for short-term planning as part of future work.The Farmed application focuses on a specific type of farmer and is designed with him in mind. We ensure that the farmers interaction with our application will be enjoyable and to the point.What we learnedAn Aging PopulationCurrent farmers are aging out, resulting in a loss of knowledge. The US may soon see a lack of people growing healthy food close to home. The US is losing farms at an alarming pace, and the majority of farm subsidies go to animal feed and sugar, not healthy food for people.Sharing ExperienceFarmers, especially the young ones who are just starting out, cannot easily support or learn from each other, but its important to them to be able to share information, new approaches, and advice on how to respond to different challenges. They need to make decisions such as when to plant and harvest, what crops grow well in their area, what works with regards to soil type and weather patterns, and how to deal with adverse conditions. Farmers primarily plan their crops by spending time researching online or they get advice by talking to other farmers at farmers markets. They would like to receive information in the moment, but people are busy, and finding the right person to talk to is all about being at the right place at the right time. One way to facilitate sharing would be to combine farm journals and weather data across people and locations. Finger to the WindWe are trying to quantify things people arent used to quantifying. These things usually go by gut feeling, such as gauging soil moisture. Everything is in the farmers head, and they make decisions based on their experience. Farmers learn from their mistakes by reviewing their farm journals. There is an opportunity to help farmers focus attention during critical times with alerts that then lead to recommended responses. We would like to add recommendations to the Farmed application in the future.SustainabilitySustainability, like the term organic, means different things to different people. To many farmers, sustainability means taking care of the land, growing the soil, and managing the farm in such a way that the farm is a closed system that sustains itself. To some, sustainability is about being able to feed the nation in an accountable way for many generations to come. To others, sustainability is about feeding the local community and maintaining a balanced personal lifestyle.What's next for ATS / EchoUser USDAAdd soil survey data over the vegetation density data.Add temperature data over the rainfall and yield data.Train the application to suggest varieties to grow and activities to perform based on past performance in certain conditions (e. g. what sort of nutrient level adjustments need to be made for certain soil types).Help farmers focus attention during critical times with alerts that then lead to recommended responses.Connect farmers to each other to share their experiences dealing with infestations and the weather.Connect farmers to arrange transportation to markets, and coordinate truck routes and deliveries, in order to reduce the environmental impact.A Walk Through FarmedBegin typing 'West Milford Road, Milford, NE, United States' into the search box above the map.  You can select it from the autocomplete options.Click on the Plant Density tile on the bottom left corner of the map.  The Map should be overlaid with vegetation density data.  The green represents more dense areas.Select the year 2010 from above the map.  The map will refresh with the vegetation density from the year 2010. Also notice below the map that the percentage of fields where pesticide was applied, manure was spread, and herbicide was applied is displayed for 2010 below the map.Scroll down to the 'Section that says "What do you grow?"Select the different crops one at a time.  The crop yield to rainfall graph will update.  It displays the yield data matched up with the rainfall data for that year.  The rainfall data is calculated from the closest weather stations to where the map is centered.Notice the Monthly Rainfall graph.  This graph shows the average rainfall for each month against the actual rainfall for the weather stations closest to the center of the map.Built Withbowercrossfilterd3.jsgitgoogle-mapsjavascriptnode.jsreactTry it outlabs.atsid.comgithub.com      Submitted to    USDA Innovation ChallengeWinner                Large Organization Recognition Award                  Created by  Data Analysis, Data synthesis and website implementation. Bryan TowerFrontend infrastructure and builds, dashboard and map setup.Nathan EvansUX Design / Research and VideoYalu YeExperience Design ExplorerExperience Design, User Research, and ContentCarol ChenDavid TittsworthChris TrevinoJesus follower; husband and father; maker of software and beerMick McGeeSeiko ItakuraExperience Designer @EchoUser  I built it with Arduino 101 and Arduino Uno. I used LM35 for inside temperature, DHT11 for outside humidity and temperature, Grove Temperature Sensor v1.2 for outside temperature, Grove Light Sensor for brightness, Water sensor for detecting rain, BaseShield for connecting Grove sensors and also some LED's for showing Rain situation,of course some jumple wires, Grove to jumper wires and a colorful LCD. I wrote two Sketches for Arduino 101 and Arduino Uno to detect and show the stable, optimum and true data with LCD.   The commits and languages are obtained from the github API, parsed and transferred to the Alchemy API from which we get how intense are the feelings of the commit. We've used AWS Elastic Compute Cloud (EC2) to deploy the application and domain.com to get the domain.  We borrowed a shopping cart and replaced the rear wheels with new ones mounted to 2 24V, 165RPM motors with worm drive gearboxes. 2 3S LiPo batteries provide power to the motor controllers, 12V buck converter, and voltage monitor through a distribution block.  The 12V buck converter provides power to a 12V DC inverter that allows us to run the Kinect from the standard AC wall adapter and charge the laptop that the C++ application runs on. Below is a list of hardware:1x Microsoft Kinect1x Laptop (Windows 10)1x Kinect power adapter1x AC inverter2x Uxcell 24V 165RPM motor2x DROK DC 5-36V 400W Dual Large Power MOS Transistor Driving Module1x Power distribution block1x Tobsun DC 24V to 12V 5A 60W DC-DC Converter Step Down Regulator Module1x Adafruit 2 wire Panel Voltmeter1x Adafruit Metro Mini2x Servo City 0.770" Pattern Set Screw Hubs 8mm Bore2x Servo City 4" Heavy Duty Wheel2x Fuse wire2x 3A Automotive blade fuses1x breadboard1x 12V car jack connector breakoutMiscellaneous screws and woodMiscellaneous 22 and 10 AWG wire, jumper wires, ring terminals, and connectorsMicrosoft has an API for C++ for the Kinect, and we used that to pull data about the x,y, and z coordinates of the spine joint on the person it was tracking.  The C++ application then used this coordinate data to make decisions on the speed of both the motors, which was then sent to an Arduino over serial.  The Arduino maps this commanded speed to a usable duty cycle and sends a PWM signal to the motor controllers.  The backend is built on Node.js and Javascript while the front-end consists of html, css, and javascript.  We developed the WebGL engine in static javascript and html and then wrapped all with node (express), bootstrap and mongodb.We used heroku and mLab for the free deployment and exposition.    Does it count as a 24 hour project if I've been hoarding laptop stickers for 18 months? I attached a few sheets of posterboard, measured and sketched out the letters, then filled them in with a lot of stickers.`        First started with a long market research - talked to more than 80 Instagram influencers and brands. Teaming-up with them to understand what they need. Solving their problem by powering their community with Kin.    We create angular-based dApp with Solidity smart contracts and proposed EIP-1837.  Multiple Alexa Skills to add to the already rich library of native functions and skillsUnity to bring Alexa to lifeMicrosoft Office 365 API to read user emailsIBM's Watson to analyze sentiment      The backend of WikiBeat is entirely Python. When a topic is received, it uses the Wikipedia api to gather content on the topic and process it into rhymes using NLTK and various logic. Then the Python wave module and the google text-to-speech api are used to turn the couplets into actual sound, and put them onto a beat.Once the beat is made, the frontend is notified, and moves to an appropriate page, where the lyrics are displayed dynamically line-by-line (as the song is playing) using javascript functions that I wrote.  After a lot of hard work,I succeed in making this amazing game for the kids.There was a problems in the glitter function but I worked about 37 Hours to solve this .The main software used in making this game is Unity , And scripts are written in C#Facebook ProductsTo make this game Facebook friendly, I have integrated Facebook Ads in this game to make this game more interesting and enjoyable.  The experience is a mashup of an Alexa voice skill and an SMS chatbot. An account linking process associates Mom's cell number with the Alexa skill, at which point the family chatting can begin!The skill even incorporates Alexa's new notifications feature to make it easy for kids to see when Mom wrote back!Accomplishments that I'm proud ofWith Mommy-gram, I was able to combine multiple platforms to make a holistic experience that leapt device boundaries.What makes me proudest is that my kids love using the skill to talk to Mom (without me forcing them to)!It turned out to be a great example of using technology to connect families!What's nextWhy, Daddy-gram, of course!Built Withamazon-alexaamazon-dynamodbamazon-lambdanode.jsplivoTry it outwww.amazon.com      Submitted to    Alexa Skills Challenge: KidsAlexa Skills Challenge: Life HacksWinner                Grand Prize              Winner                Finalist Prize                  Created by  I made the whole thing from soup to nuts (with some feedback from the kiddos)!Colin McGraw  We built this mobile app using solely Meteor to demo its powerfulness. Our team has done couple of projects using meteor before so we are luckily enough to build a MVP of RunMate just under 24 hours. In this project, we used most of the common meteor packages such as iron router, accounts-password, Blaze etc. Meteoric package is integrated for the frontend UI components and transitions. For Login part, we are using facebook login (accounts-facebook) to smooth the user login experience. Last but not least, to achieve the Tinder-like swipe card interactions, the gwendall:swing package is used.What's next for RunMateWe think RunMate is a fresh and healthy way for urban joggers to discover new running routes and like-minded people in the city. We wish to push this idea out and test the market reaction once it is approved by AppStore and Play Store. Our team is eager to see this app to grow in the future and will continuously to add new features to enhance RunMate. We hope you would like this app!Packages usedmeteor-base# Packages every Meteor app needs to have mobile-experience# Packages for a great mobile UXmongo# The database Meteor supports right nowblaze-html-templates# Compile .html files into Meteor Blaze viewssession# Client-side reactive dictionary for your appjquery# Helpful client-side librarytracker# Meteor's client-side reactive programming librarystandard-minifiers# JS/CSS minifiers run for production modees5-shim# ECMAScript 5 compatibility for older browsers.ecmascript# Enable ECMAScript2015+ syntax in app codeiron:routerrandomaccounts-facebookservice-configurationstevezhu:lodashzimme:active-routeanti:fakegwendall:swingzeroasterisk:cordova-geolocation-backgroundaccounts-uierasaur:meteor-lodashmeteoric:ionicmeteoric:ionic-sassmeteoric:ionicons-sasssacha:spinraix:eventddpcheckBuilt Withfacebook-login-apigoogle-mapsmeteor.jsTry it outgithub.com188.166.250.104      Submitted to    Meteor Global Distributed Hackathon     Created by  Overall project management and provide support for both frontend and backend development.Lawrence HuiHK-based full-stack developer. Keep hacking everything.I worked mainly on the front-end side, polishing the UI and app flow to give the best user experience.Roy HuiFull-stack and mobile developer based in Hong KongBuild the whole backend using meteor and to develop of the main functions required for the app.Mike WongNami C        PickMe is built on the Facebook Messenger API.  After work and school. Took me like 3 weeks for the video version  Pedro Yusim, Ellie Costa and Benhur Quintino came up with this fun and inventive idea. Ellie was responsible for all of the UX and UI design, while Pedro was in charge of building the iOS app from scratch.  We used our grandmothers papers as the first use case. Through family collaboration, continual refinement, and testing processes, we have developed the first iteration of Kindex. Invention, my dear friends, is 93% perspiration, 6% electricity, 4% evaporation, and 2% butterscotch ripple. Willy Wonka    Using python, aws lambda and ibm watson   Overhear all of our biggest technical challenge yet. It is built using a backend server written in Go, an audio-capturing webserver in Python, and a front-end as an Electron app. Overhear was created in less than 36 hours at HackRice.  In a hurry. With love. Coding day and night to make it awesome! :)  Twenty QuestionsWe used Amazon Alexa Skill Set and Amazon Lambdas to help us use Amazon EchoWe built a Flask server on the RaspberryPi which provides the Backend Support to Alexa by getting information from AkinatorSince the Flask Server runs locally on the RaspberryPI, we used Ngrok to Tunnel it and provide a Public Web URL that everyone can access.Akinator is the backend to our product. They don't provide an API to communicate with their database and hence we use Selenium Webdriver (Hacky!)  to use their servicesIOTWe used Amazon Alexa Skill Set and Amazon Lambdas to help us use Amazon EchoWe power the LED through Raspberry Pi and control the GPIO pin from Amazon EchoWe use the Bluetooth of RaspberryPi to control SpheroMapQuestWe used Amazon Alexa Skill Set and Amazon Lambdas to help us use Amazon EchoWe use the MapQuest API to get the optimized route and feed it to the Amazon Lambda where the states are stored for further queries  We used pure javascript and a couple javascript libraries. There is no server-side code. The UI uses materialize-css to adhere to the Material Design stardards. We also use the Google Maps API to provide autocompletion for the locations and to compute the distances between the different locations. The route planning algorithm translates the problem in a boolean expression and solves it using logic-solver, an excellent boolean SAT solver. Finally, the PDF report is generated on the client-side with PDFKit.js .      Using Apple's Objective-C for iOS targeting the iPhone only. I used the myScript ATK libraries. Specifically the Single character recognition function (Superimposed mode for all, Isolated mode for Arabic language). I managed to provide a custom time-out function between drawing characters and searching contacts.           I took naive approach this time. While I was clear with my node.js & lambda skills, I found Voiceflow extremely easy to use and achieve what i'm currently aiming for.    Our project was created with Python programming.  I wrote a bunch of macros and wrote a free form poem to express my feelings regarding the past 24 hours.  We built it with a Python backend, using the Flask framework and text-to-speech, English-to-xx language translation and the Google Places APIs.  We built the entire project during the VanHackathon, from May 20th - 22nd.    We have used the infermedica API for the core data provision. NodeJS has been used for making the backend for the MedBot with the use of natural library for NLP operations. Front end has been built in core CSS and HTML using JS with Jquery for AJAX requests.The Openshift platform has been used for running the NodeJS server and expose a REST API to interact with front end. Although the server provides a web app, the REST API exposed could be used with any other App if required.MongoDB has been used to hold data for the ongoing chats.     Outlook Add-in for iGlobe CRM is a true Office Add-in designed using the Microsoft Office 365 Security model. As all Office Add-ins it contains of a manifest file that defines various settings, including how the add-in integrates with Office clients and to enable IOS and Android availability. The web application and service are hosted iGlobes Microsoft Azure.  The Add-in is using the Authentication model and make sure the user has a valid token. Using the Microsoft Graph API the Add-in is interacting with several services on Office 365, like SharePoint Online, Groups, Planner and Calendar. What's next for iGlobe Outlook Add-in for iGlobe CRM Office 365Giving the customer an option to set and use customized fields from iGlobe CRM. Built Withgraphhtml5office-365Try it outappsource.microsoft.com      Submitted to    Hack Productivity 3    Created by  Alon Ekelund  Things I focused on :Keeping it Dynamic.Adding different ways to interact each time.Not so easy-Not so difficult-Not so boring questions.Minimum Code-Maximum Functionality.Easy & Interactive Experience (Not to make the kid feel quizzy but increase their knowledge too)That said, Lets get on with how it's currently Implemented. Programming Language: Python Services Used :AWS Polly - Different voices for different opponents.AWS S3 - For storing :      > images      > audioAWS DynamoDB - For storing:      > Opponents     > Questions     > UsersAWS Lambda - For running the code. Architecture The best thing about the implementation?It is very dynamic. I can add almost anything including levels, opponents, and questions, without changing any code.  Using React (frontend), Node js(backend) with NGINX we developed our own video live stream architecture based on gps coordinates integrated with google maps.WE also used MICROSOFT AZURE to analyze emotional state of the citizens in order to decide the gravity of events.  We have developed the Android app by interfacing Google Maps to track the user location through GPS.   It is built using the trnql Android SDK, Youtube SDK and the Android SDK. It requires no additional server thanks to clever usage of trnql SDK. The main algorithm for it to work is that the device before playing gets the current time from the global NTP server and sets its play time in its payload in the trnql smart people API, and when the others read it, they set their clock to play the video at the same time. So theoretically if the devices are in different time-zones the app would work. The app also first intelligently buffers the video sufficiently and set its state, so that there are no issues even on slow internet connection. Showing the music recommendations based on weather and activity was a piece of cake, thanks to the trnql APIs.     We use Marvelapp to make prototype, but we plan to hire a software engineer as we go along  Front-end: Ionic, Adobe XDBack-end: Node.js, Mongoose, MongoDB, Heroku  We built this power up using NodeJS and Unity 3D for the slot machine game.   This product adds to the Hyper ecosystem and is built on electron, react, and redux.  we use Meteor framework.  we picked a open source chat app from GitHub and add on the communication part to Raspberry Pi  The design was fairly simple.  I opened up the latching parts of the gate and found that a cylindrical reed switch fit perfectly into both sides.  The magnet part of the switch fits in the swinging gate and thus doesn't require a power source.  The other side of the switch connects to the Arduino 101 which has a rising edge interrupt setup on the pin.  I also connected an RGB LED and active buzzer for visual and auditory alerts and a button to allow me to mute the buzzer if I need the gate opened for an extended period of time.When the reed switch is closed I put the Arduino 101 (Curie) into a sleep state to conserve power.  When the switch is opened it fires the interrupt which starts my state machine to go through the various alert levels.  I also used the CurieTimerOne to create an interrupt driven PWM on the buzzer pin to simplify my beeping routines.  Set up AWS IoT data source using sample sensor data.Running the Python script generates fictitious AWS IoT messages from multiple devices. The IoT rule sends the message to Firehose for further processing. Created three Firehose delivery streams:  one to batch raw data from AWS IoT, and two to batch output device data and aggregated data from Analytics.Configured AWS IoT to receive and forward incoming data.Created an Analytics application to process data using Amazon KinesisAmazon Kinesis generates two output streams.BASIC_STREAM contains the device ID, device parameter, its value, and the time stamp from the incoming stream.AGGREGATE_STREAM aggregates the average value of distance between sensor and ground over a one-minute period from the incoming data.End result stored in S3 bucket.Amazon Lamba function calculates air pressure from data received through sensor, and posts to Dashbord using Amazon API gateway. In case air pressure is below than cut off value, then this app would send notification to end user on registered mobile number.So at the end user will see air pressure data on dashboard and in case of low pressure will get SMS notifications  We built it with Meteor  Using meteor and meteor packages we've created that are the basis for easily creating cloud integration apps, including support for filters, transforms, smart API endpoint pulling with last modified / since support, etc.List of packagescoffeescripthttpforce-sslhubaaa:accounts-slack # forked during hackathonaccounts-githubjsep:accounts-trellohubaaa:link-accounts # forked during hackathonsemantic:ui-csssemantic:ui-iconovcharik:alertifyjspracticalmeteor:loglevelpracticalmeteor:chaimanuel:viewmodelhubaaa:easy-meteor-settingshubaaa:easy-service-confighubaaa:json-pipeshubaaa:endpoint-puller        We used Unity and Maya, along with the esri citiengine, and the American airlines API,.    We use Shamir's Secret Sharing to create the encrypted shards. We share those shards with the contacts via a URL, and use a smart contract to emit events & make sure shard holders still have possession of the shards.Built Withcryptographyjavascriptlovesolidityweb3Try it outkeysplit.iodocs.google.com      Submitted to    ETHDenverWinner                Overall Winner              Winner                Toshi                  Created by  I did most of the crypto, shard distribution, and tied the backend stuff into Mark's frontend work.Austin  RobertsNick NeumanMark BarrassoSolutions Architect @ CiscoJuwon Bahn    This app is built mostly with Android native code (Java, XML). As there are 2 Android apps we spent most of the time in Android Studio itself. The backend was built using Backend As a Service Parse that uses MongoDB.All the designs and icons were made by our teammate Rishabh whose sole task in the hackathon was to work on the UI elements for a great experience.Offline caching is done along with a lot of other techniques!  We build a Chrome extension that masks your photos, so that machine learning algorithms classify them wrongly, yet appear completely the same to the human eye. The user is given 2 choices in the chrome extension: apply an Adversarial Patch to his/her image or evenly distribute Adversarial Noise across the image. For the Adversarial Patch, the extension adds a patch on the image provided that causes a general object recognition classifier to misclassify an image.[2] The patch is made by applying expectation over transformation, over a random area of the image. For the Adversarial Noise, the extension makes quasi-imperceptible changes to the image such that a machine learning model classifies it incorrectly by using a single gradient ascent step, also know as a "fast gradient sign method".[3] While both approaches typically require a known existing neural network architecture to compute their adversarial attacks, a.k.a. a white box attack, they have been shown to work well on black box neural network architectures as well.[3] Text input is converted into an image before applying the Adversarial Attack.      Our android application was built with Android Studio. We were able to connect the Estimote Beacons through integrating their SDK and dependencies into our system. We integrated text to speech to output voice commands.  After a brainstorm session (the reflection above) we had the idea of creating the themes and designed the process.At first, the idea was only creating an API, but then we found out that a front-end app would add a lot of value to the idea. The API was built in Kotlin/Spring boot and the front-end in React with Typescript, using Material UI.  David Lougheed and Allan Wang made the game play mechanics happen using javascript, typescript, and html. Alvin Tan was in charge of the music and boom box mechanics. Elizabeth Poggie designed the fonts, the graphic design through One Note, and partook in meow solicitation. As well we have some honorable mentions of those who helped record the meows, thank you to Will Guthrie, Aidon Lebar, and Jonathan Ng!Finally, thank you to everyone who gave their meows for the cause and made some art for the picture frame above the couch!ControlsWASD to move Player 1, E to pat with Player 1.IJKL to move Player 2, O to pat with Player 2.  DataGridJavalinOpenJDKKotlinNode (Scraper)JavascriptChrome extensionSabre APILyft APIAmadeus APILufthansa APIGoogle Geo APIFuelVideo demo of the technical features: https://youtu.be/KncPJEdiSSg.    We created a web app to upload a DICOM format (2D image) of a scan, automatically convert it to an STL format (3D rendering), and push the mesh model into the cloud, hosted on MongoDB. We then import the image into our mobile app supported by the Torch AR engine where the user can now view their model on a mobile device, create annotations on parts of the structure, and share insights with fellow medical professionals.  This was built using the MERN stack - Mongo, Express, React, Node.js. As mentioned earlier, all userdata is stored in Okta, Mongo is only used to store each problem's information. Websockets were used to implement real-time communication between users, and Dialogflow powers the chatbot.  With much gratitude to Facebook for the many resources that they have provided such as PyTorch (an open source deep learning framework) which we used for the optimization of personnel deployment and recommendation, react-native (an open-source mobile application framework) for our mobile app prototyping and development, and Facebook-graph-API & Instagram-graph-API which help us to automate online campaigns by scheduling and publishing of initiatives such as blood donation, and thanks to the diversity in roles of our team members we were able to come up with the solution named CERT.  I was thinking about what to use, but finally, I decided to use the jSoup, and implement the wrapper around it.This is a Neo4j procedure package, so it should be compiled into a jar file, and installed into your Neo4j instance.       Have done R&D and gathered the use case.Have found the optimal logic.Have given life to the logic with C# (C-Sharp).  Using an nRF24L01+ and an Arduino, and a custom build of Ethers Wallet, which had additional JavaScript calls added to expose the necessary BLE capabilities of the phone. No soldiering required.  We found this as a main problem in our company, and we take it as a challenge, to think at a solution that could be implemented in our company. We came with the ideea and we have started to think how to improve it and how it works better in the company.  Front end: We used the Python Alexa Skills Kit SDK to develop the Lambda for Alexa. We used Dialog Management and Entity Resolution to handle the more conversational pieces.Backend: Cloud server running on Microsoft Azure that provides a REST API to receive IoT data through IoT Hub.  CodingBoard is a mobile + web app backed by a few backend services. The mobile application is built on Objective-C. The iOS is able to take images and crop them to heavily reduce image size and improve OCR performance. The web app uses (React Ace) to allow the user to edit code faster. The web app allows you to quickly upload images and test outputs. It's also handy for writing code and for quick edits. Both of these clients interface with the Go backend.We have two Python microservices that handle image processing and programming language detection, and a Go service that calls upon these two services. The Go service simply acts as the data frontend for both the web and the client.    We used a few open-source projects as our bases, including this as the base for the flappy birdand this as the base for sound sampling. We then used Typescript and Yarn together to build the rest of the backend, with HTML and CSS as the natural front-end.    For the front-end, we've used the popular and well-tested CSS framework, Bootstrap. jQuery plugins we've used include: geocomplete, Dropzone.js and Lightcase. Fonts used are Open Sans and Crimson Text.Back-end is coded in PHP. We've used a third-party library Ultimate MySQL library for database communication. Facebook Graph API is used to integrate the social login feature. Images are resized on-the-fly using timthumb.php library.    As simple as the workflow sounds, there were many challenges that had to be overcome to get this to work. These are the individual problems I needed to solve.Determine when a page that needs to be read is in the camera frameIsolate the Text Block and clean up the image using OpenCVPerform OCR (Optical Character Recognition)Transform text into audioPlay back the audio through speakersEach one of the above steps introduced unexpected challenges, (some of which made me question if I would be able to finish this project in time!)Determining when a page that needs to be read is in the camera frameFor the DeepLens to be able to read a page, it needed some way to know if there is something in the camera frame to read. This is where the deep learning model was used. I looked online but could not find a pre trained model that was able to classify a page in a book. As a result, I knew I would need to train a model with new data to classify this type of object.To get training data, I did a quick search online to try and find images of childrens books. I found tons of images of book covers, but practically nothing showing someone holding the book in the correct orientation for me to be able to train with. I needed images of the actual pages with text on them. Fortunately, I have four young kids, and they have hundreds of childrens books. So, one night I grabbed about forty books, and started taking lots of pictures of myself holding the different books in different orientations, lighting, and with my hand occluding different parts of the page.During this process, I started to realize that the blocks of text in these childrens books varied greatly. Sometimes the text was white on black, or sometimes black on white. Other times it was colored text on different colored backgrounds. Sometimes the text ran along the bottom of the page, and some books had text all over the page with no logical flow. So, I decided to narrow my focus and only capture images of books that had text in a somewhat normal positioning. I figured that if I tried to get too broad of a dataset that I would end up with a model that thought everything was a block of text. Down the road maybe I can experiment with more varied types of data but for this project I decided to limit it.After I captured the data, I used labelImg to generate the Pascal VOC xml files that would be used in training.At this point, I got stuck trying to figure out how to get my data formatted correctly to be trained with MXNet. I found a few examples using TensorFlow and so that is the road I went down. I figured if I finished the project, I could always go back and get it working using MXNet if I had extra time at the end.I was able to follow one of the examples I found on YouTube and ended up with a working model that I could use to detect text on a page.Perform Optical Character RecognitionI was surprised how easy it was to integrate Tesseract into the project. Just install it on the device and pip install the python package and then the workflow was really just a single function call. You provide an image to process, and it just spits out text. I was originally going to use a separate Lambda function with Tesseract installed to perform the OCR, but I ended up just including it in my main Lambda function because it was simpler and cut down on traffic to and from AWS. The actual OCR didnt seem to take up too much compute power and compared to the round trip to AWS, the time seemed to be comparable. Plus, now I am doing more "at the edge" which is more efficient and costs less.There was one gotcha with Tesseract. It is very picky about the image quality. I had to spend a considerable amount of effort figuring out how to clean up the images enough to get a clean read. It also wants you to have the text almost completely horizontal, (which is pretty much impossible considering I want preschool aged children to be able to use this thing)I used OpenCV for most of this image pre-processing and after a number of iterations, I was able to produce images that were plain black and white text with minimal noise. This was key to getting this project to work. The end results were better that I expected, however, there is still room for improvement in this area.Transform text into audioThis step was the easiest of the whole project. Once I could get audio to play on the device, I was able to encapsulate this logic into a single function call which simply calls AWS Polly to generate the audio file. I never write the file to disk, I just feed the byte stream to the audio library and discard it when its done playing. I do have a few static mp3 files that are played on startup of the Green Grass service. I use the audio files to speak instructions to the user so they know how to use the device. I figure there was no reason to call Polly to generate this speech as it never changes so I generated them ahead of time and deploy it with the Lambda. (I love how easy it is to integrate existing AWS services into my projects!)Play back the audio through speakersGreen Grass requires you to explicitly authorize all the hardware that your code has access too. One way you can configure this through the Group Resources section in the AWS IOT console. Once configured, you deploy these settings to the DeepLens which results in a JSON file getting deployed greengrass directory on the to the device.To enable Audio playback through your Lambda, you need to add two resources. The sound card on the DeepLens is located at the path /dev/snd/. You need to add both /dev/snd/pcmC0D0p and /dev/snd/controlC0 in order to play sound.As an aside, You will need to re-add these resources every time you deploy a project to the device. At the time of this writing, Green Grass overwrites the resources file with a pre-configured group.json whenever you deploy a project to DeepLens. Accomplishments that I'm proud ofI was very happy with the end results of this project. In spite of having a full-time job and four kids, I was able to create something really cool and get it to actually work reasonably well in just a couple months. I have more ideas for future projects with this device and am excited to keep learning.What I learnedBefore diving into this project, I had no experience with Deep Learning or AI in general. I have always been interested in the topic but it always seemed to unapproachable to "normal developers". I have discovered, through this process, that it is possible to to create real useful deep learning projects without a PhD in math, and that with enough effort, and patience anyone with a decent development background can start using it.What's next for Read To MeI have a few ideas for improving the project. One feature I would like to add is the ability to translate the text that is read. (I signed up for early access to Amazons new Translate service but havent yet been approved.)I also plan to continue to improve my model to see if I can increase the model accuracy a bit as well as make it work with a broader range of books.Lastly, the text image cleanup function function, which feeds directly into Tesseract, can be improved. Specifically, it would be beneficial to be able to rotate and or warp the image before sending it to Tesseract. That way when a child isnt holding the book correctly, it could still read the text. Motion blur was also a definite issue I had to contend with in image cleanup. If the book isnt held very still for a few seconds, the image is just to blurry for the OCR to work. I have read about various techniques to solve this problem, like using image averaging over multiple frames, or applying different filters to the image to smooth out the pixels. I am sure that it's possible to achieve a better/faster outcome but it's tricky working on a resource constrained device. Helpful ResourcesThere were many online resources that helped me along the way but these links proved to be the most helpful. (In no particular order)https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8abhttp://gluon.mxnet.io/chapter08_computer-vision/object-detection.html?highlight=ssdhttps://github.com/apache/incubator-mxnet/blob/master/example/image-classification/README.mdhttps://github.com/zhreshold/mxnet-ssdhttps://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/Also, many thanks to all the participants in the forums answering questions and especially to the AWS DeepLens team for getting my unstuck numerous times! :)Built Withdeeplenslambdamxnetopencvpollypythontensorflowtesseract-ocrTry it outgithub.comgistlog.co      Submitted to    AWS DeepLens ChallengeWinner                First Place                  Created by  Hope this helps someone else who is just getting started. I learned a lot!Alex Schultz  With our mantra of exploring the world full of possibilities, Swop uses the latest blockchain technology. Blockchain puts the power in people's hands while still maintaining security of your data. This enables any person with a valid flight booking to post their flights on the application and make it available for swopping without revealing your identity. This also remove the need to transact directly to the airlines as the application will do it for you.  We used IBM Watson to power real-time emotional analyses and speech-to-text/text-to-speech, and React/Node.JS/Express.JS to create a voice-powered interface for real-time feedback and journal analysis and storage.   (Data Transfer) The real-time data from the Muse Headband is sent to the laptop over Bluetooth. We used Node JS to chunk the data into 30-second segments. (Signal Processing) We retrieve the EEG data from the Muse band, and extract the measurement of microvoltage measurements. The Muse SDK conducts band filtering on the dataset to remove extreme values and noises. Then, we conduct a Fast Fourier Transform (FFT), which gives us the amplitude and frequency of electrical signals at each data point. We average the amplitudes within the gamma wave frequency range, which results in a value called PSD (Power Spectral Density). We isolate the PSD values in the gamma band range.(Baseline Establishment) According to numerous peer-reviewed research articles, it has been found that high PSD is caused by generally unpleasant emotions (low valency), and low PSD is caused by generally positive emotions (high valency). Each person has slightly varying PSD values, so for each subject we need to establish a baseline. The emotions studied (happy, anger, confused, disgusted) were simulated by showing video clips. Future PSD values are categorized into positive and negative emotions using statistical analysis.(Server-client Model) After the signals are processed, the data is transferred to AWS EC2 Server. The server communicates with the client, which is the mobile app in this scenario. We created a Emicus.tech domain for our sponsor, .tech.      We used Microsoft Azure, Microsoft Cognitive services i.e. LUIS to make the chatbot, and the Google Calendar API to help schedule appointments.  Reverse engineer the Estonian e-Identity signature mechanism.Implement all the crypto in Python using libraries.Implement all the crypto in Python from scratch.Implement some 384-bit bignumber utilities in Solidity and EVM Assembly.Implement the Secp384r1 384-bit finite field in Solidity and EVM Assembly.Implement the Secp384r1 generator order finite field in Solidity and EVM Assembly.Implement the Secp384r1 elliptic curve in Solidity.Implement the ECDSA signature verification scheme in Solidity.Add a ton of testsFix a ton of bugsOptimize, optimize, optimizeImplement a wallet contract using e-Identity signatures.Implement a rudimentary front end for the wallet.    Two people worked on the front-end, which included all of the design, and the other to worked on the back-end implementation of the CNN (convolutional Neural Network) among other responsibilities such as web scraping similar articles. In the process, we learned or strengthened our HTML, CSS, Python, and Javascript abilities.    By getting data from our air monitoring modules and uploading it to our google cloud platform server, we're not only able to create the heatmap, but we're also able to provide the city with a lot of accurate air quality measurements.Our air monitoring stations are made of a cheap arduino-enabled board and an environmental sensor that computes the level of VOC (Volatile Organic Compounds) in the air. To process all these real-time data, we set up a Python Flask webserver that populates our SQL database which is then consumed by a front end written using React JS.        Framework was built using the UiPath StudioCustom activity package was built using Microsoft Visual Studio IDE and NuGet Package Explorer  We created a custom API using Symfony and we are using our robot to request for all invoices (updated / created).  To create the Call Assist website, we used Bootstrap. We used a framework to design a simple website that displays all of the necessary information that a first responder needs to get to and aid a civilian in need. We used the Google Maps API to show a live map based on a user's location data. For our application, we used Android Studio to create an Android app that displays a users name, general information, and location. We also implemented Twilio to send out an alert phone call to family and friends of users who are experiencing an emergency.  We used the existing Myo iOS SDK and created our own specialized algorithm to recognize different exercises.  Mike uses a leap motion controller for motion capture. A nodejs client reads data from the sensor, and send it over the network to a NodeJS server on a raspberry Pi, which controls a robotic arm via USB. A 3D printed case is mounted to an RC car for locomotion.      We used Unity and c#.        We've built GROOT on the proven Django/Python web stack. It's deployed on cloud VMs, making horizontal scaling as easy as 1-2-3.    We split up work based on expertise. Yitaek did the voice interface and twilio integration. James did full stack development. Hannah did the front end and design.  We built an Android application that allows divers and NGOs to report ghost fishing gear that they find. In order to facilitate the tracking of the smart tags which we incorporate in our idea, we created our own algorithm based on the Extended Kalman Filter with a gain which is regressed over various samples of traced paths through a Grid Long Short Term Memory network. The algorithm makes use of an Inertial Measurement Units data integrated in terms of acceleration, and also cached forecasted water current data to gain a fairly accurate approximation of the smart tags position over time. The hardware prototype was made using an Arduino and MPU6050 IMU for the dead reckoning calculations. We also made a RESTful API for the backend using MongoDB and Deployd & hosted on Heroku.  We used lean stack technologies and integrated the Office 365 into our API.        the game is designed to play standing up using a blutooth gamepad to walk/strafe and shoot, using our head/body to rotate around the world. I think this the most inmersive way to to take advantage of the wireless experience we have in mobile VR.I'm considering include an alternative control using only our head to active walking/stop and the cardboard trigger to shoot, but i'm not sure yet if it will works so good      Reactjs and determination    Love + ASP.NET WebAPI + Ionic + AngularJS + SQL Database + Cordova  Well, we started with brainstorming for ideas as we came here empty-handed. When we got the idea, we started working on it. We came up with the name and the story, made the logo, started designing and implementing the product.  We used Visual Studio and shaked it with JavaScript, HTML and some CSS.    LectureBuddy is a web-based application; most of the developing was done in JavaScript, Node.js, HTML/CSS, etc. The Lexalytics Semantria API was used for parsing the chat room data and Microsofts Cognitive Services API for emotions was used to gauge the mood of a class. Other smaller JavaScript libraries were also utilised.          Wrote code. In Visual Studio.    With laziness and lack of Wi-Fi.          The frontend is a react web app that handles listing tasks and allowing the user to submit tasks. The backend does user storage and image processing in flask and presents a REST API to the frontend to use.  iPlanner Pro is an Outlook Add-in that extend Microsoft Planner into the users Outlook client. iPlanner Pro is a true Office add-in designed using the Microsoft Office 365 Security model. As all Office Add-ins it contains of a manifest file that defines various settings, including how the add-in integrates with Office clients and to enable IOS and Android availability. The web application and service are hosted on iGlobes Microsoft Azure.  iPlanner Pro is using the Authentication model and make sure the user has a valid token. Using the Microsoft Graph API iPlanner Pro is interacting with several services on Office 365. iPlanner Pro is an extremely rich Add-in that requires permission using Microsoft Graph to, Planner, Groups, SharePoint, Exchange. Specially Office 365 Groups requires extensive permission. Office Add-ins must conform to the design guidelines for Office Store Add-ins  iPlanner Pro is built using Office UI Fabric. Data is not saved. We dont save data on our server. (we dont have access or permission). No data are saved locally on any PC. It is an Office add-in that is deployed online. iPlanner Pro will be available on all the users devices where the users have a valid Office 365 account with the proper permission given by the Global administrator. Wheres the data held?Office 365 splits plan data across a number of repositories. The plan metadata is held in an (your) Azure-based service while the comments for tasks within a plan are stored in the Exchange Online mailbox for the Office 365 Group associated with the plan. Documents and other attachments are held in the SharePoint Online document library belonging to the same Office 365 Group.This means all data are in the organisations own Office 365 environment.The GDPR and Security are key issues. We relay 100% on Microsoft and are following 100% the requirements they set. We believe iPlanner Pro to be GDPR ready.  I designed and developed the skill with Voiceflow and used APL for displaying the images.       Decentralized identifiers (DIDs)The resulting combinatorics of possible connections between any given set of entities in a mobility system is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected entities to achieve their goals. This requires an interoperable and ubiquitous method to address, verify and connect these elements together.We propose to adopt the open decentralized identifier standard (DID) as an open, interoperable addressing standard and to establish mechanisms to resolve DIDs across multiple central or decentral mobility systems [2].DIDs are the atomic units of a new layer of decentralized identity infrastructure. DIDs can be extended from identifiers of people, to any entity, thus to identify every thing. We use DIDs to help identify and manage data sets, objects, machines or software agents through their digital twins, to locations, to events, and even to pure data objects.DIDs are derived from public private key pairs. We are using innovative cryptographic solutions for secure key management by fragmenting the private key of a DID that never exists in its entirety. Our key management solution is very effective for providing very secure signing transactions e.g. for smart phones, algorithms or data sets. An integration of the key management technology into embedded devices is on our technology roadmap.A DID has the following required syntax:did:method:idstringWe are using the Ethereum Blockchain and DID method ethr for our development work.did:ethr:0x5ed65343eda1c46566dff6774132830b2b821b35As our technology stack is blockchain agnostic any other DID method based on alternative blockchains can be integrated and used.Verifiable claimsDIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked), are verifiable claims [3,4]. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifiable Credentials Working Group.Verifiable claims can be either self-issued by an entity such as a machine to provide a proof about authenticity and integrity of data or they can be issued by a third party (issuer, e.g. OEM, government, TV, service provider, bank).In mobility systems any entity might want to transact with any other entity. This means entities are engaging with each other in a dynamically defined, on-demand way. It is not pre-defined which entities interact among each other. To ensure efficient transactions any new entity in a mobility value chain must be able to independently verify other counter parties.To achieve this objective, we are using the DID approach and are anchoring the verifiable claims on a distributed ledger technology to move the cryptographic root of trust from central systems into a decentral, interoperable infrastructure.Digital twins that are verifiableA digital twin is a digital representation of a biological entity (human, living organism, organization), a physical entity (objects, machines), a digital entity (digital asset, software agent) or any system formed of any combination of individual entities.Digital twins can represent objects and entities as varied as a IoT sensors, ECUs, spare parts, vehicles, traffic lights, access gates, human users, or a city, and everything else in between. More recently they have started to be used to represent intangible entities like services, code, data, processes and knowledge. Digital twin data can consist of any life-cycle attributes and IoT sensor, telematics or compute data.A verifiable digital twin is a digital twin with attributes that are represented by verifiable claims. These attributes such as a birth certificate, authentication proof, a calibration report or sensor data attestations can be independently verified by any third party.This type of digital twin provides verifiable data about its creation, life-cycle, sensor readings, actuator commands or transactions. These verifiable data can be used for audit trails, decision making and for feedback loops in (autonomous) control systems.Verifiable driving event data chainA data chain is a cryptographic data structure that chains signed data objects together and establishes a method for data flow provenance. Data flow provenance allows verifying the end-to-end integrity of every data flow object and its transformations (additions, deletions, modifications, combinations, and machine learning processing).Processing of driving event data is operational in multiple mobility disciplines. Driving event data processing can include multiple data sources, parties, algorithms and processing steps. A human or non-human end-user of driving event data chains needs to be able to validate trustworthiness and accuracy of data chain output data. This requirement becomes of critical importance when the output data is used in safety or security relevant use cases or to make economic decision with significant commercial values involved.For establishing a verifiable data chain, we link signed objects together. The following code snippet is a payload example with a machine learning label (red traffic light), information about the algorithm that created the label (Algorithm 1) and a link to the previous data chain block (previous block ID).HEADER: PAYLOAD TOKEN TYPE & SIGNATURE ALGORITHM{"typ": "JWT","alg": "ES256K-R"}PAYLOAD: DATA{  {  "iat": 1546724123,  "exp": 1546810523,  "signer": {    "type": "algorithm",    "name": "Algorithm 1"  },  "data": {    "claim": {      "predictionLabel": "red traffic light, red traffic signal, stoplight",      "predictionProb": "0.983483",      "did": "did:ethr:0xe405b9ecb83582e4edc546ba27867ee6f46a940d"    },    "previousBlockId": "b86d95d0-1131-11e9-982e-51c29ca1f26e",    "previousBlockHash": "307b817de9b7175db0ded0ea9576027efd64fb21"  },  "iss": "did:ethr:0x5ed65343eda1c46566dff6774132830b2b821b35"}The data chain object can be verified by validating the signature of the payload. Cryptographic data chains enable users to validate the provenance of entire driving event data processing chains including the authenticity and integrity of the input data, the output data and the provenance of sensing devices and the processing algorithms.We recommend to  establish verifiable data chains for driving event data processing,  provide a DID for every entity and data set,  integrate the data chains with DID registries (e.g. validated list of OEMs).The verifiable digital twins are addressable by their DIDs and providing information about organizations, sensors telematics devices, data sets, external data sources, software algorithms and users involved in the data chain.This approach is of particular value when validation or benchmarking data are available about the sensing devices, vehicles and the algorithms that are processing the driving event data. In combination with a reputation or validation system any user can calculate trustworthiness and accuracy metrics about the output data. For instance, data with provenance will have a much higher commercial value on a data market place.As a next step, reputation methods can be integrated for both, individual digital twins and entire data chains. Further standardization work on data chain trustworthiness and accuracy metrics need to be done.Design principlesFor our digital twin and data chain integration work we are applying the following design principles:PrincipleDescriptionFrom VINs to value chainsAbstracting the Concept of Identity to a mobility system of vehicles, IoT, road & travel infrastructure, mobility systems, ML agents, driving event data set, autonomous driving/DAS feedback loops, markets and humans. Exchanging data among those entities. E2E data provenance along a value chain.Blockchain-agnosticUse blockchain for anchoring attestations or verifiable claims. Decision on which blockchain to anchor claims based on user preferences or economic metrics such as Tx costs. Use of fiat-backed stable coins for micropayments.Scalable integrationIntegrated technology stack consisting of off-chain data structures, serverless cloud infrastructure, secure key management, DIDs, ML agents, sensor data, data chain fusion and blockchain connectors.Responsibility SegregationImplementation of this common pattern for micro services design that supports scalability and maintainability of our solution.StandardsUse of existing W3C, Industry 4.0 and Automotive data standards and semantic models to ensure adaptability and portability of our solution.Business valueFocusing on simple data integrity and authenticity problems within existing value chains. Retrofitting of existing infrastructures to scale adoption.Agile driving event data chain for MOBI grand challengeAgile driving events can be divided into two groups: (1) the interaction between a drivers vehicle and the road environment, and (2) the interaction between a drivers vehicle and nearby vehicles [5].Diverse methods for enhancing driving safety have been proposed. Such methods can be roughly classified as passive or active. Passive methods (e.g., seat-belts, airbags, and anti-lock braking systems), which have significantly reduced traffic fatalities, were originally introduced to diminish the degree of injury from an accident. By contrast, active methods are designed to prevent accidents from occurring. Driver assistance systems (DAS) are designed to alert the driver - or an autonomous driving module - as quickly as possible to a potentially dangerous situation.The two classes of driving events may occur simultaneously and lead to certain serious traffic situations. Automotive industry is working on active methods and systems including machine learning algorithms to analyze these two kinds of events and determine dangerous situations in agile driving from data collected by various sensors and data from external sources. The machine learning output labels about dangerous curves, road obstacles or poor vehicle conditions are fed into control, transaction and risk systems. In distributed mobility systems the trustworthiness and accuracy of the output labels must be independently verifiable.Key question: How can I trust vehicle identity data, 3rd party data and machine learning labels that are created and processed along a distributed mobility value chain?To achieve trustworthiness of output labels we are planning to blend our verifiable data chain technology with historic driving event data and black box algorithms to build a verifiable driving event solution:  Interoperable decentral identity and verifiable digital twinning protocol  Cryptographically secured and blockchain-enabled data chains  E2E integration of remote sensing (telematics) data and machine learning algorithmsOur approach demonstrates how the following trust problems can be addressed with DLT:  Vehicle provenance and configuration  Provenance, verifiability and integrity of the driving event input data (or telematics data)  Integrity and transparency of driving event data chain when multiple 3rd party intermediaries are involved  Credentials about benchmarking of ML algorithms and training data  Aggregated accuracy and trustworthiness of predicted ML labels and attributes  We attached a camera onto a Raspberry Pi which we then mounted onto a cardboard box that would act as the cooler unit. The Pi and screen are mounted using a custom cut cardboard attachment from a used Domino's pizza box. We also attached a bright light to the Pi that we have mounted to the inside of upper box.Our Pi runs an application that takes a picture every second, uploading it to Dropbox. We then have a server running OpenCV which takes those images, processes them by taking in the tops of the bottles and then analyzes them to see what type of bottle it is. In order to train our AI, we went through multiple iterations and cuts of the photos to reject or accept the images as a certain brand.The status of what bottles are available is then propagated through the cloud to our web UI where we display how many bottles of each type are in the cooler system.Team MembersAsheik HussainSaj AroraDavid MaimanAlso thanks to Steven Yoo who was here in spirit. Built Withangular.jsbluemixbootstrapc++dropboxopencvpythonraspberry-piTry it outgithub.com      Submitted to    Hack the World - New YorkWinner                Third Place - US$1,500                  Created by  I setup the raspberry pi with sensors. Wrote the code to capture images from the camera and a camera calibration application. I worked on some of the front end UI. asheik hussainFront end development, documentation, brainstorming, assisting with the machine learning. David MSaj Arora  We started off by using a Heroku backend and we developed a native iOS app using Xcode. We used the core engine supplied by American Airlines. We used Google maps for the map part and integrated the airports from the data provided. Challenges and Rewards were implemented into the application.    We used openCV to create a UI, that feeds images to firebase and to Clarifia which then process images adding them to a database which sends json back to the UI telling the user what the object is. Using linux Debian we where able to make the openCV an .exe and Dreamweaver to design a website that would make our software available to the masses.      Mobile App:Unity3D (C#)VuforiaVoice Bot:Dialogflow (formerly API.AI)node.js (backend logic)Web Server:Heroku (node.js)AWS Node.js SDKAWS Services Used:AWS DynamoDBAWS S3AWS IoTAWS Kinesis StreamsAWS Kinesis Analytics      We used real pictures of the zitec HQ. We have an integration with google docs, reading the data directly from a google spreadsheet. The project is programmed using Vue.js framework.  We built it in Python, using the Eclipse ide. We used machine learning to train the bot into answering relevant questions.  We trained a model relying on FaceNet among other to be able to find and reveal information about the user surroundings. The input comes from a camera (embedded in glasses/phones) and is transmitted via our app to be analyzed and then inferred on. Then we use voice capabilities widely available on phones to transmit audio hints to the user.  A database of playlists was created that related each with a curated level of sadness/happiness. Then, AWS Lambda was used for Alexa to prompt the user how his/her day is going, the output is processed with IBM Watson's AlchemyAPI to produce a sentiment score. The list of playlists is then sorted by the score of the user using Python. Alexa then offers the user each playlist using AWS Lambda. The playlist selected is then sent to a Java Web Service hosted in the user's computer, which pops up the Spotify web app and reproduces the playlist through Alexa, this is possible by using the computer's bluetooth.   We built it with Node.js, Express.js, and the Messenger Platform. We would have built it in PHP but since we had to have a public endpoint for our bot to hook into, it was best for us to spin up a Heroku instance since we weren't going to be pushing constantly to prod throughout the weekend.  We started from a POC idea in which we wanted to start RPA processes from Druid conversations and viceversa.Besides this we needed to make sure that everything runs automated with no human intervention. Due to this requirement we started using the UiPath Orchestrator queues to push and consume conversation business related information. When developing our sample RPA processes to be triggered from a conversation we needed a way to send back to our Druid platform, the needed information for this trigger to happen. The idea of having a custom code activity emerged including we a REST client triggering an API call to the Druid Platform.The conversations are configured with the Conversation Designer in DRUID platform. No code required.Chatting with the bots is possible in Intranet web pages (land bot page, skype for business, skype, slack or even in Facebook).  PHP backbone to handle all the core logic (account management, authentication, operations on tags and documents, etc)Postfix server to provide an "email api", so users can use Taggrr from any email client without needing to install anything new!Javascript web front-endChrome Extension to support hosted tagging (i.e tagging from 3rd party websites)      I used my engineers expertise in programming and image processing, and I programmed it in standard languages for Windows: Visual Basic, first, then I had to move to C# and even C++ and C for more efficient image processing libraries.    I used IBM BLuemix watson services to make the app.I have then used Conversation services to make the app.I completed the app in Android Studio.Voice services are also enabled but still needs improvement.  We split into two scenes with one focused on the 2d ship building and the other focused on the 3d debris field flying simultion.  There are three main parts to our implementation:Firstly, we parse the AdvLang input. We use this to create a tree to represent the logic adventure game.Secondly, we have the main game loop, which works out what options you have at any point in the game, based on the game tree. For example, we work out where you can move, what objects you can see and/or pick up, and what events should occur automatically.Finally, we print these options to the screen, and allow the user to choose. AdvLang games can run in any standard terminal. We developed our interface using curses, and wrote our menu implementation from scratch, which was way more difficult than it really should have been. I'll take a web-interface over curses any day! :P  We used a whole variety of tech stack to build this idea to make sure its available on all platforms.   This VR game was developed using Unity and SteamVR SDK for HTC Vive.    We used React-Native to build the cross-platform app that communicates to an API on a standalone web server running Node.js and using JSON Web Tokens as authentication to retrieve local restaurants and retrieve/save user settings on their dietary preferences.Accomplishments that we're proud ofWe are very proud of the overall outcome of this app, the UI, the functionality, but most of all the purpose this app can serve to the 15 million Americans who have dietary allergens and even more who want to start viewing a healthier menU at their favorite restaurant.What we learnedWe learned a lot about how React-Native works when communicating data from and to our API server. We also learned a lot more about JSON Web Tokens and how versatile it is with Mobile Applications since they lack sessions like web browsers hold. Like most projects we face, we learn a lot more about the semantics and syntax of any of the languages we used and helps us become even more comfortable writing larger scale apps with these languages.Our DatabaseOur Database uses MONGODB. When getting a restaurant on board, we have made a web form for us to add new restaurants and menu items with ease to follow our JSON schema when retrieving restaurant data, so it is modular and easy to modify. We also use MongoDB for User sign ups and dietary settings for each user.What's next for menUThe roadmap for menU is now to start talking to more restaurants around our area to get in on this idea to help the convenience and safety of their customers and potential customers. Every restaurant business wants to increase convenience, and what better way to do that AND increase business visibility by having their mark on the map with local users! Built Withjavascriptmongodbnode.jsreact-nativeTry it outmymenuapp.usgithub.com      Submitted to    Make School's Student App Competition 2017Winner                10 Upvote Prize                  Created by  I built the underlying API for app data communication using a Node.js server and built the App Infrastructure with React-Native.Karim EllaisyThomas DiCarloLouis Kim  Swift <3 + SpriteKit    We started building it in late 2014, earning a lot of recognition from the University of Malaga and some other public entities. By the beginning of 2015, we had a functional prototype, and that's when we saw the Makers Against Drought announcement. We decided to give it a try and turn NESS into the IoT device we always wanted it to be.  Binnacle is currently a prototype - it's built with React, Node, and Express. The back-end will be powered by MongoDB, using JWTs for authentication.  Accomplishments that we're proud ofBinnacle is a tool everyone on our team would use. The central idea and its implications are what drove us to start on this road. Along the way, we have stumbled on a number of great ideas regarding features, monetization, and the interface. This is an application we're going to continue to work on, if for no other reason than we What's next for BinnacleOnce we have the central functionality in place (including a secure back-end which can support authentication), it's time to prepare for all of the data the app will collect. Built Withexpress.jsmongodbnode.jsreactTry it outgithub.combinnacle-app.herokuapp.com      Submitted to    ZipRecruiter Hackathon    Created by  Gus NordhielmMarina LupinAlex EspinozaSteven Peltzer  We used MVC .NET for middle tier SQL Server for the backendAngularJS and HTML5 for the UIand VisualStudio, TFS to maintain shared code base  We utilized and Amazon Echo, Node.JS and integrated several APIs in order to implement all of our functionality. SocialAlexaIntegrating twitter with the Amazon Alexa.DocumentationIn order to integrate oauth with the account, a user must sign up with their twitter account to allow our application to make posts to their account which is done during the enable skill process. This is done through a heroku application that does not store or cache any of the users credentials and all API keys for the user are stored on each user's respective amazon account. Sample Invocations:Alexa tweetBuild SettingsNode J.S. 6.10Basic lambda execution. Advanced SettingsNeeds at least 512 MB of ram to run. (Done under advanced settings.) Needs a timeout of at least 1 minute. ContributersBrandon AshworthChristopher DoegeAccomplishments that we're proud ofIntegration of oauth for twitter with the amazon alexa (Not hard coding keys). Implementation of random text input in order to perform tweets and text messages.  What we learnedNode.JS, twilio, twitter API, oauth What's next for SocialAlexaIntegration with Facebook and other social media platforms! Built Withamazon-alexaherokunode.jstwiliotwitterTry it outgithub.com      Submitted to    Amazon Alexa Skills ChallengeRowdyHacks 2017    Created by  I worked on the configuration and setup of the Alexa to ensure that everything worked smoothly as well as working on integration with oauth for twitter messages. Chris DoegeI worked on the configuration and setup of the Alexa and the integration of Twitter with Alexa using node.jsBrandon Ashworth  We used clarifai and open cv to recognize the trash and we used servo motors to flip a piece of cardboard left and right based on its looks. The servos were connected to an Arduino.   We spent two days 3D-printing the main structure of the project at home before coming to the Hackathon. After that, once the hacking started we added the ultrasonic emitters and soldered all the components.Once the soldering was done, we started studying the wave pattern suggested by the paper and experimented on generating them with different technologies (a signal generator, plain electronics, and, finally, an Arduino).After a lot of errors and experiments, we ended up finding the right pattern and phase of the signals sent to each cone of transducers, and started developing an API in order to monitor the levitator.  We built imgrep by leveraging the power of an Infinity Stone: the Tesseract!More seriously, imgrep is a go-cli app. It's built using Unix philosophies of program design, and works as an effective command-line tool through Unix pipes.      We have an SMTP server that will listen on the 25 port, and receive the incoming data, then parse it as a Mail Messages according to the standard SMTP protocol, then extract the html part and crawl all the links in it, after that it get each link meta info and organize all the found data then send it to a predefined webhook, so we can continue processing it.We followed the concept of "Microservices", each service will do a job, our first service is the SMTP listener with the built in crawler,Golang was used to make use of its high concurrency features and the cross-compilation that makes distributing it very easy.The scrapper sends our application the links that it fetched through a webhook which triggers spreading the newsletter to the users that subscribed to.We built the front-end component using react and react-router    The drawer is made out of foam board and cardboard with an acrylic handle.  It has a servo on the inside to handle locking and unlocking.Inside there is a board with several devices.  We have an Arduino pro mini that controls the LEDs and servo.  We also have a 9 degree of freedom sensor to measure force on the box.  In addition a BLE module is connected to be able to broadcast out to devices.A service runs on an Android phone that periodically checks for the drawer.  If the drawer is nearby, it will start monitoring RSSI (signal strength).  Once close enough, the phone sends the unlock command and the drawer will open.  Once out of range, the drawer will lock.    We have a Python script running which sends serial signals to an Arduino. The Arduino then moves the servos on which the camera and gun are mounted, as well as the servo used to pull the trigger. It uses OpenCV to take photos from a webcam mounted on a servo and the Microsoft Cognitive Services emotion API.When a photo is taken from the webcam, it's passed to the emotion API which returns a value for how happy the players are. If a particular player is looking too happy, the Python script tells the Arduino to move the gun and then shoot them!  We trained the image segmentation model using neural networks, specifically, U-Net. Then we use the Residual Networks to eliminate the false positive samples. We trained the neural networks with Tensorflow framework. The model is running interactively on a GPU server. A web app is built with Python Flask framework that allows users to have minimum trouble of getting diagnosis immediately.        We built it using Amazon Web Services and have a EC2 instance that is in charge of running the database and the admin application. The bot itself works with Lex and Lambda.The Database that we use is MongoDB, since it allows us to run complex queries required for filtering huge quantities of items.The admin is done using the MERN stack (Mongo, Express, React & Node): On the backend we use Express with Mongoose as ODM. They all run using aEC2 instance.On the frontend, we use React, Redux, and Redux-Saga.The bot itself runs with Lambda and Lex and is connected onto our EC2 instance which stores the database.For some custom iteractions, we use SNS to fire custom messages.We use CloudFormation to deploy our infrastructure.  At its core, MyMusicMate is a Slack chatbot powered by Amazon Lex for its conversation logic. Lex intents are used to guide the user into inviting friends into a new Slack channel, gather everyone's musical tastes, and enter a location so MyMusicMate can start searching for concerts.  After the initial intent is triggered by a sample utterance, flow from intent to intent is handled by Amazon Lambda functions until concert voting is ready to begin. Once voting is complete, the application will either terminate if a success condition is met, or trigger a previous intent to gather more information from users.Our Lambda functions also use Amazon's Simple Notification Service to queue actions, post messages to Slack, and communicate with Lex without fear of any function timeouts.  Lambda functions are also used for Slack integration, aiding in app installation, event subscription, and handling of interactive messages (voting) routed through Amazon API Gateway endpoints.DynamoDB tables are used to store data that must persist through the application flow (artists, genres, concerts, etc.) and beyond (user IDs, team IDs, access tokens, etc.). Artists and genres are discarded when users have exhausted their concert options and decide to expand their search preferences, while concerts are kept until the application terminates, in order to ensure duplicate recommendations are not delivered. State (current intent, channel id, timeout) and current intent data (callback_IDs, user input) are also stored, allowing multiple users to converse with Lex freely without fear of accidentally invoking other intents.MyMusicMate leverages multiple APIs to validate user input and find events for users. The LastFM Album Search API is used to get related artists by genre, while the BandsInTown Artist and Concert Search APIs are used to verify artist names, US city names, and search for concerts by specified and related artists.  Finally, the YouTube Search API is used to search for live concert videos, after which all of the content is rolled up and passed to the Slack messaging API in neatly formatted messages.Our team used the Serverless Framework to build and deploy our application onto the AWS stack.  During deployment, we retrieve s3 bucket information from CloudFormation and upload our redirect page and images of the MyMusicMate family accordingly.  One of our developers also created a plug-in for the Serverless framework to deploy our Lex bot from a blueprint file, which is hopefully available as a npm package by the time you read this :)  Found devpost for this hackathonSearched the web for some tutorials( using Trello api, GitHub pages, etc. ) Really just searched the web, mostly on GitHub there are a lot of really informative tutorials for building powerups      At the core of the Smart IOCup is an ESP32, a bunch of sensors and a servo motor jerry-rigged to a cup. This smart cup connects to the local wifi network, and starts streaming the sensor data and listens to  commands. The server is implemented using node.js. It receives POST requests from the ESP, and also GET requests for the data from the client. The client queries the server and then uses the plotly javascript library to draw a graph of the last 10 minutes.What's next for IOCupThere's still a lot of improvements that could be made!The web ui could be improved,Notifications should really be sent to your phoneThe accelerometer could be used to determine how much fluid is leftUse the angle you have to drink at to calculate itShow stats for amount drunk/sip etcBuilt Witharduinoesp32htmljavascriptnode.jsplotly      Submitted to    GreatUniHack 2017Winner                1st Place                  Created by  I worked on the server, and also getting the client website to fetch and display the graphs.Karl LundstigI worked on the website, learned some html, css and a bit about get requests.Valters Jkabs ZakrevskisI assembled the hardware and helped on writing the c++ for the microcontroller.BogdanTheGeekI programmed the controller to take readings from the sensors and control the servomotor for the dipping.Jakub Mandula    Since I just an idea, to begin with, I got tremendous help from the Microsoft documentation in learning about all the technologies in use. I started with the sensors and their integration with Azure cloud in a bi-directional fashion and ended with visualization on Power BI.    We are building the game in Unity using C#. For the 3D models we are using Blender.    For the image recognition we used IBM-Watson's Visual Recognition API (using their default data) and we parsed the output using the JSON-java library. We used fatsecrets API to look up information in their database about the foods that were recognized, and we put it all together in Android-Studio.    In this hackathon I've worked only on the meteor part so I'll skip the C++ DDP impl and RPI/arduino communication.To prevent from having to build everything from scratch, I've used as many packages as I could that would save me some time.To quickly create the UI, I've looked around at other web site source HTML and CSS to "inspire" my design from them. To create the picture on the login page, I've took a picture of the RPI with an Adruino.I've done the routing with iron:router and read the tutorial which explains how to wait for data to be fully loaded before changing page, and also to have a loading screen while the initial first load.Chartist-js is an good chart package to visualize the data. It's been easy to integrate, but it was a bit harder to customize the way I exactly wanted it. Next time I'll try chart.js.Many thanks to the Meteor Toys which accelerated my development a lot. For sure I'm going to buy it.  Yesterday.    The main framework I used was SpriteKit - Apple's own 2d engine. I found it to be intuitive and straight-forward, making it easier to focus on the logic and maths to set up this mathematical phenomenon ;). The UI was pre-sketched in Sketch, a brilliant design tool I found to truly see how the final product would look, but I think most of all the vector drawing made the quality of the images brilliant, considering I opted for a clean, minimalist geometric style. The music was made through Garageband, and though it may not be the focus of the game it proved an adequate backing to the game.   We obtained a database from EverQuote, which contained data over a period of time that identified when people at each specific latitude and longitude location were either speeding, accelerating, using their phone, or braking. Each of these points were anonymous, and we used JavaScript to parse this data file to both obtain points within a 0.5 mile radius along the route, and applied a subjective weighting system to characterize the score based on the density of each of the four events described above. We then overlapped crime data in the same fashion, and applied a final score based on this data.Challenges We Ran Into:A major challenge we ran into that limited the reliability and capability of the application was the limited amount of data we had to work with. This made it difficult to scale our idea, since we only had a limited batch of locations and a limited time frame in which the data was collected. Another major limitation we ran into was the lack of immediate availability of comprehensive crime data. Much of this data was gathered in or around metropolitan areas, so the scope in which we could extrapolate results was severely diminished. This also offered a scalability challenge, since the scores would be skewed unfairly if the route took an individual through a major city. Accomplishments that we're proud of:We were thrilled to get the application up and running at the level it is at. Two of our team members were first-time hackathon participants, so there was a lot of explanation and instruction along with writing code and fleshing out the idea. However, we were happy that we were able to take a large static data set and actually create something of immediate use for those concerned about theirs, or their families safety when travelling. We also are very excited about the various ways in which we can scale our idea. Future Scalability and Usage of VeriRoute:VeriRoute was designed in such a way that it can be scaled for a larger and larger area based on the amount of information in a given CSV file. As more and more CSV data is provided to us by EverQuote, more and more accurate routing predictions will be given by VeriRoute. Also, increasing the amount of  comprehensive crime data around the U.S fed to VeriQuote will provide it with a means to provide more accurate routing predictions.International Scalability can be achieved as well, provided that accurate crime data in the nation is released to the general public.Built Withbootstrapcssd3.jsgoogle-placeshtmljavascriptleaflet.jsmapquest-directionsTry it outwww.github.com      Submitted to    HackDartmouth III    Created by  I helped work on the API Integration with MapQuest using JSON,JavaScipt and the Leaflet framework for JS, and write the CSS for the site.I learned a ton about API Usage while working,and strengthened my bases in CSS which was awesomeAnurag AkkirajuI helped work on the data analytics to contextualize each route, and specifically assisted in designing the algorithm to compile the safety metric. I also dabbled in some of the front-end development, and learned a ton about API usage, web development, and design.Josh SuretteInterested in big data analytics, particularly in the application of computational biologySumanth Reddy PandugulaSai Priya Jyothula        KashBot is built over the Coinbase API. It also uses the XE API to perform the currency conversion. The conversational engine of the bot is built over the Microsoft Azure Bot Service infrastructure. Finally, we used Firebase in order to manage friends and their addresses. Built Withazurecoinbasefirebasenode.jsTry it outgithub.com      Submitted to    Hack the North 2017    Created by  I worked on building intents for LUIS, the intelligent language service, coding the link between intents and the natural responses the bot gives. Continuously debugging API code for Coinbase, Firebase and XE while collaborating code on Azure Bot Service.Kallen TuI developed the idea and planned the features we wanted to have for the end of the hackathon. I was responsible for handling all the Coinbase transaction functionality, including currency conversion using the XE api.I also worked on setting up the core chatbot framework using Azure bot service. samjawichHermes ValencianoI'm a passionate about creation, technology and design. Compuer Science at UPC.abderrahmen gharsallah            I hacked this together on Sunday using AReality3D RealityScript, a rapid prototyping framework built on top of Unity (that also builds to iOS/Android/Windows Phone/WebGL etc)I'm currently using regex "any match" for subject search, but the default exact subject line search that ContextIO uses may be more fitting if we get more people sending emails they want to display.     We built it using Ethereum blockchain, NodeJs, ExpressJs for server side programming and solidity for smart contract programs.  The high level architecture that has been used to build MyCity can be seen above. The core pieces are:ElasticsearchElasticsearch was chosen due to its speed and simplicity when trying to query large datasets. Due to the large about of free text data that is collected by MyCity we needed a way to effectively perform simple NLP (Natural Language Processing) to extract key words from large custom text fields.Facebook Messenger (Bot)Facebook Messenger serves as the easy entry point for the community. Our bot handles the 3 step process of:Uploading an imageProcessing this image with object detection and returning a set of helpful tagsPrompting the user for more information including their locationConfirming if they'd like to receive a follow-up notification when the issue is resolvedDisplaying the users contribution stats (points!)React Native NavigationWe used React Native to develop the iOS and Android application for the city agent.They will be able to visualise to issues locations, select the one they are interested in and the application will open the native navigation to this location.Open DataWe created an end-point to publish the anonymised data we are collecting. The data we aligns with the standards set by 311, and the intent is to eventually feed the data we collect into their database; Bucket Hosting for DashboardThe frontend dashboard is hosted out of an AWS S3 bucket due to the static nature of the app. Our Frontend is written in ReactJSMachine LearningOur machine learning backend is powered by Google Cloud's Vision AI which analyzes and returns helpful tags from the uploaded image for the user to select, thus reducing the amount of effort from the user's side.  For the front-end, we used 'React' and 'React Bootstrap' with 'JSX' elements throughout our code. This involved the responsive designing of the landing page, decision page and the result page. The back end runs on 'Python' , 'Fast API', and 'AWS'. We use Fast API, a python microframework for web dev, to build a central API around the other APIs we are using (Google's Geocoding for location, Meteo for environmental factors). This allows clear and consistent API calls from the client while handling the rest on the server. Our users are stored in an AWS DynamoDB table and receive a unique key upon successful login. The API is hosted on an AWS EC2 t2.medium instance which accepts HTTP.  Arduino: hardware to measure water flowReact, Next.js, HTML, CSS: Building website and application.Figma: Wireframing and Prototyping  google translate api, webrtc, jquery mobile, canvas, ocr    OneGraph is built with ASP.NET, hosted on Azure, and powered by the Microsoft Graph.  Built using Java Discord API (https://github.com/DV8FromTheWorld/JDA), Java-LAME (https://github.com/nwaldispuehl/java-lame), and Google's GSON library (https://github.com/google/gson)  Regarding the hardware, the mentioned features rely on a microphone (for recording the voice), a movement sensor (for recording physical activity), and an ambient light sensor (for recording insufficient exposure). All of them are included in the Simplelink CC1350 SensorTag by Texas Instruments, which is the core part of the bracelet. It is able to connect via Bluetooth and sub-GHz. Because the battery lifetime for operation on sub-GHz is longer, it is our preferred option.The data is submitted to IBM Bluemix and acts as input in the Node-RED application. MoodAmigo analyses the data with several Watson services, such as the personality insights, the tone analyzer, and the sentiment analyzer. Dashboard information is continually updated and in case of a negative trend MoodAmigo messages predefined acquaintances.To avoid sending out false alarms, the sensor data is aggregated over a period of time. For this purpose, a sliding window protocol is used.  The application consists of a react-native app that can be installed on both iOS and Android devices. The app communicates with Google Cloud Vision ML, applying a neural network model that we built and trained, to identify bloodshot eyes. It is then passed through a computer vision algorithm which we handcrafted in opencv for overall redness detection. Next, there are several games built in react-native to assess cognitive speed and impaired judgement. Google Cloud Platform's Speech Processing API is also used for short-term memory testing. Finally, summary screen displays an estimate of the user's sobriety.    Our fellow colleague Devan (engineer from Staked) was kind enough to introduce us to their team to provide api tools for integration. It allowed our protocol to earn interest via Compound, bzx and dydx. DC could not have been built without an interest bearing product. Earning interest was the missing link to build DC -- it keeps the users' funds evergreen.   I built the Game Plugin in C# first and got actions and a system to trigger them setup.Then I worked on the backend that communicates with the broadcaster's game with socket.io.The backend uses Node.js, stores data in a MongoDB database and uses Express to route the API endpoints.For the frontend I picked Vue because I have some knowledge of how it works and with the short time span I didn't had time to learn even more new systems.I found a good bootstrap for Vue to work with twitch extension and got to work.The frontend uses a lightly customized Bootstrap for styling.  A series of scripts take the raw, large GPS data from the Metropia data set and produce a smaller, cleaner version suitable for the web. We drop points that don't add much to the visualization -- We can get away with a few points for a 10-mile straight journey on the highway, but a meandering trip around the neighborhood requires more fidelity.Next, we ship the data onto a web client. This is where things get tricky -- the raw data set consists of 12 million points and nearly 800 megabytes, and we need to fit this into browser memory and compute budgets. Additional filtering where possible and good choice of data structures lets us present an interactive experience.  An arduino converts aluminum cans into capacitive touch sensors that send MIDI data over USB, which gets translated to a MIDI signal, which is then fed into a virtual MIDI port on the PC to a MIDI input in a MIDI software synth. All of the cans are empty and connected with stickers given away during the hackathon.  Node js backend that forms a link between the world of IRC and the MessageBird API. Also uses MongoDB somewhere in there for persistence.    We used HTML/CSS with JavaScript to build the front-end interface and Node.js with Express.js to build the back end. Collaboration is enabled using Socket.io, and we have an Ubuntu server running on Linode.     We built the game in iOS using Swift. We used Microsoft Azure to create the predictive machine learning model which analyzes the cumulative data (EEG and reaction times, stored in a Parse backend) from each user and predicts the probability of developing ASD. For reading data on brain activity, we used the Muse headband technology and the LibMuse SDK.             We have created a back-end server which have everything in a database. All the latitudes and longitudes are saved for the stops and routes. We have used bootstrap framework to create a unique user experience for the website and google maps to create the routes and stops. For mobile application we are using openstreet offline maps and json formatted information about the stops and routes.    This needs a lot of low level coding using C++.The gestures by touch need to be recognized with complicated algorithms.  The project is split into several portions:Mobile ApplicationsNode.js APIAWS Lambda ProjectMobile ApplicationsThe Android and iOS applications are built using the ionic framework.  This allows us to leverage the javascript skills we acquired from the API below to create a cross-platform mobile app with a single codebase.Node.js APIThis API includes the logic to authenticate a user with their Amazon Echo device.  It also handles the account creation and authentication needed to upload their contacts.  This web service uses Bcrypt to encrypt passwords along with sqlite for quick and easy data storage.AWS Lambda ProjectAWS Lambda provides an interface that allows us to upload our NodeJS code for our Custom Alexa Skill, install the new skill on an Amazon Echo device, and run the code in response to the designated skill wake wordBuilt With: DigitalOcean, Nginx, Alexa Skills Kit  We used OpenCV with its Python bindings to gather an estimate of the position on the board. This continuous scanning is fed into a ReactiveML controller, whose responsibility is to both eliminate noise and ensure that the moves are conforming to the rules of chess. The controller finally issues feedback sentences to the speech synthesis module which relies on the IBM Watson APIs to produce the sound samples.    A mechanical hand was 3D printed, filed, and assembled.  An acrylic sheet was cut and five servo motors were attached to it.  This acrylic sheet was attached to the base, which was attached to a wooden plank.  Each finger contains two lengths of fishing line: one that straightens the finger when pulled and one that curls the finger when pulled.  The fishing line that straightens each finger was attached to a rubber band mounted on the base of the hand. The fishing line that curls each finger was attached to a corresponding servo motor arm.  The signal wires of the servo motors were connected to an Arduino Uno.  The power wires were connected to a separate USB power supply.  Rubber bands and cable ties were used to secure most components in place.The software to control Wave was programmed in JavaScript using Node.js with Arduino and Leap Motion modules.  Our team met for the first time ever at HackHarvard, and we all had similar goals: build a challenging technology project that could help people connect with and learn about traditionally disconnected topics. We were lucky enough to have complimenting skillsets and were able to split the project into two major parts and get to work quickly.The visual interface was built almost entirely from scratch and we even did a little user testing during the hackathon. The database and API technologies were entirely new to our team but successfully leveraged technology from HP, Twitter, Yahoo, MongoDB and more.  separate client and not for profit apps ('management app').client gets subscriptions of whats required from management app.methods and calls to make purchases velocityjs for animations            The safety notice function is added to a business software of existence.                              Using React Native, then Ionic, .NET API, passion and courage to change everything halfway through.    A significant amount of time was spent brainstorming how we would implement a blockchain that would function without cryptocurrency, and shift its focus to function as a voting ledger. We eventually settled on creating the backend with Python, using Flask as a local server to display the front end user interface. Bootstrap was used for CSS theming, and JavaScript for making AJAX requests between the front and back end.    We used HTML to create a website, java script to create the game, css for design.    We've developed The Chatty Skeleton using Angular 7 regarding the front-end application running on Twitch.Our backend dashboard is developed in Python 3, using the framework Django, and deployed with Docker on Amazon EC2 and Amazon S3. Also each relay server is deployed using Docker on EC2.Thanks to an orchestrator application, like Terraform, we will be able to deploy multiple relay server as needed and manage cost as we growth keeping the complexity at the minimum.  It is built as a python module, so other developers can install it using pip. We first created the classes that represent the strategies that would be used, like the abstract GridStrategy and the implemented SquareStrategy. Then, we set up all of the necessary tools that we needed for automated testing, code coverage, changing code format using black formatter etc. We then added documentation with Sphinx, and uploaded it to PyPI so users can install it.  The way our application achieves the above functionality is making use of image detection features of OpenCV. We detect the images in the video stream and coordinate them with existing images in the local storage. Green boxes over a person's face mean that the face is from one of the known images, and red means it is an unknown person. Any time a red box pops up, the application records the stream and will save a video of the person for future use. Different poses of the human face are used to improve the accuracy of the prediction.  The project involved building the data set of scaled, attributed product images and building the web interface. I gathered silhouette photographs and product attributes from Wayfair.com and processed them with Inkscape and Python scripts. I built the proof-of-concept web interface using a slideshow image viewer as a starting point.  We used Android Studio, Firebase, and Microsoft Azure.     Using dreamweaver, html5, css, and javascript.  We have built TravelSafe on the android platform and used PHP in the backend support. The places, directions and navigation featues have been implemented using Google APIs.  The language itself is compiled on the back-end to valid swift code, which is executed and returned to the client.   web application  backend  mobile app  javascript  HTML5/CSS  express.js  material-ui  Data.gov  Social sharing  Sync 3  DDOT  Gov Detroit  SMART  APTA  TransitLand and TransitFeeds  Amtrak  AIOur current app with rewards platform is available on iOS and Android App Store. Our Connected Car Sync 3 app is in a testing phase with Ford engineers and will be rolled out in Ford vehicle soon. (Oodles Rewards, previously known as FuelSignal). We are developing mobility app features on top of our existing app by utilizing City of Detroit and transit data to aggregate commuters journey data and analytics to improve quality of life of Detroit residents and revive the economic vitality.  The app is built using a node.js backend with the express framework, the website is rendered using jade templates and bootstrap and the persistence layer consists of a mySQL database. Everything is hosted on AWS.  The HoloMeeting-Client was written in Unity3D as UWP app using C# and uses websockets to get real-time events from the meeting server (node.js). The PowerPoint Add-In based on C# Xamarin/WPF uploads the presentation to the meeting server in the cloud and generates a QR code to be sent to the meeting attendees.    Easy Agile Roadmaps is a serverless static Atlassian Connect app served from AWS CloudFront. Making full use of the JIRA Software API, we were able to ship this add-on to JIRA Cloud without the need for a server at all. A huge maintenance and cost saving!Easy Agile Roadmaps is built with React and Redux, which allowed us to concentrate on the finer details of the roadmap. For example, when resizing an Epic pressed up against another, you will most likely resize the one you intended. It's details like this which we were able to spend time refining by using Redux instead of less important details other front-end frameworks can make you deal with.  I wrote the main code for the game in 2 days and it took me weeks to design the game.   JSON stored in local cookies in order to communicate dataCSS and JavaScript used to dynamically represent the information  We brainstormed possible sentence structures that users might want to ask for and developed a model which captures all possible scenarios.We define the meaning of the sentence structure by looking at the different components and convert that into a database query.We used DynamoDB and node.js to store and retrieve the users information.  The Ambient Noise Skill is built upon the Node.js Alexa Skills Kit Audioplayer Example for AWS Lambda as a baseline for building proper Alexa audioplayer responses and persisting user data with DynamoDB. Since most folks listen to these ambient noises while they sleep, it was vital for the Skill to never abruptly switch sounds and to be able to handle an influx of traffic during nighttime hours. To address these concerns, I modified the example project to disable the concept of a "playlist" in favor of looping a specified track and used AWS Lambda to run my project's code. Additionally, I used incrementing values in the user's DynamoDB record to keep track of how many times each sound has been played and when the user was last active in order to personalize the Skill's speech and playback behavior in the future. All in all, it's not a horribly complex Skill, but a lot of thought went into creating a great, reliable user experience - and when it comes to voice applications, that's half the battle!  Native Android app built with android studio. Version control with GitHub. We used IBM Watsons Speech-to-Text for converting the voicemail speech into data that we can analyze. Key word analysis with related words. We used Google Calendar API to create Woxis smart reminder skill. Facebook messenger Chatbot is built with Facebook Graph API, Heroku, and Node.js.  We created a Ratpack service to run in AWS that handles installations and storing configuration. The configuration page and sidebar are built in React with Redux.      We thought of having a single homepage where you could find the map with all the desks already added. There is also an admin page where someone from Operations or from HR could add new desks or assign new colleagues to them.As a technical point of view, it was build on Symfony 3 with the implementation of Symfony 4; we also used Backbone.js for some of our frontend tasks and a custom jQuery plugin called mapplic that helped us with the implementation.    Project consist of 2 parts - robot behaviour project and server that classify human movements.Robot behaviourRobot behaviour project do next things:Provides voice interface between human and PepperTakes pictures of human and send them to server that tell robot what to doTranslate server's commands into hands movements and some speechInside the robots there's behaviour's project with Python scripts and hands animationServerServer is responsible for classifying human's hands movements and sending a robot commands how to move his hands.How it works:Server receive an image from robotThen server find human's hands (with OpenCV lib) and crop image to classify themTensorflow-built network classify movements and answer with some hand's action that's sent back to robot  We used our little knowledge of Unity  We started by implementing simple instruction sets which are common and essential in any programming language. It includes, but not limited to:basic instructions such as initializing variables and doing mathematical operations on variables. flow control instructions such as if-clause and for-loops. input and output instructions.  Build app with C# and Visual Studio. We developed app with Bot Framework.To fetch/post data in uses' business tools, we used Microsoft Graph API.We deployed app in Azure Web Apps.For communicate with users, we try to use Microsoft Teams channel in Bot Framework.  We built this with Node.js and the Atlassian Connect Express framework. The SonarQube plug-in for Bitbucket is built with Scala and uses the SonarQube plug-in framework.  Cleito ODCC is basically a Jar file that you install in Crowd, so Java was the language to use. The plugin mostly uses Microsoft Graph API and a bit of Azure AD Graph API and Office 365 Exchange Web Services (EWS).    20 Cups coffee, some cool hardware and lots of arduino and android code.Accomplishments that we're proud ofBeing able to run animations over bluetooth in such a short period of time.What's Next ?Mass production of the table to the relevant clientele!Built Withandroidandroid-studioarduinoglediatorTry it outgithub.com      Submitted to    MHacks NanoWinner                Top 12              Winner                All Submissions                  Created by  Electronics Hardware and Table Concept and DesignAkshay Bawejahttp://akshaybaweja.comAndroid DevAmanjeet SinghAndroid enthusiastBhagat SinghJasmine Sodhi  We source our captions from multiple systems (some of our own) and stitch them together with an intelligent algorithm that can come up with most accurate transcriptions.  java, JIRA API,  AUI, AJS, jquery, atlassian spring scanner, gson, active objects, REST, javax-servlet  Starting with an MVP as POC. Docker containers inside a vagrant box for PHP based API server using swagger.Swagger is a powerful yet easy-to-use suite of API developer tools for teams and individuals, enabling development across the entire API lifecycle, from design and documentation, to test and deployment.firebase for sign-in authentication and analytics.firestore to store application dataexample App "LaaBas" to provide effective medical assistance via utilizing nearby volunteers in addition to the official medical help provided by the Saudi government and Hajj organizations.      We built it using Presto API along with Android geolocation, calling, and texting.   Using Android and awesome Office 365 API and examples.    The lambda function is programmed using Java 8 and the back-end service is built using java,spring-boot    We used an AWS EC2 instance to host all of our code. We created a CGI handler page using python that we connected as a webhook to twilio. Whenever someone texts our number, it goes to the CGI page. That python script connects to multiple databases for translations between names and phone numbers, getting patient information, and getting approval for doctors to get information.  Android - we just have our own implementation of signing and sending transaction. So we just needed to catch event from the webView on the phone, track metadata, pass it to our core lib, sign transaction, broadcast it to the ethereum node -> profit.IOS - we just added import ethereum wallet logic witch control GNOSIS multiSig smart contract, and of of cause address of the GNOSIS SC. After that we added UX for creating, confirming, tracking  transaction transaction for this Smart Contract.    Starting from the UIPath tutorial, I create a code activity based on the outlook .net class. I made a test console to test the .net functionality and later on a test module to invoke a workflow.            The project was build with Android Studio and OpenCV.  Python was super helpful! The front-end chrome extension was developed in JavaScript. The chrome extension interacts with a back-end server developed in Python. The machine learning models were trained in a cloud environment and their weights are downloaded to perform local inference. The back-end server consumes the machine learning models when inference is needed and returns to the chrome extension information on what needs to be censored.    The whole project was originally called called Bobbleheads, and it won Hacketse  our internal hackathon that happens multiple times over the course of the year. A few months later we were proud to have it picked to be presented at Atlassian's ShipIt Live event, and then lucky enough to end up winning as the audience's top choice. Woot!Pushing this hackathon project forward to become a real product that gives real customers a good experience was the next challenge. Though Conversations is different than the original Bobbleheads, it delivers the benefit that we wanted from the beginning  natural face-to-face communication right within the Confluence editor.We're really proud of what the team has accomplished with Conversations Video Chat for Confluence.What we learnedStringent constraints such as UI extension points can work in your favour - sometimes doing less really is more.What's next for Conversations Video Chat for ConfluenceWe would love to see this product adopted by customers in order to get some feedback. Although we already have some additional features in mind, we want to listen to users first to identify their needs, and invest time where we can help them most. And as not all of you are using Confluence in the Cloud, we also have plans to work on a server version in the near term.Built Withamazon-web-servicesatlassian-videobitbucketcss3html5javascriptnode.jswebpackTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist 2017    Created by  I worked on the backend as well as the frontend.Riku HaavistoI promoted the project internally because of all the positive feedback we received at ShipIt Live '17.Nils BierI was involved in the UX concept and the design of the interface.Olle RundgrenI developed the first prototype - at this time called Bobbleheads - which won ShipIt Live '17.Simon KustererI integrated Atlassian Video and developed the event infrastructure and event message flow.Candid DauthK15t SoftwareK15t Software powers painless collaboration.  We re-wrote the story by adding alternative paths and endings. We also added the whole story told from the Wolf's point of view.Later we performed voice recordings in a home studio with our friends and their kids.We also enriched the story with immersive sound effects and music.Finally we found a talented illustrator to enhance the Alexa skill with unique visuals and images.      Since the idea was to create an application that will teach programming in VR and 3D, I had to figure out how to make the app usable in both case. The first approach involved creating a text IDE within the scene that would allow users to type the code, but that idea would not workout because VR users will not be able to access the keyboard that easily. So I had to think of another way to allow users to code within the scene. While doing some research I came across a block-based programing library called Blockly, this library is used for creating visual block programming languages and editors, this was perfect for my project as it would allow VR users to drag blocks around with a pointer and also allow browser users to drag blocks with a mouse, track-pad or touchscreen.The next step was to integrate Blockly with Amazon Sumerian, this was a challenge, but after a few days I managed to setup the scene with an HTML-3D entity that displays the Blockly toolbox and workspace. Next I had to figure out how to control the 3D objects in the scene using the Blocks on the HTML-3D entity this was also a challenge, but I figured out how to do it by emitting messages to the state machine each time the user drags a block to the workspace. Next I programmed an 8x8 grid that would define the path the robot has to take to get to the goal and I also designed and implemented some levels.Next I created 2 more HTML-3D entities, one to display the generated JavaScript code and the other to display information. Lastly I added a Sumerian host with a dialog component to access the Amazon Lex chatbot.  Web3-based Dapp with a smart contract allowing to perform multiple exchanges in single transaction.Build with latest Angular and Web3 versions.Deployed on IPFS under:https://gateway.ipfs.io/ipfs/QmaGAxWTC1Eqe65HPZsGJUch4vMBWgMHRmH9gGXh9rdFU9/  We build using Customer Chat SDK to develop the chat room experience into the live coding editor.Each snippet is supported by a Docker container, where the user could be free to use an full feature bash terminalWe are spawning child process to provide a terminal thought a pty connection with the docker container.We made an small hack with Messenger Bot for act as a chat room manager.ChallengesUse Customer Chat SDK (Beta) for first time. Provide a full feature terminal for run your snippets online.Find a new way to use Messenger Bot to connect people. Achieve a perfect code edition synchrony using websocketsBuild many of docker images for each language supportedWhat's next for live-coding-circleStream directly in a Facebook live video ( I couldn't finish this feature before the deadline :c )Video & Audio streaming to improve the experience Show in the browsers the website result Record a session and publish the video in the siteMultiple files supportSnippets CollectionStatical Analysis using lintersBuild IDEs plugins to provide this features tooBuilt Withbotcsscustomer-sdkdockerfacebookherokuhtmljavascriptmessengernodjsptyrubysocket.ioTry it outlive-coding-circle.herokuapp.comgithub.comgithub.com      Submitted to    2018 Developer Circles Community ChallengeWinner                Second Place - Regional Round              Winner                Bonus Prize: Best solution to Build and grow community                  Created by  Build the live editor & ui designjoel ibacetaFintech Expert / Ruby Hacker / Hackathon Enthusiast  React Native, Node.js, Google Cloud Vision, Google Cloud VM, MongoDB, Material UI, Adobe XD.  We've implemented a mobile application that shows relevant location based information and invites the user to engage in a conversation with an automated survey engine. The survey engine has been implemented using the Watson API. We've presented the location based information via a map view using the ESRI API, and via a scrollable list to simplify browsing POI's directly around the user.  We bought a black ABS pipe at Lowes prior to the hackathon and sawed it in half. We then 3D printed the floating disk using PLA with a 20% fill setting which should allow the plate to float. What's great about this project is that all the electronics (minus the Dragonboard Linux board) used came from a single Elegoo Arduino kit! We prototyped the sensor with an Arduino and a breadboard. Using pyserial, we were able to communicate through Python to Arduino. When the button is pressed, we make an API request over to our serverless Cloudflare worker. This worker would then alert locals via SMS through Twilio. To subscribe to alerts, a community member could text subscribe to our Twilio number (which only supports Aaron's number since we're on the free account). We then used Omnisci to map flood data to determine which areas would be a great fit for nowo's ark. We marked down a few areas upstream which could help effectively warn communities downstream about impending floods.   GUI, Block programming wrapper, NN Creation API and PyTorch.  We built this using Amazon Lex, Lambda, Simple Queue Service(SQS) and API Gateway. We have also used Twitter banking API and a mobile application.          The mobile App contains the necessary libraries to process the incoming Brainwaves. The Headset measures our Attention Values on a scale of 0-100When the attention of the subject reaches a predefined threshold, the Wheelchair starts moving forwardIn Order to turn Right, the user blinks twice in a span of 1 second above a threshold of 100.The Attention and Eye Blink values are also stored in the MongoDB Mobile Database every second as the data comes from the Brainwave Headset.The data in the database is then processed to calculate the Average Attention and Average Meditatiion values of the user also also to determine what Attention and Eye Blink thresholds would be best for a particular user since it varies from user to user.What's next for BrainHack Wheelchair - Smart Brain Controlled WheelchairFuture possible implementation: Obstacle Avoidance -Automatically detect and avoid Obstacles.Auto Pilot Mode - Think about a Destination such as Kitchen or Hall and it will get the user there in Autonomous Mode.Emergency Braking System - If user abruptly closes his eyes sensing some danger, then the Wheelchair will stopStaircase Climbing Mode - To assist the elderly to help climb the stairs.Built Withandroid-studiomongodb-mobileneurosky-eeg-headsetpythonraspberry-piTry it outvinittodai.wixsite.comgithub.com      Submitted to    MongoDB World HackathonWinner                Top 3              Winner                Best use of MongoDB Mobile              Winner                Best Individual Project                  Created by  Vinit TodaiEngineer by profession, Writer by chance, Maker and Startup Enthusiast    Using Ruby on Rails and a PostgreSQL database to store the users. Twilio API is used to interface with SMS numbers, and Dictionary.com, Google Maps, and Geocoder are used to locate hospitals + provide definitions. Custom multiple-regression algorithms were built to help improve the accuracy of the monitoring solution over time. Custom fuzzy-match was created for blood glucose level inputs.        The back-end is essentially a RESTful API written in nodeJS. The async functionality and fast I/O of nodeJS makes it perfect for our application.  The front-end website (for users to register their voice) is accessible from the chrome extension. The chrome extension (password managing) is written in JS; it records the users voice and forwards it to the back-end server via HTTPS, which then uses Knurlds API to verify the persons identity.  We built a simple robot in mindstorms nxt as a platform to test on, our service will work with any robot which can hold a camera, such as a drone. We built a bluetooth serial library from the ground up which runs precompiled programs on the robot to move it through serial commands. We then ran this library off of a node js server, which coordinated data gathered from the iPhone camera and allowed the user to control the robot through either voice or gesture commands.          We use Firebase for synchronizing data between Android and the Web platform, and AngularJs to make use of 3 way binding between the markup, js, and database. The mobile client constantly listens for changes on the database and makes changes accordingly through the use of our extended UI Classes.  Using the functionalities of the Leap Motion sensor and some clever vector physics, we were able to register each and every stretch as a very specific type of gesture that is recognized by our program. Then we made a game out of it to keep the stretches random and interesting.Accomplishments that we're proud ofOur program has a lot of features one would expect from a professional physical therapy session. For instance, each orientation command given during game-play will not repeat twice - thereby eliminated muscle overuse. Furthermore, we know that the heart of therapy lies in multiple sessions so we have imputed functionality that increases the constraints to our stretches as the weeks go by, essentially making the exercises more strenuous as you are regaining mobility. Post rehab, clinicians can analyze collected statistics that show the number of days taken to reach full muscle ability (i.e. max score). Here they can ensure that the patient's recovery proceeded on the correct time scale by comparing to the mean recovery time.Implications of LeapMedConsidering the time saved from outpatient clinic visits, both for the patient and the medical professional, not to mention the insurance cost savings, hospitals can adopt this solution and rent it out to patients undergoing light physical therapy or use it as a method to keep patients exercising at home. This decreases co-pay costs for patients and opens up the schedules of physicians.Built Withguijavaleap-motionnetbeans      Submitted to    MedHacksWinner                Best Leap Motion Hack                  Created by  I worked on some of the initial logic for vector modelling and contributed to game design.Victor WangI worked on the research, business and marketing aspects of development as well as perfecting the consumer experience. Look out for our app on the LeapStore!Himanshu DashoraI worked on setting up the GUI that included the tutorial and the game itself. Benjamin PikusWorked on GUI design elements and logic for the programming.  Parth SinghThe majority of the programming, some of vector modelling, and some of the overall planning for the projectRahul YerrabelliAdam Polevoy  The WeeFee Android app interfaces with existing hotspot technology. We group users into two subsets - lenders and borrowers. Lenders automatically create hotspots that borrowers can recognize. In near real-time, we monitor the amount of data flowing out of connected borrowers and keep a running sum of this amount for each lender. No public API exists that lets us easily monitor network traffic or manage hotspots without root access. We got around this by delving deep into Android's source code and using Reflection to expose critical network functionality. We subsequently fed this data through an API we built running on top of a LAMP architecture hosted on AWS.Our approach is scalable, secure, and lightweight. We also built a data analytics layer so that users can monitor their activity on the application.   Project still to be started    We have used the Google map API for the maps. Our data source for the news come from the API provided by the New York Times. We have done the sentimental analysis of the news using IBM Watson's Alchemy API. We have integrated all these API's on to our web app which is designed using HTML5, CSS3, Bootstrap, Jquery, Javascript, and AJAX. We have also done the analytics based on the data and did graphical representation of data using Chart.js  It uses the Web Audio API to get the input from the guitar, which effectively acts like a microphone when plugged into the phone. It then uses Socket.io-p2p to stream the audio via WebRTC to the 'listen' page. The Node.js server is there only to serve the static pages and to facilitate the handshaking of the p2p WebRTC connection between the 'play' and 'listen' pages.  The Roll Together Route Data Cloud backend was implemented as a Python/Flask REST API backed by MongoDB as our database. This system keeps track of trips our users have made, and computes routes from those trips using a nearest neighbor clustering algorithm. It also exposes a REST endpoint which OpenXC-equipped vehicles can use to upload realtime trip information in a standardized JSON format. The Roll Together Crowdfunding Platform backend was implemented as a Node.JS application which talks to the Route Data Cloud, serves up the admin dashboard, and provides an API used by the mobile app. The Roll Together Mobile App was implemented using the Ionic / AngularJS mobile application framework running on top of Apache Cordova. For this prototype, we only tested and submitted an Android build, but our decision to use Cordova makes developing and releasing an iOS app is extremely easy.The Roll Together Admin Dashboard was implemented using AngularJS with Angular Material theme, integrating charting support from Chart.js. It pulls data from both the Route Data Cloud and the Crowdfunding Platform and renders it on a site designed for desktop browsers.  We use python for the code, all configuration is stored in dynamodb and is encrypted by using Amazon kmsWhat's next for NimbusMore functionality and move from read only to read/write functionality  Built Withamazon-dynamodbpythonTry it outgithub.com      Submitted to    AWS Serverless Chatbot Hackathon    Created by  Did PRs and Python advice.Ory BandSocial tech-savvy guitar player on wheels.Aviv LauferCTO @ Rounds, Ultra runnerElad Weizman    Using Meteor the Front end and Egalitarian Routing was built. This can be accessed via desktop, laptop, mobile and tablets.The Blockchain part was built on IBM Bluemix Blockchain (Beta).       We have classification networks repurposed to generate images (inspired by: Gatys et al.)    I built it using vanilla Meteor 1.2 (no third-party packages) and ES6.            We applied linear geometry results to project n-dimensional hypercubes (or any shape at all) into the screen, with a clever use of matrices to surpass the 4x4 limit of GLSL  Using Android and Microsoft Band SDK for Android.  I built it using MyscriptJS PHP ,HTML5 and javaScript        We discussed our concept with workers from Red Cross, AMES, and people who have settled in Australia as refugees and asylum seekers, and used their feedback to narrow the focus of the app. We spent a lot of time developing the concept, to make sure it fills a need that is not catered for,  We created a paper prototype followed by a clickable wireframe.  A webcam and pico projector mounted above desk + OpenCV doing basic computer vision to find all the pieces of paper and the keyboard.      With a lot of tears.We use RFID technology to log locations, and we have a NEO block chain running on an Amazon virtual machine that logs the location.     We built an android app using the NativeScript framework while coding in JavaScript. Our graphics were created through the use of Photoshop.  What the user seesDream.it uses a website as the basic entry point into the service, which is run on a linode server. It has a chatbot interface, through which users can initially input the kind of garment they are looking for with a few details. The service gives the user examples of possible products using the Bing Search API. The voice recognition for the chatbot is created using the Bing Speech to Text API. This is classified using a multiclassifier from IBM Watson Natural Language Classifier trained on custom labelled data into the clothing / accessory category. It then opens a custom drawing board for you to sketch the contours of your clothing apparel / accessories / footwear and add color to it. Once the sketch is finalized, the image is converted to more detailed higher resolution image using Pixel Recursive Super Resolution.We then use Google's Label Detection Vision ML and IBM Watson's Vision APIs to generate the most relevant tags for the final synthesized design which give additional textual details for the synthesized design.The tags, in addition to the image itself are used to scour the web for similar dresses available for purchase Behind the scenesWe used a Deep Convolutional Generative Adversarial Network (GAN) which runs using Theano and cuDNN on CUDA. This is connected to our web service through websockets. The brush strokes from the drawing pad on the website get sent to the GAN algorithm, which sends back the synthesized fashion design to match the user's sketch.   I designed an algorithm that helps Judge Lexy determine if the defendant was guilty or not. I crafted a user experience that covers many use cases and is meant to be fun and engaging for the kids even if Judge Lexy rules against them.I also added a few easter eggs to surprise the kids, and put in explanations of some basic legal terms that the kids can ask about when they're unsure (i.e what's a prosecutor?). Technically, as I wanted to focus on content and user experience, I used thestoryline.io to build the entire skill. There are dozens of different logical paths and various possible responses for Lexy to make her character more engaging and the overall experience more entertaining.  we've installed Kali Linux in a Docker container;we've setup OWASP ZAP in this configuration;we've built a dashboard with Laravel Nova;we've created a flow that consumes the ZAP API inside Docker.    We used node.js/express.js to setup API endpoints on the backend.We used HTML, CSS, and JavaScript for the frontend.We used fetch to consume the API endpoints we had created.We used GitHub to manage our workflow and contribute individually to the project.We used Heroku to host our backend.We connected our frontend on GitHub Pages to a domain on the .tech platform    We built it using AWS Lambda and Dynamodb. Also we needed to scrape off the medicine names from multiple websites for the slot value containing the medicine names. Dynamodb acts as memory for Med Pal. We also had to keep a check for medicine taken or not which played a vital role in report generation and day to day medicine schedule. The medicine names were taken from various public data sets which were then used in Slot Values.  We used Magenta.js to generate music using seeds from various MIDIs. Microsoft Azure's Emotion API was used to detect the emotion of the user. Some experimentation was used to generate music that corresponded to certain moods. Bongocat was brought to life using some CSS and Javascript.  Facial RecognitionWe use an existing library for implementing facial recognition of the user. This is used to verify and use pre saved thresholds unique to each user. For the blink, eyebrow raise and mouth open detection, we use a pre trained model that takes in a video frame and returns a list of 68 (x,y) coordinates of the facial landmarks detected. We then perform computations on this data to get the respective thresholds.Morse Code DecipherThe typing process consists of registering a blink as a dot and an eyebrow raise as a dash. Once the user is satisfied with the current pattern typed, they can open their mouth to enter the string into our decipher function. Thus a sequence of these operations can be used to interpret Morse code and type a complete message. WeChat and EmailThe WeChat message is sent using WeChat API, itchat. An emergency message is sent after a user to logs in by scanning the QR code. The email is sent using Pythons smtplib library which provides backend features for handling the sending and receiving of emails from one account to another.WebsiteThe demo website was made using Materialize which is a CSS library very similar to Googles renowned Material Design. There are multiple interactive elements which have classes such as bc-button or bc-1. These help the web control script identify the list of interactive elements and how to interact with them. The website was hosted using GitHub pages.Web automationSelenium was used for web automation. It is used to choose the next bc- element and scroll to it. A separate function decides the type of element it is and how to interact with it accordingly.     Api is built with Laravel on AWS (EC2, RDS) and the front is made with jquery. Currently viewers are sending missions by sending it as a text as part of the donation they send in. We tried to break down the process and try to design what we felt the process should be. From sending in missions, to streamer being able to accept and other viewers joining in. And then after completion trying to help streamer receive the reward they can get.  With Amadeus's new Flight Inspiration Search alongside Yelp's Businesses and Reviews API on Google Cloud Platform, we learned how to mash APIs together in true hackathon spirit.      There are three core parts of Navicce:Custom map builder. It allows owner of the building to create indoor map with our format in easy and fast way. It needs only map of building, which is usually already done for emergency purposes, and small amount of time to point out walls. (Unn.png)Routing. Our first choice for routing engine was EMBERS Routing API, which we wanted to scale down to one office center or even one building. Unfortunately, we found that Routing API works only with outdoor routing, but indoor routing can be a good way for further development. We implemented our own algorythm, which works on small example data, but we believe that we can integrate EMBERS API for further scaling to bigger buildings.Autopiloted vehicles. In conditions of hackathon, we were unable to create full-scale prototype of our system, but we found some ready solutions, implemented with piloted gyroscooters. It can be easily attached to our system later as we have all needed infrastructure.  We built the platform with Python and used Flask for the server. The photos are obtained using the Instagram API and then processed with Microsoft cognitive services in order to apply the sentiment analysis and retrieve the data. Finally, the playlist is generated using the Spotify API according to parameters such as acousticness, danceability, loudness, tempo, balance, etc.    Reactor is an Office Add-in for Outlook. It is written mainly in JavaScript and uses an ASP.NET MVC site hosted in Azure for the web back-end. Microsoft LUIS is used for message language analysis. UI is written in HTML and uses Office UI Fabric to provide consistent styling with the rest of the Office suite.  We used Ruby on Rails, and use background jobs to continually sync with the ClinicalTrials.gov database. The open-source repository is hosted on Github. We have had non-core team members make several contributions to the code base so far. We look forward to continuing to work with the open-source community on developing the project further.  Entirely in C# and Xamarin  This has been low-hanging fruit for a few months, ever since I made ComportexViz work with remote HTMs, e.g. Comportex on the JVM. ComportexViz just receives instructions for what to draw, so we could totally swap a Python NuPIC server in place of the Clojure Comportex server. You can read my high-level ComportexViz design here.Accomplishments that I'm proud ofIt took 2 days to get the proof-of-concept working, synapses and all. I finished the screencast 6 days after starting this project. I think this helps make my case that this is practical.I'm glad I found a way to make it fast. The way NuPIC works, I wasn't sure I'd find a way. The solution: redesign ComportexViz a little bit, so that toggling synapse-saving is part of using it.Built WithclojurescriptpythonTry it outgithub.com      Submitted to    Numenta HTM ChallengeWinner                Innovation Prize: Muse & SmartThings kit                  Created by  Marcus LewisFanboy of @waitbutwhy  LeARn is built using a panoply of technologies. Unity/C#/Vuforia is used for 3D rendering, physics, and stereoscopic projection, JS/Node.js/Express.js/Socket.io is what keeps our backend running smoothly, and HTML/CSS/Materialize comprises our frontend stack.  We used firebase for the backend to store the searches and the user information. We utilized Foursquare endpoints to find restaurant data and then used the Google vision API to sort the images for food. Later, we integrated iMessage, calling, sharing and the navigation capabilities to make it convenient for the user rather than going to multiple apps to do each thing.  ORB was made using the Unity3D engine along with its C# libraries, and some custom Java frameworks I've made and exported to DLLs with IKVMC. As I'm familiar with Java, most of the statistics controllers and background game management controls were made in Java. However, to deal with input and GUI, I used Unity's built-in libraries. Personally, art is not my forte, hence the minimalistic and colorful design of the game. Still, I decided to go with Cinema4D to mock-up and make the 3D models for the cubes of various sizes.  Using Unity, C#, JavaScript, Oculus Mobile SDK and Photoshop works - Built for Samsung Gear VR  ExplORer was built with MEAN stack (Angular.js, Express.js, Node.js, and Mongo DB), using Socket.io. The application is hosted on AWS with multiple EC2 application servers running Ubuntu OS. We also use HAProxy load balancers.    At first we finalize our full project features and the needed tools and APIs. At the very beginning of the development process we focused on the simplest UX and UI design of Pinage. We used Google Design Sprint method to design the UX and UI of Pinage. During the UX design we design a couple of mockups. After finalizing the the UX we develop the server end of our app. We developed the server end by PHP and the mysql database to provide the service to the client app using Rest API.In Pinage client side is an native android application which is built with the ALOHAR location API. We developed the location aware messaging service in such a way that user can get the tagged message immediately after enter the specific area. We used the ALOHAR location service effectively and thus we successfully minimize the battery usage of device to detect the Geofencing event. We used the some external library such as android volley library for Rest API communication with the server. We also used other third party library in UI design of our app.      The solution is built for android and web and is connected to a dummy backend.  Build as an office add-in for Office 365 using the FedEx Tracking API  I built it with ReactVR technology using C++ and QT    I used the HTML experience I got over winter break learning with codecademy and sheer will to write the code. CSS and I are friends now  We used keras to iterate and improve our deep learning model quickly. To generate the synthetic data we used python. The frontend was built using angular and react.  We built it using the React Native framework with Firebase for authentication and database storage and Stripe for payment processing.    It was built off React, with some backend python and APIs.     This project consists of three subproject:BirdBot - This project contains the actual lambda code that gets deployed to AWS and serves as the actual deployment for this project. The other two projects are dependencies of BirdBot (DataPrep indirectly; BirdBrain directly).BirdBrain - This project contains a module that is embedded in BirdBot (below). The module has libraries for building responses to the lex/alexa questions and the necessary data/query work to build those responsesDataPrep - This project contains code necessary to retrieve all data sources (AAB bird data; Clement's Taxonomy; EBird County Histograms), process them, and write them to their final home (dynamo, s3).    Node.JS, mongoDB, ExpressJS and the Twilio Video APIWinnersWe came home with two grand prizes at the Rutgers University #HackRU Hackathon!!!Best Rutgers University HackBest Twilio API HackThanks again to everyone for showing your support!!!Built Withbluemixexpress.jshtml5ibmibm-watsonjadejavascriptmaterializemongodbnode.jspassporttwiliowebrtcTry it outru-mentoring.herokuapp.comgithub.comdocs.google.comwww.instagram.com      Submitted to    HackRU Fall 2016Winner                Best Rutgers Hack               Winner                Best use of Twilio API                   Created by  Worked on Node.JS backend, Passport.JS authentication, mongoDB, and the Twilio Video API in webRTCAnthony DelgadoI'm a software developer & technologist from NYC. -Focus on DevOps. Created a portable & scalable micro-services architecture with a continuous integration environment to ensure integrity and deployability of project. - Worked on back-end Node.js Server.-Building and Integration of Twilio WebRTC public beta programmable video API.-Design, ArtworkLinkedInJoseph PulaskiFullstack Software Developer & Behavioral Communication ExpertI worked on the twilio video chat implementation. Back-endMatthew GiordanellaAdriana Castro  1) iOS app for convenient working and checking, iOS widget for quick access to cargo's statuses.2) Smart-contracts and blockchain for transparent info about cargos.3) Frontend part - monitor for showing list of cargos and their statuses and for getting info for companies.4) Real-time python server for detecting shaking and temperature during a ride.  Using Swift!  We created 3 contracts: One to handle new studies being posted to the ledgerOne to handle peer reviews of those studiesOne to handle approving peer reviewsFrom here, we built a Web3.js API which integrated with our React interface to created a simple way for users to read studies and write to the ledger.  We use a Solidity smart contract to reflect/relay the state of the bucket (temperature, lighting, soil moisture) and change that state if the environment or user requests it (turning on the fans to lower the temperature, turning off the light, activating water pump). An Arduino with sensors monitors the inside environment of the bucket. This information is relayed to a local Raspberry Pi running a Go-Ethereum node, which posts this information to a smart contract. The same Raspberry Pi runs a NodeJS program that listens to the state of the smart contract. A front-end built with React allows users to easily view & change the state of the bucket from a remote location, using a Metamask infused browser.    We used openCV running on an azure server to identify the lines and separate colors, then move that data onto a python app that generates midi and wav files, that we then play back to the user.  We made a virtual classroom environment in Unity and loaded it up with various physics experiments. The experiments are then completed with the assistance of the gesture tracking from the pmd/gestigon cardboard VR headset. We automatically send completed data from each experiment to a server which stores it using the educational documents API provided by Houghton Mifflin Harcourt (HMH).  We have an Office 365 AddIn to integrate Planner and SPO Tasks. And this uses Graph API for communication.  We have an Outlook VSTO to extend Planner and provide tighter integration with Outlook Tasks & Outlook Calendar.What's next for Apps4.ProBackup / Restore the plans in Planner including the attachments and conversationsCopy / Move / Repeat tasks in PlanLink dependent tasks. Based on dependent tasks, automatic calculation of Due DateBuilt Withdrundaljsgraph-apioffice-365Try it outstore.office.comwww.apps4.prosway.com      Submitted to    Hack ProductivityWinner                First Prize               Winner                Best use of Microsoft Graph              Winner                Best SharePoint App                  Created by  NarasimaPerumal Chandramohan  Using Blender for modelling and Unity for scripting  First Phil prototyped and model the entire assembly in SolidWorks. We gleaned inspiration from other medical devices for their smooth contours and easy to clean surfaces, as well as current cutting-edge internet-of-things devices such as the Nest Thermostat and Apple Watch. We fabricated the model out of 3d-printed PLA and laser cut acrylic.     The first iteration of the application was written in Go. After discovering incompatibility issues with the Azure platform, the application was completely rewritten in javascript using Node.js. The application consists of two main parts the server and client, with each communicating with another through http requests.The server is where all of the heavy data manipulation is conducted. The server receives a query from the client via an HTTP GET request. The server then takes that query and using Twitters REST search api finds 1000 tweets that match that query. Then using regular expressions, the server strips and removes all the referenced URLs (in order to make the semantic analysis easier). After the data has been prepared, we perform an analysis of the tweets using semantic analysis tools hosted on Azure. The analysis tool then returns a score based the words used, and from that we generate a confidence statistic.The second part of the application consists of the client. As mentioned beforehand, the client sends a query to the server, and the server in turn returns a summary of the analysis it performed. The client then generates a stock recommendation based on a determining algorithm, displays a rough summary of that data using a pie chart, and gives a confidence statistic. The client was written in the standard HTML5/CSS/JavaScript while reference several other open source libraries for the visuals and the communications.       The app is built with Swift in Xcode. We pulled park data from the Yelp API, and integrated with our own custom backend which stores all the data. This allows the interface between the app and the backend to be clean and maintainable.      First, we acquired data from the Hy-Vee website by scraping the website using python. The python saves the results of a post request and then manipulates the result into json format so that the lambda function can call the pricing or sales information. When using the echo dot, Alexa calls the lambda function every time that a user invokes the skill. We created multiple intents within the skill so that a user can say a variety of things and still get the correct results.   PruPay app is built using the native Android SDKs for better security. The backend is hosted on Google Cloud and leverages Firebase tool set for user authentication, encrypted data storage, notifications and analytics.A Node.js API gateway is used to authenticate with Visa API network and abstracting the complexity so that the mobile app is light and better performant. It is hosted on Heroku.Google Apps Script was used to build a gmail plugin to scan for invoices.  Its built visual studio, C#Accomplishments that I'm proud ofDesigned a custom activity which is capable of exploring a entire package and returns the whats need for the user to look into.What I learnedLearned more about custom activity and designing a custom activityWhat's next for Validate Folder Structure and Compare ContentMore planned updates in futureBuilt Withc#Try it outgithub.com    Created by  Banuprasaad Boopathi    The team divided the work into a front end using react and drizzle, and a contract-only backend building off of bonding-curve logic by bancor and oed and an inline-assembly proxy factory by gnsps. We started with a collaborative whiteboarding session, then worked with a combination of pair programming, automated testing with truffle, and manual testing off of Kovan.  Using a bootstrap theme on Brackets we were able to edit the html/css and javascript in order to create the kind of website we wanted. Since we were a group of 3, we also used GitHub to collaborate.  Utilizing a locally designed and manufactured smart wristband with BLE and NFC capability, our team crafted the minimal data required for a secured and low-bandwidth mode of communication. We used react-native to harness the power of native Bluetooth API and robust app performance. For the backend, we used Google Sheets API as a flexible and familiar database and Google Drive for CDN. MongoDB is used to push real-time location of tour groups. The hardware components are low cost and durable and the software stack is scalable.   We have used Unity as our main development platform and setup and integrated several SDKs such as Mapbox, Android SDK, Google AR Core, and Java development kit. Mapbox helped us get a general map of our location and then we built a virtual representation of walls and rooms as blocks in Unity. We had to make the walls invisible virtually but still manage to hide objects behind them so that the user can only see the object representing the destination when there are no walls blocking in between him/her and the destination.    Sunrise is an IOS app built in Xcode using Swift.  Multiple different IOS controllers and pages with segues connect to each other to create an inviting interface.  Using different controllers, texts, calls, and outside links can be accessed.    Using Face Recognition we aim to authorize signed up users to log in via their face. We selected the Flask micro web server written in Python and some machine learning models and libraries. We used one-shot learning to train the model. It requires only one image per person to train the model. So, when a user signs up, they enter the information along with an image from the webcam. After that, whenever the user wants to log in, we will scan their face and match the features without a model. The one with the closest set of features, if available, will be prompted and we will ask for their password. If the credentials are right, the user can log in into the system. This is a groundbreaking technology and in the coming time, can act as a replacement for passwords too for the laptops.      The service is a web app communicating with a remote web api. The frontend was built using Angular.js, running off a Node.js server during testing, and then being converted to static HTML, CSS, and JS files to be served and run client side.The backend server was written using the Phoenix framework in Elixir. The backend was responsible for receiving requests from the client, communicating with the database, and computing a rolling average of parking status.The website and the backend are served and running from an AWS instance, with Nginx used to forward requests correctly.  We used Python in conjunction with Google Maps Platform to fetch JSON data on local attractions. The web interface we created with HTML+CSS and served with Flask. The server is hosted on a Raspberry Pi.  WWWorld was built with rapid prototyping using Sketch App and Adobe Illustrator, and was then further implemented on the Front-End using HTML, CSS, and JavaScript.The Front-End is supported by the Twilio Sync Javascript SDK for syncing live video and live reactions. Authentication is handled by a node backend issuing JWT tokens.  Using Azure functions for specific serverless logic like:Polling Security AlertsReceiving Notifications from Graph APIManaging Graph API notification subscriptionsProcessing and transforming dataUsing Logic Apps:Receiving Alerts from Azure FunctionsPushing updates of Alerts back into AzureEnriching alerts with data retrieved from other above-mentioned API'sPosting a ticket to a third party cloud-based ticket systemOther than Functions and Logic Apps we are using CosmosDB for detecting duplicate security alerts. We are also using an Azure Data Lake for storing information so we can use AI to further analyze the security alerts.    Runs on any browser using WebVR, WebXR or Prismatic. collaboration, community networked A-frame, ExoKit, Three.js, Unity  We built it in multiple modules, separating the data receiver (robot communications), hardware control (robot outputs), and data transmission (shoe). After each was developed and tested individually, we began the long - and somewhat tedious - process of integration.  After many excruciating hours of brutal work I took a break and wrote this program in three minutes.  I used ReactJS for the frontend, NodeJS on the backend, along with libraries such as React Router, Socket.io, MomentJS  There were 2 big parts.1) Video analysis. We managed to process top-100 IMDB movies (not just metadata, but videostream!) using modern deep learning technics like face detection, emotion classifier and other. 2) Android app. Our team created amazing application from the UX perspective. It is beautiful, handy and very interesting to use. It's like a game, but with end-use benefits.  Sleepless nights, great team of freinds who are working together for a long amount of time.We have on-chain and off-chain part of solution. More about it you can find in our repo.  It's actually surprisingly complex.React Frontend for the end users, implementing the controller that looks for the average loudness of a sample being over a certain threshold.Backend Websocket server to handle all the users claps and deliver state changes to the actual game.A game which responds to what was provided by the websockets. Taking a ratio of active clapping devices vs total connections to scale it. (For dinorun, 50% or more have to clap in the time frame of 300ms)Each user contributes equally to whenever the movement happens, so it requires many people to work together and collaborate.Adding a new game is as simple as copying about 20 lines of code into it's init function.  The app is written in Swift and built with Xcode. It exists almost exclusively as an iMessage extension. The mini-game uses SpriteKit, a 2D game engine, to render sprites, simulate physics, and detect collisions.  We first began with 3D printed chassis for the VuMark targets. These targets are identified and parsed by the program and cross checked against our cloud database on Vuforia. We then created 3D, textured, models in Blender that will hover over the VuMark targets.We then wrote the code in Unity that will calculate voltage and current values using concepts from vector calculus and matrix algebra.    We use IBM Bluemix so that Bob could talk more natural and allow flexibility in the conversation. We also allow add-on so that the reminder is more that just an alarm, but more like a real person helping you with your productivity.  We first got together and devised a plan to use Python, Flask, and React as our stack for this project. We created a database cluster in MongoDB. We simulated transactions being created by making JSON files in the database. We developed the front-end by using react.js. The API calls were hosted by a web sever through Azure.     We installed an open source operating system (OpenWRT) onto a new router, and built a server to run on it. This server sends new connections to our React web app, wifico.in, which authenticates the device and awards 100 Wificoin for each new device. The router's server also tracks devices to monitor their bandwidth usage and prove data delivery. When someone uses your router's bandwidth, you get awarded proportional Wificoin, which appreciate when more people share their bandwidth.  It is built on the concept of Robotic Enterprise Framework (ReFramework). It can be easily integrated with non-ReFramework projects as well.BOT is configured to read the emails by subject by adding the search criteria as a filter in Get Outlook Mail Messages activity. It reads the subject (search value) from Data\Config.xlsx.Successfully tested the execution for different customers. It works very well without any change in code. It can be run standalone or can be integrated with any project.What's Next?Publish it under the centralized umbrella called as BOT Factory or Market Place  which can be easily downloaded by the RPA professionals having access to it.How to use Read Email By Subject Plug & Play workflow?Filename: Read Email.xaml(A) Pre-requisite:  Download the complete project repository(B) Update Config.xlsx:  Optional: Update the value (e.g. ABCD@Company.com) against the Mail Account in Data\Config.xlsx. This field is useful if there are more than one email account configured in Outlook. If it is left blank, BOT will monitor primary mailbox by default.  Optional: Update the value (e.g. Inbox or Inbox\UiPath) against the Mail Folder in Data\Config.xlsx. If it is left blank, emails will be read from the Inbox folder by default.  Enter the search criteria (text) for email subject. BOT is configured to read the email by searching the keyword in subject of the email.  (C) Invoke: Integration with existing project  Use Invoke Workflow activity in your project sequence/workflow  Click import arguments 2.1. Optional: Pass Config (Data Dictionary) variable to the input argument in_Config, if you are using ReFramework(D) Run: Standalone execution  To Run standalone: Open Read Email.xaml -> hit run Project repository details: /Framework/InitAllSettings.xaml  Read Email.xaml /Data/Config.xlsx project.jsonBuilt Withemail-automationoutlookreframeworkroboticenterpriseframeworkroboticprocessautomationrpauipathTry it outgithub.com      Submitted to    Power Up Automation    Created by  Imran LoonRobotic Process Automation Professional  It is built on the concept of Robotic Enterprise Framework (ReFramework). It can be easily integrated with non-ReFramework projects as well.Configured the BOT to read one or more email attachment(s) as an array of string to store the attachment name with folder path. BOT attach one or attachment file(s) to the email using Invoke Method activity. Send Outlook Mail Message is used to address the Email Subject, Email Body and Recipients details.Successfully tested the execution for different customers. It works very well without any change in code. It can be run standalone or can be integrated with any project.What's Next?Publish it under the centralized umbrella called as BOT Factory or Market Place which can be easily downloaded by the RPA professionals having access to it.How to use Send Email Multiple Attachment Plug & Play workflow?Filename: Send Email.xaml(A) Pre-requisite:  Download the complete project repository(B) Update Config.xlsx:  To recipients email address - stores recipient email address to be marked in 'To' section of the send email activity  Cc recipients email address - stores recipient email address to be marked in 'Cc' section for success/exception scenario(C) Invoke: Integration with existing project  Use Invoke Workflow activity in your project sequence/workflow  Click import arguments 2.1. Optional: Pass Config (Data Dictionary) variable to the input argument in_Config, if you are using ReFramework 2.2. Optional: Pass runtime message (string data type) variable to the input argument in_strMessage, value will be updated in the placeholder mentioned in email template 2.3. Pass Email Template Filename (string data type) variable to the input argument in_strEmailTemplateFilename 2.4. Pass attachment filenames (Array of String) variable to the input argument in_arrAttachments, you can pass more than one filename separated by comma as a delimeter(D) Run: Standalone execution  To Run standalone: Open Send Email.xaml -> hit run Project repository details: /Framework/InitAllSettings.xaml Send Email.xaml Main.xaml project.json /Data/Config.xlsx /Data/Input/Input File.xlsx /Data/Logs/Audit Trail.txt /Data/Output/Result.xlsx /Data/Template/Exception Email.docx10 . /Data/Template/Success Email.docxBuilt WithreframeworkroboticenterpriseframeworkroboticprocessautomationrpauipathTry it outgithub.com      Submitted to    Power Up AutomationWinner                Snippet - 3rd Place                  Created by  Imran LoonRobotic Process Automation Professional  We build all our models in PyTorch.Font retrieval model: we used the pre-trained model provided by [FIXME: add link] to retrieve fonts in the Google Fonts dataset.Raster font generation:1) Variational Autoencoder: we implemented our own autoencoder to learn the latent space of different characters and fonts.2) StarGAN: we modified the code of [FIXME: add link] to manipulate different attributes of a character, such as bold-ness and serif-ness.Vector font generation: we tried to reproduce the model proposed by A Learned Representation for Scalable Vector Graphics, implementing it from scratch. We first train a variational autoencoder on raster images and use gated recurrent units and a mixture density network to generate SVG commands and obtain vector font files.    We think that creating a company or into legitimate interaction is too complicated! And we want to simplify it.  The mobile app is build using react-native an open source library by Facebook for making native mobile app. The back end is supported by google's cloud platform  firebase with use of the cloud functions to tailor notifications.   Android, Kin SDKAWS + PythonEnthusiasm to build for my gaming community. Accomplishments that I'm proud ofBeing a game designing company, we feel proud to build a tool for gamers that would keep them stay curious and connect them with other gamers peer.   What I learned*Gamers love rewards, lootboxes, barter and surprises. :) *Mobile gaming industry has 500M+ daily gamers.What's next for LootBox - El DoradoWe want to be showcased as the platform that drive users and engagements to the games and we want to be go-to-app where gamers come together to socialize.Built Withamazon-web-servicesandroidfacebook-graphkinphotoshoppythonTry it outplay.google.com      Submitted to    Kin Crypto ChallengeWinner                3rd Place                  Created by  Acted as a Product Manager and managed product design and developers team. Viral Replay#entrepreneur #two_exits #inventor #five_patents #hardcore_gamer.    Front-end: JavaScript, Bootstrap, Ace Editor, JQueryBackend: NodeJS, ExpressCI/CD: Google Cloud BuildProduction Server: Docker containers running in a Kubernetes cluster in Google Cloud  Using Qlik Sense Desktop Edition and connecting and generating sample data and using statistical data that are existing on the internet.It also connects live data from a smart metering solution to a DB2 database from where it takes data.  Once a decision has been made regarding what the skill will do and what it won't do, building and testing were easier.  I always start with a list of the must-have features to produce an MVP (Minimal Viable Product).  The first version was built, submitted to certification and passed.  What really made me very excited is the number of unique users and skill activations (67 in a couple of days) without advertising it anywhere.       The skill utilizes a Video Player, an Audio Player, and APL documents.  Some of the media is hosted in S3 while some are loading directly from public repositories on the internet.   THE FISH6 total that sit inside the holes of the wooden platform.Each fish was made out of 2 mixing bowls. One slightly bigger than the other, so that the top half bowl can swivel over the bottom bowl when the fish opens and closes its mouth. The top bowl is connected to the bottom with a wooden dowel and twisty ties. The top fin is made out of hard foam we found at AxMan. The hard foam fin was screwed in to the top half bowl and the googly eyes were glued on using E6000. We gave each fish a unique look with a sharpie marker and googly eyes. Inside the bottom bowl of the fish, we sanded the surface down and then used E6000 to attach two ceramic magnets, followed by an RFID card, and topped with another two ceramic magnets. The only piece inside the fish that is glued down is the first magnet since we wanted people to be able to easily remove the fish from the rod once caught.THE SMART FISHING RODThe tip of the rod contains an RFID sensor with two magnets in the middle (taped together) and that is wired all the way up the line and down the rod that will connect to the teensy inside the box located near the base of the rod. This wire is secured with zip ties along the way.The tip of the rod has the top half of a 3 inch diameter plastic Easter egg to hide the RFID sensor and was meant resemble the cartoonish design of the tiny rods from the original game. A positive side effect of this is that it makes the game a little more challenging because you have to angle the rod and have it timed just right in order to catch the fish.The fishing line is made out of colorful paracord that has its ends singed off with fire.Each rod is created using a wooden dowel attached to a foam sword handle.Towards the base of the rod will be a container holding all the circuitry and components of the smart fishing rod. We will be putting all the components together inside this container, the day of the hackathon. Each container will have the following: an RFID sensor board, an OLED screen, a 2 ohm speaker, an MPU (gyroscope/accelerometer), a teensy with prop shield, and a 5V battery pack.THE PLATFORMA circular wooden platform that is approximately 4 ft in diameter, that fits inside the kid swimming pool.There are 6 holes in the wooden platform for sitting the fish inside. The rim of the bottom bowl for each fish sits perfectly inside each hole.There are four wheels mounted to the bottom and located towards the outside of the platform, one in each quadrant of the circle if you were to divide it into four equal parts.The motor will drive a spinning wheel under the edge of the platform that will hopefully rotate the platform smoothly.There is a hinge installed going down the middle of the wooden platform to allow for folding up of the circular wooden platform into two halves. It is divided in such a way that 3 wheels are under one half of the platform and a single wheel is under the other half of the platform. This is to allow for easier transportation.THE MOTORA hole will be made on the side of the kid swimming pool to affix the spinning driver wheel on the inside and attach it to the motor on the outside. We will test out the motor for the first time, find the right speed to play the game at, and make a control station for the motor all on the the day of the hackathon. THE INCLINING BASEFour incline wood pieces attached to a wooden pegboard with nuts and bolts.Fits under the center of the pool, had to be elevated by 2 foam sheets underneath. This helps give the fish the extra height so their mouths will fall open wider, making them easier to catch when they are on the tallest part of the incline.We will need to affix the center of the inclining base to the center of the pool the day of the hackathon to keep the inclining base from sliding around and getting caught underneath the wheels.   I used APL and Node.js and REST calls to my server running in the AWS cloud as an EC2 instance with a SQL database.         To automate the Parrot AR Drone 2.0 Elite, an Arduino Uno connected to multiple ultrasonic distance sensors was connected to the drone. Each propeller blade was sputter-coated with aluminum metal (~50nm thick) to reflect UV light. A UV light source was mounted under each propeller blade at an ideal distance from the blade to best reflect the UV light below the drone (the physics is explained on our website). Finally, the drone itself was programmed to autonomously roam a room via feedback from its distance sensors mounted in each direction, at an altitude permitting UV inactivation of viruses and pathogens.     FreeMe consist out of three main components:Bathroom feminine hygiene dispenser:Our prototype runs off android app software and beacon disguised in the sanitary pad.When pressed, it snaps a photo through Android device and creates a web profile. Photo is being analyzed through the use of facial recognition api (Face++).  The picture with metadata is being stored in Parse database. Donkey platform is being used for push notifications to notify the police department. Police Android app:Android app that uses GIMBAL apis to detect RSSI signal from QUALCOMM made beacons. Whenever the app detects those specific beacons it notifies a police officer by providing them a dialog with the picture and metadata of suspected human traffic victim. Web controlled Administrator page for Police department:Gives administration power to the police to browse, sort, delete or edit all submitted profiles as well as send push notifications through Donkey to the police officers. Created in Javascript, HTML, CSS using Parse and Donky APIs, MapQuest AOEL                  Built with Gamemaker, designed in Photoshop, animated with Aseprite and covered in tears, sweat and determination.  The game was developed mainly in C# using the Unity 3D engine. Most of the effort went into the custom gesture recognition and visual effects. The gestures were coded based on the raw input data from Unity's API. Whilst Unity goes a long way to help with regards to rendering, we put quite a bit of time into the custom shaders: the burnup dissolve effect of the asteroids and the convolution pixel shader ripples. Unity's own physics and mesh colliders meant that the game object interactions were mostly fairly simple to implement, meaning we had time for some polish in the competition schedule.  We designed the leg in a 2 day marathon, then began the sprint to print the leg which took about 3 days of printing. Then once at the hackathon, we assembled the leg to the custom brackets and supports we have designed. The actuators handle a 12V rail for charge and can support about 1500 pounds of lift. (The frame cannot however.) With a Arduino Mega and a Seeed Motor Shield we reverse the polarities of the current digitally to control forward and backward motion. Then slaved to a Pi, we have software written in Erlang and Python that match the bio-mechanically correct calculations of leg motion matched to the person it was measured for. Once everything was wired up, custom communication protocols had to be made as well as software that allows generation of walking keyframes. Kinda cool we think.  This chat application uses Moxtra's chat client and Microsoft's Cognitive APIs to help you get the upper hand in your chat conversation. It makes use of Micrsoft's Text Analytics and Bing Search APIs to automatically detect and present the user with snippets of information relevant to the topic of conversation. The Moxtra client provides the basis of the chat and we build on its collaborative capabilities with our features. For every incoming message, we use the Micrsoft's Sentiment Analysis API to give the user a feel of overall conversation so far. We also use the Keyword Extraction and the Topic Detection APIs from the Cognitive API suite, to extract keywords from the incoming messages and detect their categories (context) to be able to use Bing search to give the user an access to the latest news and web articles relevant to the topic. We also use Wikipedia's API to display text snippet's on the most significant topic in the conversation. This application also uses Micrsoft's Language Detection and Translate APIs to help the user translate text at his finger tips to a language of his choice.    The core of share a route is java and spring, the services works thanks to mongodb and google geo service.  We built our application using Flask as the backend, Bootstrap as the frontend, and IBM Watson for speech-to-text and emotion analysis. Built Withbluemixbootstrapflaskhighchartsibm-watsonTry it outgithub.com      Submitted to    HackRice 6Winner                1st Place                  Created by  Jacqui LeeTang ZiluTianyi ZhangKevin Li    First, we gathered data from ura ring API, which is then analyzed and stored on Azure web platform. Each user have assigned ring and when they come close to specific devices, such as smart coffee mug, smart ligthing or raspberry pi with speakers these devices react according to the sleep quality and activity of the person, by either congratulating, suggesting caution or strongly suggesting to slow down and sleep. Coffee mug receives images from android application, which gathers data from Azure web application. Smart ligthing and sound are controlled by raspberry pi, which also gets the data from Azure web application and identifies the specific user through bluetooth signal and its strength.    We started by creating data structures which contained properties for each element type. By using these data structures, we then iterate through each element using a loop and print each one programmatically to the canvas. On the other side of the editor, we created an algorithm that would take a user generated input, strip down all filler words, and get the basic meaning of the line of code. We can then use the output of this algorithm to set assign different properties to the data structure. This project totaled to around 1500 lines of code.  We built the script using python and telepot.  We used ruby on rails and the google maps API to connect people within a neighborhood so that only neighbors have access to posts relating each particular neighborhood complex.   We decided to utilize existent technologies and minimize coding effort as much as possible. So, we chose Salesforce Einstein and Community platforms. First provides with AI algorithms that we trained to recognize images. And second is used for building the UI for shoppers. We also didn't forget about shop's needs. Salesforce platform is a CRM system that provides with detailed analytics about sales and customers.   Our team built a low cost prototype of an FES stimulator using Arduino, an accelerometer, and a TENS nerve stimulator. We used this hardware to stimulate the Tibialis Anterior. Accomplishments that we're proud ofWe're excited to have built a functional prototype in just 24 hours! It both senses gait pattern and stimulates the muscle to cause a contraction at the right time.What's next for foot++We hope to test our prototypes, then scale up to produce these for low-resource settings.CreditsEmbedded Systems Mentor: John NaultyHardware: William Wnkowicz and Marion LBHardware Hack Inspiration: Midori Sanchez, Miguel Moscoso, Kevin Flores, Jhedmar Callupe, Elizabeth Albarracin, & Toms VegaBuilt Witharduinohardwarenerve-stimulatorTry it outgoo.gldrive.google.comdrive.google.com      Submitted to    health++Winner                Grand Prize: 2nd Place              Winner                Grand Prize: Finalist                  Created by  Peggy (Yuchun) WangPeggy is a currently a junior at Stanford studying Computer Science in the AI Track. She's interested in AV, Robotics, and AI.Swaril MathurPierluigi MantovaniRenu KondraguntaPierre KarashchukJessicaSelinger              I built RokkSalt using the Laravel PHP framework and using a MySQL database to store information. I used the Infusionsoft API and the oAuth authentication method to allow users to connect to their Infusionsoft Application. I then built in email support through Sendgrid to notify users when joining as well as when broadcast emails need to be sent. I then integrated the order form and subscription billing through Stripe and their API to handle failed/successful payments that would automatically handle approving or denying access to the service.     First, we took the best ideas from each of the custom network resource management solutions we had built and layered them into a single platform. Then we added configuration options to make the system flexible across the types of healthcare programs were seeing today and supportive of the inevitable evolution well see in Alternative Payment Models in the coming years.  Finally, we conducted focus groups with industry leaders across the country to validate our ideas and assumptions. We incorporated what we learned and released version 1.   Data Ingestion is done using collectors which read SMART data attributes from clustersSMART Attributes are read at frequency of 2 hours and sent to common databaseAdapters feed data from this common datastore to Model (good and bad) to get anomaly scoreAnomaly score is used to classify dataFinally the classification is analyzed against actual class  Server: NuPIC, Python iPhone: SwiftResources: CLA(HTM) whitepaper, Jeff Hawkins's videos, Matt Taylor's NuPIC tutorials, Dr. Subbutai Ahmad's lectures, NuPIC community videosWhat's next for Fitness FortunaCombine multiple data source from wearablesMake effective use of anomaly detection inferenceNatural language user interface to humanize the HTM algorithmBuilt Withapple-watchnupicpythonswiftTry it outgithub.com      Submitted to    Numenta HTM Challenge    Created by  Gopal IyerR&D Engineer in the Automotive IndustryVikas IyeriOS Engineer  Ha came up with the idea. He also worked as the product manager and helped in designing the product vision together with Keven who is our UI Designer. Kevin and Omar created the interfaces for the screens. Omar also helped Vikram and Yuta with the program logic and the coding work. Vikram led the dev team and put everything together in the end.    Dark magic (Twilio + Azure + Wolfram Tech + Node + MongoDB + caffeine)        We used Firebase to store user data and for instant notifications. We use Pebble.js for the Pebble app. We use iOS and MapKit for the iOS application. We used Azure and the Photon board for a project to monitor weather data and relate it to emergency frequency.  First we analysed the available data and discussed the different data models with UPM and Bluemix experts. Then we created paper prototypes to design the flow necessary to visualize the data.Finally we design all necessary 3D models (earth, truck, ship, train) and made them interactive. In parallel we have integrated the Bluemix APIs to include original UPM data directly in our solution.Technology used was: Microsoft Hololens, Cross Platform Development Tools, SketchUp, Unitiy 3D, Git, Visual Studio, C#, ...    The web application relies primarily on the Google Cloud Speech-to-Text API to asynchronously transcribe audio to text while also storing the timestamp of each word. This data is stored in a Python Dictionary with the words acting as keys and a list of timestamps of the words occurrence as the value.. To construct the relevant suggestions that the program offers, the Google Cloud Natural Language Processing API is used to identify words from the audio with high levels of salience. Promoting words with higher salience over sheer frequency, we prevent the inclusion of filler or common words.APIs UsedGoogle Cloud SpeechGoogle Cloud StorageGoogle Cloud DatastoreGoogle Cloud Natural LanguageGoogle Cloud  App EngineThe front-end is two styled HTML pages, one where the user can upload a file and one where the user can see the results. When the user uploads a file, we store it in the Google Cloud Storage Datastore using an API call. A python endpoint on our Flask server (described below) then processes the audio file and returns a list of keywords and a dictionary mapping each word to a list of its start times in the audio. Next, when the user tries to search for a specific word in the audio, the program will retrieve a list containing the start times of that word in the audio and allow the user to toggle between each one, very similar to the command-F functionality.  The backend part of this Dapp is built with Ethereum smart contract. The source code is hosted at GitHub and is automatically published to Github Pages by continuous delivery with Circle-CI triggered by commits in master branch.Important libraries we used:web3js: communication with Ethereum smart contract methods and fetching eventngx-vcard: vcard generationqrcode: VIC QR codes visualizationjspdf, html2canvas: frontend-based PDF generation  Using Python, I'm calling the NASA free API, to get the image and its explanation.      Android Studio was our main platform for development. We used Clarifai's Image Recognition API to determine a list of possible objects that the image might represent. We then created an algorithm to help classify which bin the item should be disposed in depending on the objects identified in the picture. We created a responsive and intuitive user interface, to allow the user to conveniently navigate within the application.  The skill Backend runs on a php server. The script uses the Alexa UserID (hashed) to create an entry in a MySQL database to store the WiFi's SSID and Password. There are different kind of scripts running on the server processing those data:Skill (plain JSON parsing and creating via PHP - just minimal code, no framework, yet)QR Code Generating (using the PHP QR Code lib) used by Skill for displaying WiFi Credentials on Display / Card or the (deep)link to the webpage.Website (just some HTML,CSS and PHP code) for browser use (also supporting universal links to deep-link into the iOS app if installed).WebServices (just some PHP code) to communicate with the AppThe iOS App is written in Swift an only uses default Frameworks of apple. For the Skill this app is extended by an upper top-left skill button in the navigation bar. This invokes an menu with the options:Enabling the skill: If Alexa app is installed, a deep link is called. Otherwise a link to the amazon skill store is invoked).Connecting via QR: Start scanner for Connection Code. On the camera image, there is a speech bubble overlay, which shows the phrase how to get the connection code from an echo show.Connection via PIN: Speech bubble, which shows the phrase how to get the PIN from an echo and a form to enter itDelete connection (if there is an active one).If an Alexa UserID is linked by QR Code or PIN, the credential detail view presents a switch. If the switch gets activated, the credentials are transferred to the WebService. On the Overview View, the credential wich is stored on the skill is marked by a grey Alexa Skill icon.    framework: Symfony 4 and MySQLFront END:  material-ui , react, react-redux, react-router-dom, momenthttps://developers.google.com/chart/Condorcetm - ranked voting (https://github.com/julien-boudry/Condorcetm) ML with TensorFlow    Myself and Olanrewaju Ahmed and Korode Yusuf used React as frontend, MongoDB as a no sql database and GraphQL for querying and endpoint. It was my first time using those stacks but kudos to Olanrewaju Ahmed who gingered us. We do little data mining to get sample data related to what kind of information our end users need to see such as list of tutorial centers, work-spaces and instructors.    The core of the app powerfully uses the here.com api to power the map as well as all the routes and bus stations. Then, using the web-geolocation api, we added the user's live location to the map gotten from here.com. Google cloud was used for data storage and vue.js was used to implement material design to make the UI look good and simple the use. Many web services then complemented the app to power the backend to connect the trackers with those waiting for the bus station.   The extension itself is build in ReactJS. The API communication goes to a AWS API Gateway with different Lambda-Function, depending on the API call, behind it. The Lambda-Function are calling the game database and adding the content coming from the extension. The auth is a API-Key which is stored within a .env file.   The framework was built using Scala. However, existing genomics DNA analysis programs can be used within the framework without modifications. These programs are implemented in both Java as well as C.        We used Unity to design the space and add scripts, insert custom music, and an HTC Vive to dive in and enter the rhythm.      The digital clock is rendered using an HTML5 canvas element. We used the JavaScript time API (i.e. the Date class) to keep time. The calendar is written in HTML, CSS, and JavaScript, with webfonts via the Google Fonts API.  Bleeding edge facial recognition algorithm paired with down to earth chrome extension  This app is construct using java, google maps API and Rightmesh API.  I built both the android applications using Java. A Parse backend deployed on Heroku and local SQLite databases support both the applications. Wellness Diary makes use of Microsoft's Cognitive Service APIs to perform sentiment analysis. The therapists and messages are obtained from the Parse backend. Articles are also fed to the Parse backend and can be obtained by web scraping.Wellness Therapist uses the Parse backend to retrieve messages and also enable therapists to send messages to their patients.   The core of MedicSMS is based on the Twilio and IBM Watson APIs. We use Twilio to receive and send SMS messages to our end users, while we use Watson's natural language classifier to classify the symptoms of an individual and then recommend the best course of action. Our two different backend services are written in NodeJS and Python, while our frontend is using AngularJS, Google Maps API and CSS3. Our services are hosted on Heroku and Google App Engine.Built Withangular.jscss3google-app-enginegoogle-mapsherokuibm-watsonjavascriptnode.jspythontwilioTry it outwww.medicsms.orggithub.comgithub.com      Submitted to    TechCrunch Disrupt LondonWinner                Twilio              Winner                Hack Disrupt 2nd Place                  Created by  I came up with the idea and worked on processing the symptom data, training IBM Watson to diagnose the most likely condition and then feeding this into a decision tree that was then sent back to the user. I also implemented the translation functionality, and organised the database of user information.Mike CurtisGeorge StefanisHarley KatzSam HeslopDhaval Patel  I've been working at it for about four 5 months now from scratch learning swift and the SpriteKit library. Unfortunately, since I started with zero IOS development knowledge, much of the code is retrospectively bad, but it gets the job done! With stack overflow and sleepless nights I slowly pieced the code together to this point. The other part was of course the graphics which I drew in Autodesk Sketchbook Pro. Art was an equally daunting challenge, but it helped me develop some cool skills and in my opinion fairly good 2D art.  Compan-Oekaki- Colaborative Vision Board was built with Node.JS, Express, HTML5, and CSS3, and AWS Elastic Beanstalk. The collaboration digitally space was built with amazon web servers, share js, and node js.       We used Unity and C# as a game engine, and connected the PC app to an iOS app that we made. We made use of an algorithm we found online which allows us to measure the user's heart rate in real-time using a technique involving the torch and camera. The heart rate is sent to a MySQL database, and the data is then sent to a web page using our own PHP API. Changes in heart rate are automatically detected by Unity, where a script keeps a constant connection between the interactive experience and the database.  It is built using the JavaScript library three.js. Other than that, it is a regular web tool.  By spending the weekend on virtual parties, figuring out HTC Vive, Sleeping in the cloakroom tent, eating ryechips and promising everyone else this is the game of the century to keep a fire under our asses.  Using Python and Unreal Engine, drone photos are programmatically combined and re-rendered into a single 3D world model. Photos are fed into AI powered image recognition software and labelled accordingly onto the model. Using Unreal Engine, the model is used as the surroundings for a virtual reality touring experience  Alexa Site uses:Amazon Alexa Skills KitAmazon Lambda FunctionsInternal Squarespace APIRead Full Case Study: On MediumBuilt by Chris Grant,Twitter: @chrisgcoWebsite: chrisgrant.coBuilt Withamazon-alexaamazon-lambdaamazon-web-servicesnode.jssquarespace      Submitted to    TechCrunch Disrupt NY 2016Winner                Disrupt NYC 2016 Overall Winner!                  Created by  I built the Alexa Intents, Amazon Lambda Functions, and the interface with the Squarespace API.Chris GrantFreelance Web Developer, Glass Explorer, and CTO of Landme.org  We used Unity2017.3 along with the SteamVR and VRTK plugins to kickstart our VR development efforts. We used two vive trackers along with custom built straps to securely fasten the trackers to the feet. We then needed to come up with a music beatmap encoding system to store the timing and position of each step. Finally, we used Unity's UI system to create the visuals and built our own parsers to read in our song data and present the visual steps to our players.  Unity Engine.    WITH BIENE  I used HTML, CSS, and Javascript. HTML and CSS displayed the pages while the Javascript created the bitmap and QR code.      HTML5, CSS3, Google Maps API, JS, JSON  Josh and Dalton built the mobile app in React Native. While Josh is proficient in React's paradigm, neither of the two were very familiar with React Native. React Native is a framework for building performant, native mobile applications by leveraging existing web technologies in a React paradigm focusing on declarative, functional user interfaces.Eli focused on the web application, which was built in Angular, a powerful JavaScript framework for building fast, scalable web apps. Eli is very comfortable with Angular, but took the opportunity to review ReactiveX and integrate it in every way possible.The team initially outlined features necessary to make a minimum viable product before moving on to later bonus features. This allowed the team to stay mostly in sync throughout development, while still leaving time to polish portions of the user experience.          The Adobe extension is a Node application at runs inside of Photoshop CC et al. The JIRA extension is a Ruby on Rails application that uses the Atlassian Connect framework. We were able to build on our previous experience developing add-ons.  Server-side: nginx as a webserver, uWSGI to host flask based python applicationsBack-end functions: wikipedia, lyrics, translator, imgur, wunderlist, twitter, email, sunsetcalculator (expected)Front-end: Android side-swipe multitoolkit (Swiss army knife) appBuilt Withandroid-studiojavapythonTry it outgithub.com      Submitted to    jacobsHack! Fall 2015Winner                jacobsHack! Grand Winner              Winner                Microsoft Azure Hack                  Created by  Jean-Paul BreuerLaura de KeizerRashiqAndroid Development @Microsoft. CS @TUBerlin. Hackathons.Nick LeePayments Backend @ Uber Amsterdam. CS from Jacobs University Bremen. Hacker.     Bhargavi whipped out the HTML Activity log page with intresting activitiesLori was the power point and image wizard the power pointSadio rocked the HTML Account Stats and Leader Board PagesSandy developed the Android App and  Landing Page Sarah master minded web server CSS and developed  the HTML food activities pageMichelle cranked on the Rewards page  I used the Microsoft Graph API to give the user access to their SharePoint document libraries. I access Sites, drives, and DriveItems. The file uploading is done via the Resumable Upload API for DriveItems.I built SharePoint File Uploader UI using Apple's UIKit. The code is written in Swift. The user's OAuth tokens are stored securely in the device's Keychain.  Our focus was to make ethereum accessible via simple set of RESTful APIs for healthcare.  Our audience for these APIs are EMRs and other healthcare applications.  After building the initial APIs, we built a simple application to step through the 3 user agents of the api. patient - wizard to create a smart share contract launched from within a patient portal source provider - create a transmission contract to share information securely base on governance of patient share contractdestination provider - retrieve, decrypt and acknowledge receipt of information sent by source provider  The website is built using JQuery, Jade and HTML/CSS. The backend is a NodeJS server that uses Import.IO to scrape the web for the information used to compare jobs, and Mongolabs to store data. IBM Watson Analytics is used to provide detailed analysis at the end, and to help the user find a solution.  Built with coffee & love <3 What I learnedLeveraging the Twitter API & Spotify API - never used either before! And being realistic about what can be built in 24 hours... with trips to the planetarium, card games, and hikes in the mountains!  What's next for CU Party PlayEnable folks to downvote! Enable DM when more than one result is returned for a track. Also allowing to search by album, and much more! Built Withjavascriptnode.jstwitterTry it outkimcodesin.space      Submitted to    HackCU Episode IV    Created by  javascript ninjaKim NoelTechnology is much more than a passion for me. I have a natural need to create, imagine, and build. API helpAbdulhakim AjetunmobiCode and stuff I guess.Harshpal Bhirth          Simulation System:In the beginning, we try to prove the efficiency of Tunnel Comet. We built a simulation system in unity which has two-lane traffic in the tunnel. Both lane is simulated with AI cars running on it, while one lane is normal road and the other lane is Tunnel Comet road. Tunnel Comet road is setting up sensors to detect car pass through and light patterns to serve as visualized guides. By this design, user can carry out the comparison between normal road and Tunnel Comet road. Furthermore, the simulation system has two mode  God mode and player mode. In God mode, user can observe whole tunnel in god view and see the objective traffic statistics of each lane. In player mode, user can experience the driving in first person view and even VR(virtual reality) view if user has VR glasses. These two mode also allow us to make sure the strategy of Tunnel Comet is appropriate and the driving experience is comfortable.Prototype:After proving the efficiency of Tunnel Comet, we built a prototype of Tunnel Comet road. It contains four ultrasonic ranging modules and three LED lights to form a segment of road. We used an Arduino board to connect all components and implement same strategy algorithm as simulation system on it.:Unity:LEDArduino        ,     room- . ,       ,           .   ,           .      / ,        .    The measurement is compromised by a specifically developed cap to suit every PET plastic bottle. In its inner canal an adapted mechanical turbine is located to provide data about speed of flow of liquid. The signal is being processed by a special System on a Chip that is able to track the speed of rotation of the turbine and thus the liquid and provide the output data via Bluetooth to a smartphone with a wells application. The whole system is powered by a battery that is wirelessly charged via Qi standards, that enables to store data in case of application and the cap being disconnected. The measurement is thus reliable, energy efficient and comfortable for the user.The wells website is available here and supporting code for the app can be found in the links below.What's next for wellsOur mission with wells is to improve your hydration via a simple, smart experience. We definitely want to pursue the project beyond the hackathon!Built Withadobe-illustratorarduinoblecsshtml5javajavascriptphotoshopTry it outwells.skgitlab.doc.ic.ac.ukwww.dropbox.com      Submitted to    Hack Cambridge    Created by  Android development.Daniel ZvaraWorked on business development and pitch.Elizabeth DlhaHardware! Martin FeriancI worked mainly in the website and app front-end development. Also assisted with hardware building.Filip Stollar    Google SketchUpGoogle SketchUp used to make the platform 3D design. We have made 3D model of Smart Floating Farm. We simplify our model for Intel hackathon.Smart Floating Farm 3D DesignWe make dashboard with HTML5, CSS3, and Javascript. To add style we use Bootstrap and make interactive chart use Highcharts. We make it eye-catching for everyone that using the dashboard. FritzingFritzing used for the design of electronics hardware to support us in sensors configuration with Arduino101. We used internal and external libraries to make this visualization.Smart Floating Farm Sensors and Outputs Configuration-Fritzing designArduino IDEWe use Arduino IDE to built the firmware. We use libraries from internal Arduino IDE and from internet. For external library, we used  SEEED RGB LCD , Adafruit DHT11 , Adafruit RTC , Adafruit ADS1115 , MQ02 Gas Sensor, Dallas DS18B20 , Rotation from Gyro & Accelerometer, Time Alarm , and Time Lib . For internal library, we used Wire, pitches.h, CurieIMU and Software Serial. We used Mario Bros Tune developed by Dipto Pratyaksa to play sound at the beginning. IMU rotation angle we got from Erikyo. SFF Arduino sketch can be found at MANTIS GITHUB.Eagle LayoutWe build our own shield to make easier in pin connection. We used Eagle Software to build the schematic and board..Smart Floating Farm Custom Shield  It was a team effort both at the idea and implementation stages.  Using 0x protocol v2 and Set protocol's newly published setprotocol.js library.  With a lot of passion and dedication :) And also with some Symfony 3 and Vue JS.  The backend was written in Node.JS. We used Webcam.js to snap pictures every few seconds. We used socket.io to allow for remote activation of the surveillance system. We used Microsoft's Computer Vision API and Azure functions to detect the intruder in the house and developed/trained a convolutional neural network with thousands of images to determine the danger level of the intruder and whether he is carrying a Knife or a Gun.The database we used for this application for login and user registration is Dynamo DB.We used the Twilio Messaging service to send SMS messages to the police and the user. We used the Google Maps Geolocation API to detect the location of the phone, which is sent to the police.We developed the front-end of the application using Angular JS.    Built using NodeJS and Javascript leveraging Azure Functions to interface with Azure AD, Microsoft Graph, Azure Table Service and Lithnet Password Protection for Active Directory.All secrets are stored in Azure Key Vault. The WebApp is Application Insights enabled.The WebApp is deployed using a Docker Container into Azure App Service.  I made use of the Cisco Spark API as well as other APIs, such as API.AI...The fun part was to find APIs that would complement each other in a way that made Diana look smarter!  We used a ruby on rails backend with numerous integrations to build a more wholesome application.  by calling the google map api we collect the information, and by the back-end algorithm we sort the rent, rating, and competition in that specific business. And report the final perfect address to the customers.   We used the Vuforia AR API in Unity and C# scripting, and tested the app by sideloading it onto our Android phones. The physical components were made using Microsoft Powerpoint and Vuforia vision targets.    Using zero-knowledge proofs and zokrates circuit compiler for the zkSnarks.    The backend is in two parts, both written in Python:Speech Recognition:Continuously running Python script using Azure to obtain input from mic -> textUsing regex parses questions to extract optionsSends parsed question to our REST API via POST request REST API:Flask-basedRuns a Twitch Chatbot to read chat and tally votesPoll endpoint for extension to get dataThe frontend was designed with Figma, built with Parcel and Sass.    The app takes advantage of Watson's Dialog and Classifier service, and I wrote my own python script in order to determine what food or restaurant the user was looking for using a complex mix of statistics. Once Fetch knows what you are looking for it uses the Yelp API to get a list of restaurants in the area. It then finds a corresponding menu using Foursquare and Locu, and after checkout the order goes to Postmates to have it delivered to your door.  The system uses four main parts: An Alexa skill, an AWS Lambda function that handles the skill, a webserver, and a Chrome extension.The Alexa skill simply allows the user to dictate commands, and the Lambda function processes these. The function then sends the command to the webserver (in a HTTP POST request). This webserver runs a Socket.io server, and when it receives a command from the Lambda function, it emits an event to the Chrome extension containing the command. The extension receives this event, processes the command, then runs it on the current browser tab.  We used iOS app and iPad camera for motion detection and capturing the pictures of garbage. We trained visual recognition on IBM Watson to categorize garbage items. We used Arduino and three Servo for opening trash cans. Amazon Alexa helps us build our restock list with the grocery items we run out of.  We built the google chrome extension in javascript using gmail.js, an open source framework. The backend was written in python using NLTK and hosted on Azure.     We took a decentralized approach, adopting Market Protocol in decoupling the interest rate swap positions into margin-embedded Long and Short position tokens. We used Airswap to find the price of the Long ad Short position tokens. Given the importance of liquidity in the model, we programmed a bot to arbitrage away the spread across the Market Makers (Minters) of the token pairs and the Hedgers/Speculators.Lastly, in order to create virtuous loop to further incentivize the Market Makers, we implemented rDai as the collateral in Market Protocol in Minting of the position pair. Thus, Market Makers will be able to earn interest from the rDai collateral, in addition to earning the spread.For this hackathon, we are able to start from scratch and launch our LSDai on Ethereum Main Net. Since we are the first Market Maker in this product, we have decided to donate the interest accrued from rDai in the collateral to EthBerlin.   We built the chrome extension using Javascript with Microsoft Cognitive Services (Face API).  BLHD  First, we created a tool that would allow us to generate large quantities of training data for our model. This involved creating a classification system for the change in an index for a given day, and a Twitter scraper that downloads the President's tweets. Next, we paired each of our tweets with the classification from the day that the tweet was published, rebalanced our training data, and uploaded it to Google AutoML for training. Once the training is done, our React.js frontend and Node.js backend allow the user to use the model to predict changes and visualize the result.  Dashboards for hospitals to check the status of patients in their area were constructed using Ruby on Rails, HTML, CSS, Javascript. Your pacifier 's smartphone application was built using Xcode and swift. We used arduino and sensor to develop your pacifier prototype. The 3D model of your pacifier was designed using fusion 360. I designed a mechanism that extrudes air by pinching and pushes chemical into the baby's mouth.  We used the PCA (Principal Component Analysis) to build the model by hand, and broke down the video into one second frames. Previous research has evaluated the mouths of the deepfake videos, and noticed that these are altered, as are the general facial movements of a person. For example, previous algorithms have looked at mannerisms of politicians and detected when these mannerisms differed in a deepfake video. However, this is computationally a very costly process, and current methods are only utilized in a research settings.Each frame is then analyzed with six different concavity values that train the model. Testing data is then used to try the model, which was trained in a Linear Kernel (Support Vector Machine). The Chrome extension is done in JS and HTML, and the other algorithms are in Python.The testing data set is comprised from the user's current browser video, and the training set is composed of Google Images of the celebrity.    Our team reverse-engineered the remote protocol using a software-defined radio (SDR) and audio waveform tools. We used a 434 MHz transmitter connected to a Raspberry Pi to send the signal to the outlets.The Raspberry Pi was registered as a device on our Bluemix IoT Platform instance, meaning that the Bluemix Node Red flow could feed into the flow running locally on the Pi. Using the Amazon's developer tools, we created custom skills for Amazon Echo. After a vocal cues, the Amazon Echo was configured to send to an HTTP endpoint opened on Bluemix. Bluemix filtered and parsed the vocal intents and routed them appropriately to the Pi.  Language: PythonTools:PySimpleGUI for the GUIPyInstaller for making the executable    Researched about the DeFi platform. Read their blogpost, learnt what these projects do. Learnt how developers wants to access these platform. Built APIs around these.  Conrad and Justin scraped more than 3k good memes from RedditWe then embedded them using Language models (BERT) and CNNs (InceptionV3). We then ran PCA on the combined embeddings and used Agglomerative Clustering to generate 5 planets of memesBen and Alex built a beautiful UI for the product              There is one neural network for the treble clef (right hand) and one for the bass clef (left hand). This is because the higher right hand notes often play a unique melody, while the lower left hand notes are often repeated, slower chords. I wanted to make sure these two very separate styles of music didn't confuse the neural network.                       Conceptualis en environnement Azure, utilisant un Web API et un Web Job, le tout en C#. Speedlunch a t ralis complment pour distribution grande chelle!!! Nous sommes pratiquement "ready to market" et pourrons installer le bot partout, en utilisant le Slack Button avec l'authentification OAuth.  Step 1: Start with an ideaStep 2: Keep hacking away at itStep 3: Repeat step 2 until it works  Built it using Apple Watch and Pepper Robot  For hardware, we used a Particle Photon, Neopixels, backlit LEDs and LCD screen, a range sensor and a photodiode. The Photon controlled basically everything. The sensors are there to preserve battery life.The iOS app was built using Swift. The Photon had an iOS SDK for interfacing purposes.        We used Node, Angular, and of course the Alexa API.  The Kinect is processed with the supplied api. We process various face information and print it on the stream and send it out to our flask server. We used python for post processing, analysis and integrating the server. Node and React were used on the mobile phone side to pull Google maps data and draw event markers on a map. The front end is a combination React, jQuery and Google material design.    For the summarization itself, we implemented Jolo Balbin's thesis on summarizing a single topic text block. It involved finding the most common ideas, and ranking the sentences based on how similar they were to other sentences. This was coded in Python, and lives inside an AWS Lambda function. Most group chats have many conversations going on though, so we also built a classifier function using IBM Alchemy's concept extractor and clustered similar words together using NLTK. We then ran the summarization script on each of the specific conversation topics and got out key points.The Slack Bot was hosted using a Python flask image running on Microsoft Azure, that's where all the API calls and data lived. This was called on by the Lambda function on AWS. We then used our Lambda function and Azure backend to link functionality to the Echo.  Current plan is to do a raspberry pi build with a cheap microphone - perhaps a simple peizo element will suffice since we're mostly just after SPL and some rough frequency information.    