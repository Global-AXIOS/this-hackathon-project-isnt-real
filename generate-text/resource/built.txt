  We use C++ and some input files for text for the quiz. Based on Spark AR  Java IDE from Eclipse   We use C++ and some input files for text for the quiz. Based on Spark AR  Java IDE from Eclipse  We used the godotengine.org with the https://godotengine.org/asset-library/asset/215 for the majority of development work. Version control was performed using Github. Planning and note taking were done collaboratively over google docs. The audio was cut and reformatted using Audacity. As far as division of labor was concerned, Ethan handled nearly all of the important code elements, as well as the creation of particle effects. Matt was in charge of multiple miscellaneous tasks, including the creation of the debugging UI, location of the sound effects, and the composition of the write up.    Using python and the Atom text editor.  Website with calculations done in python  On account creation, the users can select from a list of interests they have. They also answer a few questions to determine where they lie on the political spectrum. The questions are binary, thus the user either moves farther "left" or "right" with each answer. This is done via Google Forms, which in turn spits out the data to a Google Sheet. A Python script harvests the data and creates user documents on our MongoDB.A (text) chat session is created for two users that have overlapping interests but lie on opposing sides of the spectrum. The users are free to speak with each other for however long they desire.After a chat session is complete, a Python script interfacing with the GCP APIs sends the chat log off for processing. Google returns information regarding the sentiment of the log, which we condense to a number that denotes how successful the interaction was and how much "change" to reward the two users.The crypto-currency was built from a small implementation known as "NaiveCoin." It runs on an Ubuntu VM from GCP.    The project uses an Arduino Uno and a Raspberry Pi. They will both be housed in a box, attached to a tree. The Arduino will be connected to a microphone, and will use its built-in LED, and the Raspberry Pi will be connected to a GPS module, a Wifi module, and a light sensor on a breadboard. When the microphone on the Arduino senses a chainsaw, it will activate the LED, which will trigger the light sensor on the Raspberry Pi. When the light sensor is triggered, it will use the Wifi Module to send an email to the Forest Service, which will include the coordinates picked up by the GPS. We believe this is an appropriate and effective use of technology, given that all the components are relatively inexpensive, small, and easy to procure, meaning it will not be difficult to widely implement it.  We used html, css, and javascript XCode    We used the Arduino IDE to develop our game, creating multiple files within the folder that all communicate and share data with each other. None of us had ever created using Arduino so we went to the workshop last night and learned everything we used from there and from what we could find in open source materials. Aside from the programming aspect, much of the hardware (like the color sensor, or the LED ring) we had never used before, so we had to learn how to solder the pieces together, and connect them all to the board.  Accomplishments that we're proud ofOriginally we had all planned to not present our project at the end of the Hackathon, but after how much we had accomplished and how proud we were considering we learned everything in one night, we decided we stay and show others what we had made. Built Witharduinoc      Submitted to    HackHolyoke 2019    Created by  I worked mostly with configuring the neoPixel  light sequences and wiringCaroline KnoteI worked with designing the light sensor and the success/fail logic of the game. It was my first time using Arduino, which was tough to learn, but really rewarding to see my work come to life. Rose SheehanSophie Vincent Using Unity as the main engine, we utilized Google's ARCore SDK for Android devices in augmenting the virtual objects onto the physical space. For detecting and analyzing the user's speech, we used IBM Watson's Speech-to-Text, Tone Analyzer, and Text-To-Speech Unity SDKs. For quick prototyping, we used Procreate, Photoshop, Premier.  We used the flex/bend sensors and attached them on each finger of a glove. Then, we connected the flex/bend sensors to a texas instrument MSP430F5529 launchpad for display. We used the opencv library to be able to map the whole face in terms of coordinates. We extracted eye coordinates that gave 6 specific points on the plane. Using the Euclidian geometry, we found out if the eye lid is closing with putting limitations in relation to the timespan it occurs. Not relying just on the coordinates and to be more precise and efficient we trained a model using Microsoft ML, which analyses if our given frame is a frame with closed or open eye. The training set was images of people with eyes open and closed while the classifier predicts if the driver has his eyes on the road or not. We got an open-source dataset for Skin cancer MNIST dataset from Kaggle, which classifies skin lesion images into either cancerous of skin-disease category. We trained a Resnet 50 classifier on 10,000 images (without data augmentation) and 40,000 images (with data augmentation - flip, mirror, grayscale images, etc.) and classified it into 10 different skin disease classes. We used the pre-trained weights of ResNet50 from the ImageNet dataset and did transfer learning by learning the weights of the final layer. The best accuracy that we received on the validation set was 78.2% while training the model for 50 epochs. We plan to improve on this model’s accuracy by taking the following steps:   We used Java to run an applet. We created a recording class to save the recording data as a hashmap of timestamps and speeds/locations. We saved each musical note as a discreet audio file and played them back in the recording class.    To build this, we spent time planning the application's tech stack, class structure, and data flow on paper and whiteboard. This pre-planning stage helped us immensely, as we were able to divide up into two pairs, where one was more back-end focused while the other was front-end focused. We used React Native as our primary technology for developing the user interface, along with Firebase as our back-end database.   We used the Alexa Developer Console to create Mombot, and we connected it to an Alexa so that it can be used in the real world. We setup an interface to store user's phone numbers and keywords in a database. Then we use a software defined radio to listen to local emergency frequencies and send communications through AWS transcription. We compare the result against our database and if we find a matching keyword, we send alerts via twilio to phone numbers subscribed to said keyword. We made use of the programming language Java and the development environment Jetbrains using IntelliJ IDEA We built an app using python libraries which enables us to read the text via a webcam, and the image to text converter, which can be found in the library, can output the image as text instantly!  We used Unity for programming; GIMP and Photoshop for digital art. We used real instruments for the background music and sound effects. All the voice-overs were done by us, using voice recorders. We also used SoGou Pinyin application to type in Chinese as well as EasyHindiTyping to type in Hindi. Code   We built this prototype through the developer platform Actions on Google for Google Assistant. With hours of research into common problems students face, we put at least 5 example phrases as Machine Learning training data for the agent for each intent we have covered. The intents include SchoolPorblem, FamilyProblem, JobProbelm, RelationshipProblem, ImposterSyndrome, LifeSaver, Encouragement, Appreciation, etc. We also provided at least 5 example responses the agent can reply to each intent. During our testing stage, we are able to do live demo through any phones with Google Assistant installed and developer accounted linked.  We use Python to create the project with following steps:create_empty_board () creates an empty board, and returns the board;draw_board (board) prints the board on the screen;valid_move (board, col_no, row_no) judges whether the move is valid;player_move (board, current_player) determines which player is going to move and asks for the grid that he wants to move.next_player (current_player)enables players to move alternatively;full (board)checks if the board is full;win (board)sets the winning rule and determines if a winner exists;play_five () calls all funtions to play the game.ChallengesWe first encountered difficulties in drawing a square board. We adjusted the space between characters for many times and solved it.Then we had the biggest challenge - setting the winning rules for the game. We need to consider rows, columns and two diagonal directions, and to check if there are 5 consecutive, same-colored chess pieces in one of these places. After multiple attempts, we finally decided to use for loops to accomplish the goal.Accomplishments that we're proud ofWe are proud of ourselves because we never give up reaching the goal!We are proud of ourselves finally solving and conquering all challenges!We are proud of ourselves successfully designing the complete program structure!And of course, we are proud that the program can successfully run!What we learnedHow to apply and combine the knowledge of Python that we have learned into a  complicated program;The ability to innovate on the things we have already learned;The spirit of teamwork: cooperation is crucial to acheive goals;The improtance of being persevered and never giving up!What's next for Five-In-A-Row Board GameWe hope that after further exploration in programming, we can add images to our game and make a web page for it.Built Withpython      Submitted to    HackHolyoke 2019    Created by  Yuchen ZhaoHaojia LiJinghan Xu Sasha made the Photoshop rapid prototype of the website. Jessica wrote some java code.  WIP  Technical team and experts and using technology (web&mobile apps) to be connected with the targeted segments and get their feedback directly. ff test We built the mobile app using flutter framework and dart programming language for building cross platform application that works on both Android and ios. using IoT device connected to mobile applecationBuilt Witharduinoibm-watsonjavaTry it outgithub.com      Submitted to    green-hackathon-finalist    Created by  I Worked on ux/ui design .Eman  Mohamed EzzatI work as a Business person. It is the first time to apply what I learnt.  Yossra NoorMahmoud SalamaMohab Ahmedkhaledelbialy Elbialy  Build digital Agri community  by using dart           The application is powered via the Blockstack decentralized computing network and built using React.js. Thanks to Blockstack, user entries and data are stored in Gaia, Blockstack's storage. This provides a lot of security due to the multitude of authentications required.   c Our first step is to calculate the average daily carbon footprint per capita. We scraped the numbers of annual water, electricity and fuel usage from the school website, multiply them by the carbon emission factor. There are approximately 2,335 students and 312 staff and faculties at Mount Holyoke last year, so we get the daily average carbon footprint per capita: 36012122.87/365/2647 = 37.27 ton/per day/ per personWe also want to take school’s recycling programs into consideration. The following chart is the annual resources recycled. We calculate the offset to be 4 tons/per capita/ per day.                    We built it on Visual Studio Software using the language C++.       I don't build it yet due I try to contact so many developers and no one want to join forces.       Capture knowledge from all available sources (documentation, emails, chats etc...) Transform knowledge data into unified format. Deliver and get access thru multiple channels           There are a number of components to this skill:The skill code itself:I built this skill using the Jovo framework. It is developed in Node.js using typescript, and data for each user is being persisted to the skill in DynamoDb. The skill supports the APL interface and has full screen APL support across for all devices, throughout the whole game.The Leaderboards:The leaderboard data can be accessed within the skill, from the website, or from the email service. Leaderboard data is updated whenever a user finishes a puzzle, and is stored in and AWS RDS MySql instance.The Website:The website allows users to view their rankings on a 7 day / Last Month / All time basis. You can look up your user and see your scores for each puzzle you completed. This out of skill component was designed and built to help with user retention. It's built as a static site hosted and Amazon S3, written with javascript in VueJS. Data is accessed via a backend node API hosted in AWS Lambda and API Gateway using the serverless framework. The Email Service:To send emails to users, there is a recurring job each day.  Once a day is completed and we have everyone's scores, we can publish that days results. Using an AWS lambda running on a cron schedule daily, we load up all the scores from the database for the previous day for users that have requested emails. For those users, we calculate their individual stats for percentile and place in time and points and deliver it as an email using Twilio's SendGrid. This system was an integral part of the design as I really wanted to focus on user retention to drive adoption of the ISP.The ISP: For the In Skill Purchase component of the skill, I decided to go with subscriptions. Since I was focused on retention and daily usage, I figured that a recurring model would be the best approach for this skill. I've decided to brand the ISP as the puzzle league, and by subscribing users get extra benefits like more games, and the ability to reveal extra letters. The free component to the game is substantial, but by paying a small monthly fee, those dedicated users can get some extra enjoyment and fun.  Platform uses Canon's CCAPI to automatically fetch media from the digital camera. The content is then stored using Filestack and edited using the Filestack's API. With React, very slowly.  Backup+ is built on a standard HTML/CSS/Javascript stack and powered by Filestack's API. On top of this, we use APITracker to monitor the API  The game incorporates a number of features including: APL, ISP and Reminders. APL is integrated into the voice-first experience to show at a glance progress and status. ISP integrates two product types, consumables and entitlements. Upsells are effectively integrated into key moments of gameplay, such as lack of lifelines or requirement of a hint. Reminders allow users to have a notification at a time of their choice to continue their mission and should help with repeat engagement. Built using node.js and an AWS backend, the game is unique in that every question provided is custom audio, with background noise, voice actor and special audio effects. I created level-based scripts to keep the quantity and variety of questions somewhat unique and increase in complexity as the player gets past milestones and virtual levels. It is built in mysql, html and php scripting language.         The app was built with a Redux, React Native, AWS, Filestack Workflows, JavaScript, and the Canon Camera Control API.   We built a web page using React that has a form for gathering ingredients and a place to display the recipes when they come back.  We used Spoonacular's api to access their database of recipes and apitracker to track the use of our api.  amazon web services, Node.JS and React.JSBuilt Withamazon-web-servicesnode.jsreactTry it outgithub.com      Submitted to    DeveloperWeek Austin 2019 Hackathon    Created by  Amir Yunas   Have only built some wires, need help building Beta and guage complexity.  [Sam] Ux/UI design, Frontend Backend, Deploy[Che] Frontend Backend architecture, DeployWe used HTML/CSS, Javascript (used OpenCV API for face recognition), ejs, Node.js, and Heroku.  The app was built with a Rails server, React Native, and the Canon Camera Control API.  Unlike Java, it is not a build (compilation) language, so it is not built, but through syntax checking (Pylint). I built this game primarily through the usage of collections objects. I used various sets and maps to construct objects that would hold the possible word families.    By creating 64 frames with APL and changing the background color of the frame, I realized Othello game in Alexa.Putting othello's original rules into the skill was complicated such as a mechanism that the background color of the sandwiched frame is automatically changed.Built Withjavascript      Submitted to    Alexa Skills Challenge: In-Skill Purchasing    Created by  stpete ishii     By Guidance and motivation It is made with lambda and node JS.     Conducting user research by interviewing users to create personas and user goals to create a user friendly product where I utilize low to high fidelity wireframes to produce a prototype for usability testing to check for errors. I have built many electric bikes and motorcycles. MOBI, like every project starts with an idea, a sketch, a CAD model and a set of drawings for construction.  The MOBI frame has been lazer-cut from sheet chromoly, then folded and welded. Suspension A-arms were mandrel bent, and jigs created to hold the job whilst welding. Other components were machined on a lathe, or CNC machined. Battery pack was built using 18650 cells and spot-welded together using copper strip. The project was funded through investment, and has taken a year to get from idea to high-end prototype.  I put together btcd and LND documentation and laid out the steps from start to finish.   Android studio and Java for back-end From a real life use case based on Car Sales. After an initial call to get and assess a few, quite generic and common business drivers and needs I got a sample of a few records extracted from real customer production DB (it was MapR data lake but was the same if it was an OpenEdge, an Oracle, a MSSQL or any other one). Transformed these records into JSON file we build a step-by-step process that show how to bring real data on the mobile device of an end user. I build the app at the same time I was explaining how to do this with the power of Progress.   We built the front end dashboard with react and the backend with flask. We used PyTorch with fastai to make and deploy the deep learning models. We used Java to make android applications. we used Google cloud and netlify to deploy our backend and frontend respectively.NOTE: the training happens on the CPU for the demo, if we switch to a GPU it would take way less time to train but due to the cost of GPU'S we couldn't show it.We used 3scale from red hat to secure the backend. Initially, we focussed upon making the most important parts of the project that is using java and bash scripts to generate the code from the object dimensions and python scripts to deal with training our model. Hence, in this way, we completed the first part of making an automated front-end designer. Secondly, we made our Php project my second team member worked upon that. Firstly, I designed all the screens to have the best customer experience. Worked upon the backend so that for the time being we can demonstrate the working of our model. Implemented Redhat developer studio to develop our fuse integration project.   Using Jovo Framework + Ruby/Rails API app + MySql        Used Google's AppEngine as a foundation, we used python for the backend and HTML and CSS for the frontend. Advanced methods in React Framework (React.JS, Javascript, JSX)MapBox API (used by Facebook and Snapchat) QR Code API, PG&E SDK APIPhotoshop for our icons, logo, etcGitHub use for collaborating in developmentFirebase Database: Back End to retrieve and store dataStrengthened our communication, time management, and teamwork skills.   The client will take a short survey of their needs. Those results are used access a database of service providers.  We built our web app using the AWS cloud9 server and Google AppEngine. We used Python, HTML, CSS, and JavaScript to make our program. We also implemented Google Maps API and a dataset from the city of Salinas's website in order to make our map page with a heat map overlay that shows where crime rates are higher in the City of Salinas.  Our Application uses Twilio API as the primary way of sending text alerts to users.All of this is done in Node.js and Express.  Ideation - narrowing down our brainstorming to just Homelessness. Combined idea with Gamification. User Personas - Who is our target? Design - How do we make this experience usable? Easy to access for these users? Prototype & reiteration - Via Adobe XD, through an iterative process, we wire framed and prototyped the solution.    We buit it using python and the acuweather api    We used Java and many online resources and built it from the ground up.  We built it using Java programming language and tried our best to structure it. We built the application using Python and Tensorflow. We built it using three layers: front end, back end, and the database.  By using Javascript as the middleman between the XML file and HTML website, we were able to create a thriving website of database-to-user interactions. Twilio API (SMS), Google Maps (GCP), Flask, and NgrokFloodGate's creation required building and utilizing functional RESTful APIs and packages to create a responsive site. The site is styled via Python's Flask library. The backend server is supported by Ngrok, providing localhost access to webhooks. The SMS ping system is backed entirely by Twilio's Programmable SMS API, providing FloodGate users and first-responders with RESTful, real-time notifications. We also use Firebase's Cloud Firestore database service to host our sensor and SMS data (with SQLite3 as an offline backup) and GCP's Google Maps Platform to display location-based data on interactive maps.Arduino, C, Python, Matplotlib and a Whole Lot of SolderThe FloodGate sensor is built using an Arduino 101 and Ultrasonic Distance Module HR-S04 using the established Arduino IDE and C. Serial data is handling via Python's PySerial library.  I used Java for the first responder software, Maven to use the MongoDB, Jackson and JSON repositories, and Google Cloud Platform and MongoDB to handle the movement of the scraped images and AI scanned data in the cloud.I used Xcode and Swift to make the phone application, which uses Google Firebase for a login system (logging in isn't required but helpful), and has a Feed, Assistance, Meetings and Settings pages. I used CocoaPods to use Firebase in the application.   Our website is built as a prototype in Sketch.  Used ARKit to trace paths.Used MapKit to see SOS signals and send SOS signal. Our Google Chrome extension is built using Javascript, CSS, and HTML as the overall framework. We first created our chrome extension template and the extension window that gave information on PenniesForGood and directed users to a graph of their charitable progress in the 3 tracks we created (education, health, hunger). Then we created a buy window that would be enabled when the “add to cart” button in Amazon is clicked. This button will link to another popup that will prompt the user to donate, telling them how much we round up the price too. If the user says says, they are directed to a list of 3 charities based off of the product they bought (e.g if a user buys instant ramen, they are directed to charities looking to address hunger). This price is then updated appropriately in the Amazon checkout. Our data is also accumulated within a progress bar page that showcases a progress bar to our nearest goal that updates every time someone clicks yes! This was built using android studio, firebase, and MapBox.  Swift, Google APIs, Firebase, Apple APIs The demo web application uses the Accuweather API to gather historical and forecasted weather data in the user’s location.  Each datapoint is configured in our algorithm with adjustable weights, which are tuned to aggregate a relative safety score from 0-10.  The backend is implemented with a Python Flask server on an Amazon EC2 box.GetOffTheRoad uses the following technologies in the sample implementation:Twilio Communications APIAccuweather APIApache HTTP ServerPython / Flask Web FrameworkMongoDBAmazon Web Services (Ubuntu EC2 Server)  We built the blockchain using java IDEs Intellij and Netbeans, and we enlisted the help of a few coding mentors.   We built our website on Amazon's web services (cloud 9). We used coding languages such as Python, HTML, and CSS. We also involved APIs from Google and Twilio.  web app via HTML/CSS/Javascript, our own API, and Amazon Alexa For the frontend, we built a web app using React. On the backend, we have a few different technologies in use. For storing data and hosting our demo, we used Google Cloud, MongoDB, and UiPath Automotion because of its ease of use. For capturing the user's health data including their heart rate, food intake, and water intake, we wrote a backend in Python using Flask to pull the data and store it in the cloud. We also built a separate Node.js backend to capture information about the user's range of motion in their limbs, which we prototyped capturing from either a Bosch sensor tag or Android device.Future ImplementationsWe would like to use the Twilio API to incorporate video calling features. Built Withcssflaskgoogle-cloudhtmljavascriptjsonmongodbnode.jspythonreactuipath      Submitted to    HackPSU Fall 2019    Created by  Abhinav MamidipakaAnnika Iyengar  Our user interface relies on google diagflow to build the chatbot. Our database relies on google firebase to host our data. Our backend web app runs on flask. The machine learning clustering algorithm which determines best coordinates for relief camps utilizes sklearn. Our weather prediction utilizes accuweather's api. The dashboard for the web app is coded in react.js.  Used python to scrape and convert data  Use weather API to get data and use PyQt5 to build user interface(UI).  We used XCode/Swift to build the front-end of our app and interact with the AI Chat Bot using Google Cloud's Dialogflow.   The stack relies heavily on WebSockets. When a user raises an "event," which in this case is making a drawing stroke on a canvas, or moving a card between categories on our board, the event is sent between clients using WebSockets. We then pass the event to the persistence layer (Java API), which relays the completed event to every client currently connected.The frontend is built with Bootstrap, we have the server running on AWS. We decided to build this by dividing into different product development roles and teams. Vishal created a custom API that the iOS application could query data from using Python, Flask, and HTML. Rahul combined databases of information while cleansing and parsing through thousands of datapoints of expiration dates using Pandas in Python. Harish developed the front-end iOS application using SwiftUI which calling the API and displays the information for the user. Arjun worked in a variety of roles, from data scraping to flask development and front-end design within the team. James focused on using web scraping techniques through Python and BeautifulSoup to harvest and add data to our database.   Justin built a heroku flask API to serve live NBA data & Dayana built the front end for WEb3-React. Casmir also wrote GO-Cosmos-IBC for RapNameServices & "NBAChain". We built two classes: one composing of the GUI, and the other composing of a combination of the Accuweather API and the Twilio API.For the project as a whole, we used Java. However, for the GUI, we used a combination of Java and JavaFX.More specifically, we took a user input of a town/city from a text field in the GUI. Then, we sent that town/city input to our API class. In that class, we manipulated the weather data corresponding to that location to calculate and display the current weather conditions, driving safety, and outdoor comfort of that place. We also used our API class to give the user an option to have all of that data (location, weather conditions, driving safety, and outdoor comfort) compiled and sent to a phone via text message. By implementing a cleaner design and reconfiguring the way that users will navigate through the website, we built a more intuitive and user-friendly product. We used google sites as our platform and embedded HTML code to add features, including: the scrolling news section, adding social media platforms to the site, and font/formatting manipulation. We implemented google forms as a means of submitting applications.  Method - Message AnnotationCleaned the messages for removing numbers and special characters.Tokenized the messages and the stop words are removed.Further cleaning to split the hashtags into a complete sentence.Processing to get full sense of known abbreviations.Labelled data trained with a GridSearchCV model.Any new emergency request will now be annotated with appropriate tags.Method - Disaster PredictionLimiting the disaster to Wildfires for the scope of HackPSU.Labelled Images are trained on a CNN.Binary Cross Entropy Loss and RMSProp.Polling satellite data periodically to make predictions based on the learned features.Responding to the prediction by broadcasting safety alerts to all the citizens in the vulnerable areas.LAMP StackHosted on a GCP VM instance running UbuntuApache for web serverMySQL database for all the citizen, appeal data and annotations.PHP as a backend with integration of machine learning with Python We created the scene in Unity3D. Using ARFundation toolkit, the app will detect features such as planes, windows or a designated image and spawn virtual objects in our desire. With game logics and animations, the scene is interactive and flexible for added features.  We built a website using HTML, Node.js, and CSS that takes in a user’s input. We also wrote a python script using pandas to reformat all of the data files into a standardized format. We also created an R script that takes the user input and the data and visualizes it in the form of histograms, scatter plots, line graphs, etc. Mobile app wireframe using Balsamiq3Website using Vue.js, BootstrapWhat I learnedConsiderations designed around the interests of the homeless and donors is crucial to attracting the target demographic to use our set of web and mobile toolsWhat's next for HandUpRegulations: restrictions on OTC medications, similar to welfare guidelines (as per SNAP, WIC) Get corporate participation and matching Built Withbalsamiqcss3html5vue.jsTry it outgithub.com      Submitted to    Girls in Tech Hacking for Humanity 2019    Created by  logo design, also contributed to project concept, mockup website contentDeemSumAllDayI was the product manager and I also worked on the website front-end development. Stephanie GloverI worked on creating the mobile app wireframe. Calista WongVijaypriya  Ganesankrypto100 registered the domain name on domain.com I used Unity Engine to develop the simulation and a surfeit of terrain mapping software to create the simulation.  Python Django and developer access to Smartcar API. We worked as a team of 5, helping one another during times of struggle. We worked on FrontEnd using React Native, a cross-platform development environment and Backend using google-cloud and firebase. We used a lot of GCP products and some of which include: Firebase Authentication, Google Maps, and Google Cloud.  We used Java and GameMakerStudio to interact with SmartCar API.  Retrieved students' information from Google Classroom API. Processed data to get ready to be uploaded to Salesforce.Build local Webservice to automate first two stepsAttempted to design an Apex function in Salesforce to send requests to the WebserviceAttempted to designed report format in Salesforce for demonstration  As stated earlier, we built our application with the usage of React and Node. We utilized Node to pull the data that we were interested in from Google Classroom, and we used React (and Reactstrap) to create a friendly user interface.  Using Python, we built a script to access the API and generate an HTML page with interactive node information.Main libraries used were requests, folium, and ip2geotools  We approached our project with the goal of constructing a functional website that could be accessed readily and immediately. We utilized Google App Engine to deploy our project composed of html, css, javascript, and python.   Python Google App engine and Linear Regression Twitter Data*Using openly available twitter data-set (5 million tweets) we indexed them at scale, using elastic search. *We calculated sentiment of every tweet using machine learning model Consensus Data*Acquired various political and statistical data, of USMerged both these data-facets using geo-spatial arrangement of real time mapping I built the site with HTML, CSS, and vanilla JavaScript. Material-UI for the user interfaceReact and JSX for websiteWebpack for bundling and deploying siteHeroku for hosting flight engine server Front End - React & Material.UIBack End - Flask & Firebase  Node.js server and a mongoDB. Android app using Kotlin and Java. iOS and Mac app using Swift.  Forked from knownorigin.io The reDAOmint is built using the Cosmos SDK using a fork of cosmos/gaia that includes IBC support. A diff of our work can be seen here: https://github.com/regen-network/reDAOmint/pull/3.We produced two new Cosmos modules and an ORM package to create the reDAOmint.redaomint moduleThe redaomint module implements a shareholder DAO that produces dividends of ecosystem service credits for shareholders and dividends of the underlying asset pool for allocated land stewards as described above. It also provides governance proposal and voting support for shareholders. It uses the existing Cosmos bank and supply modules to track holdings of shares, and interacts with the ecocredit module for verification of good land stewardship and distributing credits. The primary documentation for the module can be found here: https://github.com/regen-network/reDAOmint/blob/reDAOmint/x/redaomint/keeper.goecocredit moduleThe ecocredit module provides a fractional NFT with metadata specific to ecosystem service credits. It allows for:the creation of new credit classes with a list of approved issuersthe issuance of credits for a specific piece of land and time frameexchange of fractional portions of individual creditsburning credits in order to remove them from circulation (in the language of carbon credits this is called retiring and means that you are using the credit as an offset)Documentation here: https://github.com/regen-network/reDAOmint/blob/reDAOmint/x/ecocredit/keeper.goorm packageIn order to make the implementation of the above modules easier, we implemented an orm package to handle secondary indexes and the automatic generation of ID's. This is inspired by the Weave SDK's and is something we've been intending to build for a while. While it was its own "mini-project", it greatly simplified implementation of the other code. Front end and back end coding was designed using Python and the technique of machine learning  The device is simple, using 4 simple tactile buttons wired to a Particle Photon, which connects via webhooks to a Flask server deployed to a Google Cloud Platform VM instance. The Flask server uses the MongoDB Atlas API to save all button press requests for order confirmation and supply information, Twilio to send texts to users, and Sendgrid to send emails as well. The enclosure was designed in SolidWorks, sliced in Cura, and printed on a Lulzbot Taz6 and Creality CR-10mini. What's next for buttonsI will see how I can couple the device to actually order supplies directly- like the Amazon Dash button. I will also add more functionality to the buttons-- and will build more button devices! The device can also be used by consumers- put this device on your fridge so you get reminded to throw out that old milk, or to use your peppers before they go moldy! The platform is really versatile.Built Withflaskgcpmongodbpythonsendgridtwiliovm      Submitted to    HackDuke: Code for Good 2019    Created by  I did everything. :)Jason Liu [ ] We built the frontend in React.js and React Bootstrap and the backend is a REST API built in Express.js[ ]We built a trained a neural network to take in a preprocessed photo and return a label for it Pay using WAVES with domain.com I have used Hyperledger blockchain to build this  Under the hood, Squink uses a generalization of the Uniswap constant product formula that allows for multiple assets and a user-specified level of price volatility. In very low volatility markets such as stablecoins, this optimization allows Squink to support an extremely high level of liquidity using a very small pool of capital. We created our own Tendermint-based chain using Lotion to implement the automated market maker and to host various faux stablecoins.Accomplishments that we're proud ofSquink has the potential to undercut all existing services by 2-fold or more (10-fold if we consider Coinbase). We’re very proud of such a clearcut value proposition.What's next for SquinkWe would like to configure Squink to hold the bulk of its stablecoin assets in a reserve composed of interest-bearing form, such as cDAI or cUSDC. This would enable Squink to offer some combination of better returns for liquidity providers and even lower prices.   Built WithgojavascriptpythontypescriptTry it outgithub.comgithub.com      Submitted to    DeFi Hackathon    Created by  Daniel PyrathonFrancesco Agosti We utilized MySQL for the database storing the student's information, React for the UI and Node.js Express for the server. Leveraged the Cosmos SDK and the Flutter UI to build the tooling. Google web hosting and API lots of sweat and tears. mostly using truffle and solidity We built our whole demo using Adobe's prototyping tool: XD  We built this application using the dash library in python    We built it using reactjs front end and django backend with graphQL and a sqlLite DB -Whimsical    Cloud-based; For visual workspace for collaborative wireframes, flowcharts, sticky notes and mind-maps. -Figma    Cloud-based; Design, prototype and gather feedback from user testing.-Excel    Providing our database of trees.-ArcGIS Web application allowing sharing and search of geographic information. For location-based database filtering.-Slack    Communication, synchronization and file sharing among team members.  I used Reactjs and the Google Maps API to build the front end which displays the data sent from the server. The UUID(Universal Unique Identifier) of each detected bluetooth device is shown as well as the GPS coordinates of the drone when the bluetooth signal is detected. The client for the drone uses Noble, a Node.js library for bluetooth, to scan for and get information on the device. Then, the drone client will send the data collected to another Node.js server which will send the data to the first responder's central hub.  We used cosmos and a lot of coffee. I built it using Cosmos SDK.  The android app was built using Android studio; we implemented a scan button to launch the camera app.  Once the camera app is launched, the user can take a picture.  Afterwards, we create an intent to go to another activity where the user can construct a rectangle around any area that is within the boundaries of the imageView (which displays the photo).  After drawing a legitimate rectangle, we prompt the user to enter a String that corresponds to the handwritten data.  Once we have the fields, rectangle coordinates, and image (base 64), we put this into a json which can be sent to the cloud instance. The response consists of a JSON document with the digitized form. This cloud instance was processed with Google Cloud Vision API in python, and a JSON document with all of the results was sent back to the android app for the user to receive. The backend processing, including storing all of these events and converting addresses to lat/long coordinates, is a Python flask server, and the front end is an Angular web app. In order to construct the database of certifications, we used the EPA API as well as the Selenium API to scrape websites for certifications of thousands of companies. The database of agricultural materials was pulled from an open source database, collected by European agencies.  We used android studio to build the companion app that tracks user's typed texts and relays them to our server (built via flask) which runs the data through a machine learning model to interpret the user's mood. Then we display the data in graphs on our website built via html, css, and js and statistics. We then encourage positive thinking through the use of cognitive behavioral therapy initiatives like a gratitude journal  We built several parts including a dashboard for managing the AR experience, a dedicated AR web app built on HTML5 canvases, an API for accessing GIS data sources, and a visualization dashboard for GIS data based on geolocation. We used Bootstrap and PHP for the front end and Python for the backend. Data was stored in a .csv file, and Pandas was used to process the data. We used iexfinance to retrieve stock performace data. We used the API exposed by Capital One, a remote Ethereum Node running on Infura Infrastructure Provider connected to the Metamask Chrome Extensions that allows to manage your Ethereum accounts, sign transactions, move money, etc. The app was developed using React and totally serverless with the Metamask authentication system. Our Front-End was developed using React and CSS. We built a database back-end using MongoDB Atlas on Azure and used a Java Spring API to map the database to endpoints.  Initially, we wanted to create a web-application using React, but then we thought that it would be fun to learn app development so we experimented with android studio. We coded the back-end in Java and ran the code in the android studio. We also used Procreate to design the logo.  