This was built using android studio, firebase, and MapBox.  
 Swift, Google APIs, Firebase, Apple APIs 
 The demo web application uses the Accuweather API to gather historical and forecasted weather data in the user’s location.  Each datapoint is configured in our algorithm with adjustable weights, which are tuned to aggregate a relative safety score from 0-10.  The backend is implemented with a Python Flask server on an Amazon EC2 box.GetOffTheRoad uses the following technologies in the sample implementation:Twilio Communications APIAccuweather APIApache HTTP ServerPython / Flask Web FrameworkMongoDBAmazon Web Services (Ubuntu EC2 Server) 
  
 We built the blockchain using java IDEs Intellij and Netbeans, and we enlisted the help of a few coding mentors.  
  We built our website on Amazon's web services (cloud 9). We used coding languages such as Python, HTML, and CSS. We also involved APIs from Google and Twilio.  
 web app via HTML/CSS/Javascript, our own API, and Amazon Alexa 
 For the frontend, we built a web app using React. On the backend, we have a few different technologies in use. For storing data and hosting our demo, we used Google Cloud, MongoDB, and UiPath Automotion because of its ease of use. For capturing the user's health data including their heart rate, food intake, and water intake, we wrote a backend in Python using Flask to pull the data and store it in the cloud. We also built a separate Node.js backend to capture information about the user's range of motion in their limbs, which we prototyped capturing from either a Bosch sensor tag or Android device.Future ImplementationsWe would like to use the Twilio API to incorporate video calling features. Built Withcssflaskgoogle-cloudhtmljavascriptjsonmongodbnode.jspythonreactuipath      Submitted to    HackPSU Fall 2019    Created by  Abhinav MamidipakaAnnika Iyengar 
  
 Our user interface relies on google diagflow to build the chatbot. Our database relies on google firebase to host our data. Our backend web app runs on flask. The machine learning clustering algorithm which determines best coordinates for relief camps utilizes sklearn. Our weather prediction utilizes accuweather's api. The dashboard for the web app is coded in react.js. 
  
 Used python to scrape and convert data 
  
 Use weather API to get data and use PyQt5 to build user interface(UI).  
 We used XCode/Swift to build the front-end of our app and interact with the AI Chat Bot using Google Cloud's Dialogflow. 
  
  
 The stack relies heavily on WebSockets. When a user raises an "event," which in this case is making a drawing stroke on a canvas, or moving a card between categories on our board, the event is sent between clients using WebSockets. We then pass the event to the persistence layer (Java API), which relays the completed event to every client currently connected.The frontend is built with Bootstrap, we have the server running on AWS. 
 We decided to build this by dividing into different product development roles and teams. Vishal created a custom API that the iOS application could query data from using Python, Flask, and HTML. Rahul combined databases of information while cleansing and parsing through thousands of datapoints of expiration dates using Pandas in Python. Harish developed the front-end iOS application using SwiftUI which calling the API and displays the information for the user. Arjun worked in a variety of roles, from data scraping to flask development and front-end design within the team. James focused on using web scraping techniques through Python and BeautifulSoup to harvest and add data to our database. 
  
  
 Justin built a heroku flask API to serve live NBA data & Dayana built the front end for WEb3-React. Casmir also wrote GO-Cosmos-IBC for RapNameServices & "NBAChain". 
 We built two classes: one composing of the GUI, and the other composing of a combination of the Accuweather API and the Twilio API.For the project as a whole, we used Java. However, for the GUI, we used a combination of Java and JavaFX.More specifically, we took a user input of a town/city from a text field in the GUI. Then, we sent that town/city input to our API class. In that class, we manipulated the weather data corresponding to that location to calculate and display the current weather conditions, driving safety, and outdoor comfort of that place. We also used our API class to give the user an option to have all of that data (location, weather conditions, driving safety, and outdoor comfort) compiled and sent to a phone via text message. 
 By implementing a cleaner design and reconfiguring the way that users will navigate through the website, we built a more intuitive and user-friendly product. We used google sites as our platform and embedded HTML code to add features, including: the scrolling news section, adding social media platforms to the site, and font/formatting manipulation. We implemented google forms as a means of submitting applications.  
 Method - Message AnnotationCleaned the messages for removing numbers and special characters.Tokenized the messages and the stop words are removed.Further cleaning to split the hashtags into a complete sentence.Processing to get full sense of known abbreviations.Labelled data trained with a GridSearchCV model.Any new emergency request will now be annotated with appropriate tags.Method - Disaster PredictionLimiting the disaster to Wildfires for the scope of HackPSU.Labelled Images are trained on a CNN.Binary Cross Entropy Loss and RMSProp.Polling satellite data periodically to make predictions based on the learned features.Responding to the prediction by broadcasting safety alerts to all the citizens in the vulnerable areas.LAMP StackHosted on a GCP VM instance running UbuntuApache for web serverMySQL database for all the citizen, appeal data and annotations.PHP as a backend with integration of machine learning with Python 
 We created the scene in Unity3D. Using ARFundation toolkit, the app will detect features such as planes, windows or a designated image and spawn virtual objects in our desire. With game logics and animations, the scene is interactive and flexible for added features.  
 We built a website using HTML, Node.js, and CSS that takes in a user’s input. We also wrote a python script using pandas to reformat all of the data files into a standardized format. We also created an R script that takes the user input and the data and visualizes it in the form of histograms, scatter plots, line graphs, etc. 
 Mobile app wireframe using Balsamiq3Website using Vue.js, BootstrapWhat I learnedConsiderations designed around the interests of the homeless and donors is crucial to attracting the target demographic to use our set of web and mobile toolsWhat's next for HandUpRegulations: restrictions on OTC medications, similar to welfare guidelines (as per SNAP, WIC) Get corporate participation and matching Built Withbalsamiqcss3html5vue.jsTry it outgithub.com      Submitted to    Girls in Tech Hacking for Humanity 2019    Created by  logo design, also contributed to project concept, mockup website contentDeemSumAllDayI was the product manager and I also worked on the website front-end development. Stephanie GloverI worked on creating the mobile app wireframe. Calista WongVijaypriya  Ganesankrypto100 
 registered the domain name on domain.com 
 I used Unity Engine to develop the simulation and a surfeit of terrain mapping software to create the simulation.  
 Python Django and developer access to Smartcar API. 
 We worked as a team of 5, helping one another during times of struggle. We worked on FrontEnd using React Native, a cross-platform development environment and Backend using google-cloud and firebase. We used a lot of GCP products and some of which include: Firebase Authentication, Google Maps, and Google Cloud.  
 We used Java and GameMakerStudio to interact with SmartCar API.  
 Retrieved students' information from Google Classroom API. Processed data to get ready to be uploaded to Salesforce.Build local Webservice to automate first two stepsAttempted to design an Apex function in Salesforce to send requests to the WebserviceAttempted to designed report format in Salesforce for demonstration 
  
 As stated earlier, we built our application with the usage of React and Node. We utilized Node to pull the data that we were interested in from Google Classroom, and we used React (and Reactstrap) to create a friendly user interface. 
  
 Using Python, we built a script to access the API and generate an HTML page with interactive node information.Main libraries used were requests, folium, and ip2geotools 
  
 We approached our project with the goal of constructing a functional website that could be accessed readily and immediately. We utilized Google App Engine to deploy our project composed of html, css, javascript, and python.  
  
 Python Google App engine and Linear Regression 
 Twitter Data*Using openly available twitter data-set (5 million tweets) we indexed them at scale, using elastic search. *We calculated sentiment of every tweet using machine learning model Consensus Data*Acquired various political and statistical data, of USMerged both these data-facets using geo-spatial arrangement of real time mapping 
 I built the site with HTML, CSS, and vanilla JavaScript. 
 Material-UI for the user interfaceReact and JSX for websiteWebpack for bundling and deploying siteHeroku for hosting flight engine server 
 Front End - React & Material.UIBack End - Flask & Firebase 
  
 Node.js server and a mongoDB. Android app using Kotlin and Java. iOS and Mac app using Swift. 
  
 Forked from knownorigin.io 
 The reDAOmint is built using the Cosmos SDK using a fork of cosmos/gaia that includes IBC support. A diff of our work can be seen here: https://github.com/regen-network/reDAOmint/pull/3.We produced two new Cosmos modules and an ORM package to create the reDAOmint.redaomint moduleThe redaomint module implements a shareholder DAO that produces dividends of ecosystem service credits for shareholders and dividends of the underlying asset pool for allocated land stewards as described above. It also provides governance proposal and voting support for shareholders. It uses the existing Cosmos bank and supply modules to track holdings of shares, and interacts with the ecocredit module for verification of good land stewardship and distributing credits. The primary documentation for the module can be found here: https://github.com/regen-network/reDAOmint/blob/reDAOmint/x/redaomint/keeper.goecocredit moduleThe ecocredit module provides a fractional NFT with metadata specific to ecosystem service credits. It allows for:the creation of new credit classes with a list of approved issuersthe issuance of credits for a specific piece of land and time frameexchange of fractional portions of individual creditsburning credits in order to remove them from circulation (in the language of carbon credits this is called retiring and means that you are using the credit as an offset)Documentation here: https://github.com/regen-network/reDAOmint/blob/reDAOmint/x/ecocredit/keeper.goorm packageIn order to make the implementation of the above modules easier, we implemented an orm package to handle secondary indexes and the automatic generation of ID's. This is inspired by the Weave SDK's and is something we've been intending to build for a while. While it was its own "mini-project", it greatly simplified implementation of the other code. 
 Front end and back end coding was designed using Python and the technique of machine learning  
 The device is simple, using 4 simple tactile buttons wired to a Particle Photon, which connects via webhooks to a Flask server deployed to a Google Cloud Platform VM instance. The Flask server uses the MongoDB Atlas API to save all button press requests for order confirmation and supply information, Twilio to send texts to users, and Sendgrid to send emails as well. The enclosure was designed in SolidWorks, sliced in Cura, and printed on a Lulzbot Taz6 and Creality CR-10mini. What's next for buttonsI will see how I can couple the device to actually order supplies directly- like the Amazon Dash button. I will also add more functionality to the buttons-- and will build more button devices! The device can also be used by consumers- put this device on your fridge so you get reminded to throw out that old milk, or to use your peppers before they go moldy! The platform is really versatile.Built Withflaskgcpmongodbpythonsendgridtwiliovm      Submitted to    HackDuke: Code for Good 2019    Created by  I did everything. :)Jason Liu 
 [ ] We built the frontend in React.js and React Bootstrap and the backend is a REST API built in Express.js[ ]We built a trained a neural network to take in a preprocessed photo and return a label for it 
 Pay using WAVES 
 with domain.com 
 I have used Hyperledger blockchain to build this 
  
 Under the hood, Squink uses a generalization of the Uniswap constant product formula that allows for multiple assets and a user-specified level of price volatility. In very low volatility markets such as stablecoins, this optimization allows Squink to support an extremely high level of liquidity using a very small pool of capital. We created our own Tendermint-based chain using Lotion to implement the automated market maker and to host various faux stablecoins.Accomplishments that we're proud ofSquink has the potential to undercut all existing services by 2-fold or more (10-fold if we consider Coinbase). We’re very proud of such a clearcut value proposition.What's next for SquinkWe would like to configure Squink to hold the bulk of its stablecoin assets in a reserve composed of interest-bearing form, such as cDAI or cUSDC. This would enable Squink to offer some combination of better returns for liquidity providers and even lower prices.   Built WithgojavascriptpythontypescriptTry it outgithub.comgithub.com      Submitted to    DeFi Hackathon    Created by  Daniel PyrathonFrancesco Agosti 
 We utilized MySQL for the database storing the student's information, React for the UI and Node.js Express for the server. 
 Leveraged the Cosmos SDK and the Flutter UI to build the tooling. 
 Google web hosting and API 
 lots of sweat and tears. mostly using truffle and solidity 
 We built our whole demo using Adobe's prototyping tool: XD 
  
 We built this application using the dash library in python 
  
  
  
 We built it using reactjs front end and django backend with graphQL and a sqlLite DB 
 -Whimsical    Cloud-based; For visual workspace for collaborative wireframes, flowcharts, sticky notes and mind-maps. -Figma    Cloud-based; Design, prototype and gather feedback from user testing.-Excel    Providing our database of trees.-ArcGIS Web application allowing sharing and search of geographic information. For location-based database filtering.-Slack    Communication, synchronization and file sharing among team members. 
  
 I used Reactjs and the Google Maps API to build the front end which displays the data sent from the server. The UUID(Universal Unique Identifier) of each detected bluetooth device is shown as well as the GPS coordinates of the drone when the bluetooth signal is detected. The client for the drone uses Noble, a Node.js library for bluetooth, to scan for and get information on the device. Then, the drone client will send the data collected to another Node.js server which will send the data to the first responder's central hub.  
 We used cosmos and a lot of coffee. 
 I built it using Cosmos SDK. 
  
 The android app was built using Android studio; we implemented a scan button to launch the camera app.  Once the camera app is launched, the user can take a picture.  Afterwards, we create an intent to go to another activity where the user can construct a rectangle around any area that is within the boundaries of the imageView (which displays the photo).  After drawing a legitimate rectangle, we prompt the user to enter a String that corresponds to the handwritten data.  Once we have the fields, rectangle coordinates, and image (base 64), we put this into a json which can be sent to the cloud instance. The response consists of a JSON document with the digitized form. This cloud instance was processed with Google Cloud Vision API in python, and a JSON document with all of the results was sent back to the android app for the user to receive. 
 The backend processing, including storing all of these events and converting addresses to lat/long coordinates, is a Python flask server, and the front end is an Angular web app. 
 In order to construct the database of certifications, we used the EPA API as well as the Selenium API to scrape websites for certifications of thousands of companies. The database of agricultural materials was pulled from an open source database, collected by European agencies.  
 We used android studio to build the companion app that tracks user's typed texts and relays them to our server (built via flask) which runs the data through a machine learning model to interpret the user's mood. Then we display the data in graphs on our website built via html, css, and js and statistics. We then encourage positive thinking through the use of cognitive behavioral therapy initiatives like a gratitude journal  
 We built several parts including a dashboard for managing the AR experience, a dedicated AR web app built on HTML5 canvases, an API for accessing GIS data sources, and a visualization dashboard for GIS data based on geolocation. 
 We used Bootstrap and PHP for the front end and Python for the backend. Data was stored in a .csv file, and Pandas was used to process the data. We used iexfinance to retrieve stock performace data. 
 We used the API exposed by Capital One, a remote Ethereum Node running on Infura Infrastructure Provider connected to the Metamask Chrome Extensions that allows to manage your Ethereum accounts, sign transactions, move money, etc. The app was developed using React and totally serverless with the Metamask authentication system. 
 Our Front-End was developed using React and CSS. We built a database back-end using MongoDB Atlas on Azure and used a Java Spring API to map the database to endpoints.  
 Initially, we wanted to create a web-application using React, but then we thought that it would be fun to learn app development so we experimented with android studio. We coded the back-end in Java and ran the code in the android studio. We also used Procreate to design the logo.  
 We used html, css, google maps api, and fitbit sdk. 
  
  
 We used android Studio 
 We built this app from scratch using the python framework, Django, Javascript, HTML, CSS, and the Google maps api. The site ended up being hosted and deployed on Heroku. 
 Main Features:-Implemented a graphical user interface to enhance the user experienceUtilized the necessary HTTP python modules and Google Maps API framework to effectively retrieve GPS location of the concerned individualEstablished an automatic text message and email delivery system through existing SMTP and Twilio infrastructureEnacted text-to-speech conversion to yield an audio message attachment to be sent via email using the gTTS module  
  
 We used python to create both the GUI and the backend. Using the google-api, we scraped results of the Google Forms and input them into a clustering algorithm powered by sklearn. A one-zero distance metric was used for determining the loss associated with different answers to multiple choice questions. Sentiment analysis was performed by parsing student inputs and searching for positive and negative words (from the canonical list of 6000 training words), and determining which direction the sentiment was more likely to be. On the GUI side, we used tkinter to create a graphic for teacher use. Teachers need only input the URL to google form results and the number of groups they seek to create, and GroupYou will generate the associated teams and display them in the GUI. 
 We built it using our research but applied with HackPSU sponsors in mind. 
 Each person was tasked with creating a separate class for each API  that accesses and formats its data.We tied all of our work together in Github and collaborated to have it working on Amazon Web Services. 
 With passion, python, and a whole lot of coffee! 
 our primary database. The entire components are pushed and maintained in Docker containers. We built a native iOS app using Swift and their new UI framework, SwiftUI. The app is entirely asynchronous thanks to the Combine framework and the declarative design of SwiftUI. 
 We started by building prototypes of the website and developing the user-flow for the Matching tool. Then we brainstormed different survey questions, designed a scoring algorithm that ties into the backend data architecture.  
 We used Android Studio to develop the app. 
  
  
 Payme Bear is built with a crypto-based gift-card-like payment and stored value program. It uses a React-based client for the frontend interface and to interact with contracts and decentralized wallets, with the help of Zabo. Payme Bear Ethereum accounts are created via Web3. After depositing funds, a portion is immediately sent to Aave to earn interest until recovered by the user or used for purchase (in progress). QR codes are generated on the edge device for payment at merchants (in progress) and merchants can log in to accept payment as well (in progress). 
 We built smart contracts on the Ethereum blockchain to represent the protocol. We drew inspiration from Maker, Compound and Yield designs to design our insurance protocol. We built an insurance protocol that uses put options.On the front end, we used Figma to test out our designs and then built a React app. 
 We used Andriod StudioBuilt Withandroid-studiocharitysearch-apifirebasegoldmansachsmarquee-apikotlinTry it outgithub.com      Submitted to    HackTX 2019    Created by  robertyguan Guanqwertyhwintrandominique 
 Python for the basic stuff, Perl for some data generation, Flask as backend to interactive web app, Javascript for web app. 
 Riize is a cross platform mobile application built on React Native. We built it using several mobile friendly components. We also modeled a Firebase Realtime Database that would display how we would store and rank content that we would provide the user to ensure the highest quality is delivered. 
  
 We built it in two-fold - Mahir spent his time organizing 5 years worth of quantitative data from over 100 securities (kindly provided by Goldman Sachs) to understand what kind of effects a specific Alito Index would have on an industry and it's related securities.On my end, I spent my time classifying 3500 unique data points of Congressional photos, from 6 different Congressional classes. I trained a facial recognition model and image classification model to understand who Samuel Alito was, and what his physical features were, and then fed massive datasets to eventually get an Alito Index for each member of Congress. Once each member of Congress was assigned an Alito Index, we could simply average each Congressional Classes Alito Index, and apply it to the 5 year dataset of Securities, and look for instances of correlation. Our end goal was to show that diversity in Congress does play a fundamental role in how markets move and how your vote (and every vote) counts, especially with voting season right on top of us. 
  
 The meat of our project consists of two API calls. First, we scrape the website for the article title and details. Then, we use the ParallelDots Natural Language Processing API to extract keywords. We then take a select few of those keywords and query the NewsAPI, and filter those results by relevancy.  
 Unity engine: C# scriptingVuphoria AR library: Marker/image target libraryGoldman Sachs Marquee: Financial score API dataIEX Cloud: Realtime stock API data 
 We used Heroku, the API of GroupMe and hosted in on Heroku due to the user-friendly environment it offers. 
  
 Cosmos SDK uses the Auth module to store the facial recognition data. We used the Zoom SDK to compare that facial recognition data to a live scan providing facial recognition authentication. 
 Using CosmosSDK, we created a module that can validate stateless SPV proofs. The IBC module was not ready for our application, so we used LotionJS to put togther a simple working application. We have a front end and collect proofs from a bcoin node. 
  
 For our backend we used Python to code a Multinomial Naive Bayes Classifier to classify articles using a labeled news article database. We also used Flask to build the Webapp's backend. For our frontend, we used HTML, CSS, and JS to create a functional and appealing website. 
 Blender! My full source list is 20+ sources and running - and that's getting written up since a couple of people have asked. 
 We architected and designed a cross-platform mobile application using Flutter. We integrated AI Libraries such as Google Cloud Speech-to-Text and Google Cloud Natural Language API and integrated them using Cloud API functions, Flask, JavaScript, and Python.  
  
 ParkX was first prototyped in Adobe Xd in order to streamline the workflow process. Our user data and parking locations are stored in MongoDB Atlas clusters hosted using Azure. We also utilize Google Autocomplete to allow the user to easily select a location. The app was developed using Java and XML in Android Studio. 
  
  
  
  
 We created the backend using Flask, which serves up an API that the front end will use. The API makes CRUD operations to our mongoDB to maintain users and posts. The front end is written in react.js, and serves the webpages and feeds that our users will see. The whole thing is containerized using docker, and served on an Azure kubernetes cluster. 
 As the Cosmos Hub does not support smart contracts or token issuance, it was decided to develop a separate Cosmos Zone that takes care of the smart contract logic and bATOM issuance. To prevent events like slashing from creating undercollateralized LSPs, it was decided to keep 10% of the bonded ATOMs as the slashing collateral. Whenever the slashed amount goes over 5%, the Everett Zone automatically submits an unbonding request to the Cosmos Hub, therefore liquidating the LSP in the risk of undercollateralization.A previously suggested ICS proposal regarding interchain accounts was utilized. This proposal allows the creation of a special address on the Cosmos Hub, which is controlled by Cosmos Zone. This feature is crucial in order for forced liquidations to be triggered in the case of slashings that go beyond the threshold amount.Simplified bATOM Generation Process Generate an Everett Zone controlled user-specific interchain address on the Cosmos Hub Transfer ATOMs to the newly created interchain address Send data including the validator to delegate the transferred ATOMs to A delegate TX is sent to from the Everett Zone to the Cosmos Hub via IBC If all is successful, excluding the slashing collateral of 10%, new bATOMs are granted to the delegator’s addressSimplified ATOM Recovery Process Requests the closure of one’s LSP by submitting a close TX to the Everett Zone The Everett Zone validates whether the user has enough funds in their account If the user’s account contains enough bATOMs, the bATOMs are locked and the ATOM retrieval process is initiated. An unbonding TX is sent from the Everett Zone to the Cosmos Hub If 4 is successful, the locked ATOMs in 3 are burnedA LSP can only be closed by the delegator who initially opened it. The delegator is required to hold the same amount of bATOMs they first created.TLDR of the add-ons done during the hackathon Swapped out our previous pseudo-IBC in exchange for the real thingUse of Interchain Accounts A better frontend for non-aliens Liquid Staking Position (LSP) NFTs, an additional token that has governance rights & receives rewards Tokenized Leveraged LSPs, allowing delegators to choose different risk & reward profiles (e.g. buy 3x leverage LSP tokens to receive 3x the rewards, while the money you lose when slashed also increases 3x.) 
 A lecture contains three key components, speech, visual(input from the board) and transcripts, from which we have to understand the context to provide a good set of questions.Used text detection, HTR techniques by using tensor-flow to get the text from the board.Used an online tool voice-notepad to get the text out of the speech.Employed NLP techniques using nltk to understand the context and form questions. 
  
 We build it using Unity and ML-Agents 
  
 Basically we reinterpreted cellular automata to separate the input space from the message space. What that means is we can run cellular automata in three different ways, forming a chain of actions and interactions. The visualization we created takes an ordered image (gradient mapped along a hilbert curve) and disturb it by passing the inputs through a randomly chosen pair of cellular automata rules and observing the localized effects of their interactions. Message = Rule(Input)Rule(s) = Input(Message)Input(s) = Rule(Input)Passing an input through a rule to receive a message reduces the amount of information we can extract from it, because running that message backwards to extrapolate the initial input in fact can only give you a possibility space that contains probable inputs that generated that message. Sp when you group two rules and a message, what you're creating is a mapping between one input space onto another input space, where only some of those inputs are relevant. To do this, we setup an ordered hilbert curve and used colour to express that order. Then we disturbed that order by choosing two random rules and seeing how the curve visually translates when run through them. The function we used to update the colour values is the same used in reinforcement learning to update the reward expectations.  
  
 I built this with HTML, CSS and JavaScript.  
 We developed a mobile app using Android Studio, programming in Java and XML. 
 Our front end framework uses Vue js. We use the Vuetify material design library for our components. Vue router is used for routing pages and Vuex for state management. For our back end we are using Firebase. We make use of Cloud Firestore databases to update our data in real time. We are also using Cloud Functions in firebase to make use of the google calendar API to schedule tutoring times. The ConnectWise api is used to create a new session and email the links to users. Clicking these links will connect tutors and students to an online session where they can share their screen and communicate by voice. 
 Right now it's in the ideation, building, and prototyping phase. We are still researching the best ways to build this DAO distributed cryptocurrency.  
 The MVP:We have a home page with search bar and popular topic for user to navigate the knowledge base. Home page is linked to the document pages which hold the original information of the topic in markdown. Users can choose to edit the content and the edited information is hashed and saved using Git microservices. The hashes from the Git is also saved to the blockchain, cosmos here specifically. We chose Cosmos, so that if there are informations that are stored on the other blockchains like everipedia on EOS can be migrated to our blockchain if needed in the future.What we learnedWhat we learned most is from experiencing with Cosmos SDK. We are happy that we are incentivized to learn this new technology other than just building Dapps on ethereum. We realize that more time is needed to use the SDK for us and we want to continue the learning post hackathon.What's next for FormContinue integrate with the cosmosTransfer knowledge across blockchainsExplore use cases in wikipedia or newsBuilt WithcosmosgitreactTry it outgithub.com      Submitted to    DeFi Hackathon    Created by  Maying ShiSigned up late. Anyone still looking for teammate wanna crank out an project. Mostly here to learn! Preferably, close to LA?Elden S. ParkCreator and adventurer. Web tech enthusiast 
 We built "About This" mainly on Azure. We used Azure Web App Services for the front-end website, Azure Cognitive Services - Text Analytics API for the sentiment analysis of news articles, Azure batch computing to schedule hourly collection of news articles for sentiment analysis, Azure Blob Storage to store JSONs of sentiment scores for articles, Azure Data Lake Containers to organize and store the Blobs, and Azure Functions for the front end to retrieve sentiment scores from the backend.For the front end Azure Web App Services, we used React to create the website. The front end used HTTP GET Requests to our Azure Functions, an event-driven serverless platform, to retrieve sentiment data from the backend.The back-end used Selenium and Beautiful-Soup to get links and article content for searched terms. The articles are then ran through Azure's Text Sentiment Analysis API to determine sentiment scores for each article. Once the scores are calculated, they are stored in the database as well as returned back the users. The scores are saved to the database so that if the user or any other user wishes to search for that term again, the data will be ready much quicker. Additionally, all search term scores stored into the database have their articles regathered and rescored every hour to allow for as up to date sentiment information of the search term.  
 IBCXRP -testnetCOSMOS (lotionjs)Hedera HashgraphBuilt WithhederaibcjavascriptlotionjsrippleTry it outgithub.com      Submitted to    DeFi HackathonWinner                Testnet Challenge - Xpring                  Created by  Jackie ZhangBamboozledKent MakishimaBetter Luck Tomorrow 
  
 Austin built models in Blender and textured them in Unity. Ines assembled the back end in Unity. We utilized textures from Quixell for many of scenes, which was extremely helpful in establishing a realistic look in our models. Conversations we held prior to the Hackathon with Ryley Mancine, a researcher in Psychiatric Science at Michigan State University, were immensely helpful in providing the prior theoretical knowledge that informs our level design.  
  
 We all built it in game maker. 
  
 We built a plugin for the new, plugin-enabled Metamask browser extension.  The plugin provides a secure, in-process bridge from the running Dapp browser code and a locally-installed Agoric blockchain client.We modified the Agoric sample autoswap (i.e. uniswap-like) Dapp and the Agoric sample wallet to use the MetaMask plugin's newly-provided in-browser APIs to fetch balances, conduct the exchange and query transactions.All code written was in Javascript. 
 The team used an open source algorithm to mine our blocks that store data such as transactions and increases or decrease of property values. We used  
 Swift to build an iOS application that buyers can buy, refund and trade their concert ticketsTendermint blockchain with cosmos sdk to run module specific functions (buy, sell, refund)EC2 instance to host the blockchain node for rest api endpoints 
 https://github.com/ZcashFoundation/libbolthttps://github.com/cosmos/cosmos-sdkhttps://github.com/kaiba42/rainbolt-price-feedhttps://github.com/kaiba42/rainboltdhttps://github.com/rusty-jules/rainboltImplementing libbolt as a server in rust and creating a client to interface with a cosmos app chain.  
 We used Python in all parts of our codings. At first, we generated several synthetic light-curves and created a large training dataset. Then we created a model using our training set via the Convolutional Neural Network (CNN) technique. Finally, we checked the performance of our algorithm using both the synthetic data and the real data from the Kepler mission in a test set. 
 We used Renpy to code the project, drew all of the art, and composed a theme song that plays throughout the story. 
  
  
 We built it using native Python 3 and Matplotlib. A lot of abstraction and object oriented programming was done to simplify the process. We used realistic particle decay kinematics and probabilities, taking into account special relativity. 
 Cloutfit was built in Flutter, a mobile app framework, and the underlying database was Firebase. We used Flutter because it exports to both iOS and Android. Github was also used for version control and to integrate our individual assignments 
 We used HTML/CSS and Bootstrap for front-end; as well as JavaScript, Jquery, and FlightEngineAPI (provided by American Airlines) for the back-end. We also used Adobe Illustrator and Photoshop to generate the graphics for the UI. 
  
 We built it using the Python coding language and by using the Twilio software. 
  
 We used JavaScript, React framework. We incorporated a Twilio API to send SMS messages. 
 We developed a tool to pull tweets in real time from Twitter that were geo-tagged and relevant to the wildfire we're tracking. Using this, we triangulate the location of the fire at a given point of time and chart a path of the fire. Using our time-series and random walk models, we then find the areas that are mostly likely to be caught up in the blaze. 
  
 We built it using Azure (Computer Vision, Maps), Python/ Flask, MongoDB, and Javascript. 
 We built an HTML file with a point accumulation system that would return point-based feedback after user submission. 
  
 python cs1grapics 
 We used pygame for all the graphics, and computed the gravity, speed, and position using numpy. Numpy provides us with the FFT and matrix operations tools we need. 
 It's built using Unity and C#. We designed assets in Illustrator.  
  
 We dove into the unknown world of quantum computing using qiskit, a python module heavily invested by communities and IBM. We read papers on quantum pseudotelepathy games and found the strategies for Mermin-GHZ game. We then converted the strategy into an actual quantum circuit and implemented it. We also learned dash and various html and css to make our web app beautiful. We explored machine learning to see if one hidden layer neural network, which can model any binary function, can learn the classical optimal strategy. But, the result of the neural network did no better than a classical random strategy. Built WithdashkeraspythonqiskitTry it outgithub.com      Submitted to    McGill Physics Hackathon 2019    Created by  Justin LiXavier Capaldi 
 We used Naive-Bayes, a machine learning algorithm, to identify them. We labeled 1000 youtube titles as clickbait or not to train it. 
 We used python and the google maps api 
 JukeCloud provides an aesthetic webpage that interfaces with a backend server to provide an interactive GUI to each user. The backend was done primarily in python utilizing flask, while the front end was rendered in HTML and CSS serviced by React.In order to acquire the songs, we utilized Youtube’s API to search for a song and play it through embedded Youtube audio.  
 We started from defining different functionalities we would like to achieve. Then we dice deep into the corresponding components our app should include for those functionalities. Our implementation is in Xcode. We implement the backend using Google Firebase. More specifically, we user Firebase authentication library for user login and signup, and Firestore to store user information. We choose Firebase out of the consideration of reliability and stability. To fetch hospital and medical center information, we call API offered by the MEDICARE HOSPITAL COMPARE DATASET by giving a URL including the geolocation information of the user and the search radius defined by him/her. We've also added the functionalities of displaying route to the hospital and rating of it after the user pick one from the search results. 
 The core of our product is connecting shelters together based on need. In order to get our data, we scraped Goodwill stores for product information, formatted them to be stored in our Mongo DB database, and kept track of their specific categories. With a set of categories, we could easily show what percentage of a shelter's inventory is made of a certain category, and allow shelters to specify what percentage they require. With this in hand, we could match shelters together based on their respective surpluses and shortages.The website itself is a Node JS web server, with a React based front-end. We designed the site to emphasize simplicity and to showcase the surplus/scarcity percentages the best. The whole web app is hosted on Microsoft Azure with a Continuous Deployment workflow through GitHub Actions.   
 We modded a guitar amp, replacing the built-in speaker with a speaker with a plastic cap, which allowed us to attach the driving rod to the center of the speaker. The driving rod is attached to the plate at the center. In order to keep the rod upright, we built a wooden support base which is mounted to the speaker. The simulation uses a given formula for representing the nodal lines of a Chladni plate derived from the 2D wave equation. Using a plotting library in Python we graphed the equation, displaying the Chladni pattern for the given parameters. 
 On Python (numpy) we did all the computations which involved finding the eigenstates of a Hamiltonian.On JS (p5) we coded UI on the browser.With Socket.io we got the JS script and Python script to communicate. 
 We used a Python module called PDFMiner to analyze the content of the resume and then used regular expression to formulate and identify the name of applicants. When the name has been identified, we search and replace all names in the resume with mosaics and create a new PDF document that need to be graded by HR. This name vaguer is our recruiting system’s core function. We also designed the system prototype with Figma, detailing the interface’s look, reviewing interface, and how the recruiter can select the final applicants after they’ve been graded. 
 We used the MongoDB to create a database based on UT Austin's “See my waitlist” webpage. This database contains a unique number, the course name, current waitlist number, and last updated time for different classes. Each time a student using our extension visits “See my waitlists”, the chrome extension identifies the class information on the page and updates our database. We accomplish this by building an API in MongoDB Stitch to let the extension POST to the database without being able to access all fields. Additionally, we used a second API with HTTP GET to allow our web app to pull the data and present it. The web app was made using React, HTML, and JS, and was hosted on bluehost.  
 We used Java and JavaFX for the simulation. The software development cycle was straight-forward. The workload was split between research, graphic design, base classes and GUI. 
 We used openai gym to create a new module. In addition we also used pyglet to render the current state of the environment.  
  
  
  
 We used CSS, Html, and Javascript to create the website. 
 We built this program in react and using a neo4j and a mongodb database on Microsoft azure. 
 ReactMarqeta API*Vim 
 We utilized various technologies for our application to work. The application utilizes OCR in order to convert the image of the license plate into computer friendly text, we also used APIs, to get the vin number from the license plate we utilize API, also to get all the information about the car from the vin we use the NHTSA api. Other technologies we used is github pages to host, and languages utilized are Swift, ARKit, HTML, CSS, JS. 
 Unity. 
 We mainly used vanilla Java, along with the help of JavaFX for visualization/GUI.We were able to effectively model Quantum States and Operators using matrices and kronecker products. 
  
 feels was built as a progressive web application, which allows it to provide a rich experience on all platforms. We used node.js and sqlite to build the backend and React for the frontend. Our sentiment analysis was implemented using Azure's text analysis API. Simple metrics such as heartrate are currently used to evaluate a user's mood, but we also plan to examine accelerometer data to detect shakiness. 
 We created a signup application for the volunteer/professionals and their details are stored in the database. They can sign in and view the people in distress. We created a streaming filter for Twitter with the #HACKPSUHELPLINE tag and the location is shared with the rescuers on a google map. The rescuers can then choose the nearest person to help and the application will provide the shortest path to their desired location. 
 The whole code started from the Laplace tide equations, which we discretized and evolved with python. 
 Used the matplotlib.pyplot and tkinter modules to visual trajectories. Euler-Cromer methods in conjunction with Newton's laws were used to alliteratively compute trajectories.  
 First, collectively, we made a mockup of the app to determine how the app would function and the features it would include. From there, we split into teams for the front-end, back-end, and mockup. The back-end team found and incorporated different APIs to implement the numerous features our app provides. The front-end team started front-end programming using various bootstraps. Starting from the home page, they planned and implemented each section. From there, they created linked pages such as the login page, the registration page, and the driver/rider profiles. Finally, we linked the backend to the frontend and registered a domain name to host our website.  
 We used the Google Maps Javascript API to identify the locations of road blocks for diabled people such as stairs and places under construction and store the locations as Jason objects, which will turn into arrays after parsing.  We then use Google Map Geocoding API and polyline function for displaying all those stored location to disabled user so that they will know which path to use. All the data are stored in local browser storage. 
 To solve the Schrödinger equation, we first used the method of lines to turn the partial differential equation into a system of ordinary differential equations by discretizing the spacial dimension.Then, we used Euler integration to solve the resulting system of ordinary differential equations.We then solved the equation numerically using js (for the browser) and python (for data generation).We then plotted each time step using p5.js, including a useful UI to play around with different potentials and initial wave functions. 
  
 Our stack was Android Studio, Java/Kotlin, and our backend was developed with Firebase, and Flask. We were able to use Heroku to deploy much of our backend, Firebase as our server/storage, Android Studio as our mobile application, and Flask as our web application.  
 We created our own image based dataset, through online resources and our own data collection. Our image data was sorted into 6 categories — potholes, graffiti, broken sidewalks, signs covered by trees, broken traffic signs, or littering — after we uploaded our dataset to the Google Cloud Console.  Through the use of Google’s Cloud AutoML Machine Learning system, we developed a deep learning Neural Network Image Classifier that identified images and placed them into one of our five categories with an average precision of 0.981.  Additionally, the frequency of false positives generated by our model was 5.00%, and the frequency of false negatives was 9.52%. Prior to using Google’s Cloud AutoML, we experimented with Apple’s CreateML to develop models for our image classification system.  While CreateML served as a useful placeholder, we experienced difficulties in achieving a high enough level of image identification precision with this software.  After we managed to get AutoML working, we replaced CreateML with AutoML and our app’s functionality increased significantly.After creating a working machine learning model, we focused on the app development aspect of our project.  We originally attempt to develop both Android and Ios versions of our app, but we eventually decided to just focus on Ios.  We intend to develop an Android app after the completion of HackDuke, since we intend to eventually place our product on the app store.  We developed a basic UI to allow the user to change the category of the maintenance request (for the rare cases when AutoML fails to classify the incident correctly).  Following that, the user is asked 2-3 additional questions about the event.  These questions come from the City of Durham website. Lastly, the information about the incident is emailed to the correct person within the City of Durham maintenance department, along with the address of the incident and the photo of it.ChallengesInitially, we used Apple’s Create ML app to develop a machine learning model that would predict the type of environmental issue we categorized based on Durham One Call’s form.  CreateML was unable to create a model for us that was precise enough for our needs, and often misidentified maintenance incidents even after extensive training.  We solved this challenge through the use of Google’s AutoML system instead. We were really excited that our neural network was fully functioning after training for two hours. However, we believe that if our data set had provided a more extensive and accurate depiction of all of our categories, we could develop a significantly more accurate Neural Network Image Classifier.  Unfortunately, due to the time constraints and lack of a data set which we were faced with, we were unable to achieve a more accurate machine learning program within 24 hours.Another challenge we faced was developing the app from Android, since, none of us had any Android App Development experience.  We were unable to overcome this challenge in the allotted time, and so we instead decided to focus solely on developing an IOS app.What's nextMoving forward, we would like to get QuickReport to work with the Durham One Call website. Furthermore, we plan on developing our app to address environmental and maintenance issues in other locations in North Carolina (e.g. Raleigh) as well as around the world. For instance, Raleigh also has a convoluted online request form for resolving street issues that must be filled out by a user. QuickReport would be a viable, efficient approach towards addressing those problems. Built WithautomlcoremlhtmlswiftuikitxcodeTry it outgithub.com      Submitted to    HackDuke: Code for Good 2019Winner                Best Energy Hack              Winner                Novice Prize                  Created by  ramisbahi Sbahichriisyangpnwnigel 
 The frontend was built in HTML/CSS using JQuery and bulma. Our first goal was to create a (rudimentary) connection between the Client-side and Server side applications. This was key because we needed to make sure our backend and frontend were incrementally growing together, providing for a much more organic and hassle-free development. From there, we divided and conquered, getting to work on the Client-side design, scaling quickly yet carefully, allowing ourselves time at the end to really put the finishing touches on the appearance. We set up the firebase database and got to work developing server-side HTTP request handling to classify images to objects, identify objects as recyclable/non-recyclable, and returning clean, easy to use responses to the client. 
 We built it using very basic HTML skills that we acquired here. Through a long learning curve process. 
 We used python2 and python3 with getOldTweets to pull the Twitter post information, Google Sentiment Analysis API to generate sentiment information.For the data analysis, Jupyter Notebook was used in conjunction with Java Swing to help visualize these plot in an clean GUI fashion. 
 We used Spotify's API to build the datasets as well as a neural network of 1 hidden layer. 
 Treeler was built by our team using an array of technologies. We used React for the front end, GraphQL for querying, Microsoft Custom Vision for Machine Learning and Visual Recognition. We also used git for version control and sharing our code. 
 We built the client-side of Flare using React Native (Javascript) and the server with Python Flask. Our deep learning model was constructed using Tensorflow on a convolutional neural network - the validation accuracy was approximately 72%. 
  
  
 We created all 3d models in Blender and programmed & assembled everything in Unreal Engine 4 
 We used a raspberry pi and turned it into a network using DNS mask and postapd. Android Studio to create the app that people in need will use to send information to the drone. Flask API to return the information in json format. Angular front-end to have a ui to show the responders the location and severity of the emergency. And lots of sockets to send information from device to device.  
  
 Using a raspberry pi and ultrasonic-sensor we can detect the change in resistance of a rubber band to get the wind speed. We send the data to Google Cloud to analyze if there is a possibility for a tornado. Then we send that back to the user in a visual form so they can understand it better and be better aware. It can also call emergency responder if need be, by the user's request.  It can detect the threshold we have set in the code to see if you are in danger of a tornado. 
 We used the American Airlines API to find flight data and the Dark Sky API to get hourly weather forecasts over the course of a flight. We used Python Flask for both the frontend and the backend. 
 Sensors and data collection are done with Arduino, and the visualization of the heat flow is made with python.We had to use libraries such as nRF24 for the Arduino transmitters and PyQt5 for the visualization.Our two Arduino boards' setup are rather simple and include a temperature sensor and a transmitter/receiver on each. The one outside acts as a transmitter, and the one inside receives the outside temperature while measuring temperature inside. This one is plugged to a computer, to which it sends the data.For the visualization, LaPlace’s temperature formula was used to determine what was the temperature gradient caused by conduction, then Navier-Stokes' equation was used with certain specific boundary conditions that apply to double-pane windows, as a function time. After that, we used discretisation and iterations to see what happens if the window is taken from space and had the temperatures applied, and calculated a whole bunch of iterations (IE a second or two) to give a pretty decent picture of what was going on with convection after the disturbance balanced itself out. 
 We used Swift to build the app and Azure to visualize data, eventually relying on CoreML and CreateML to create models. 
 We built the backend of the site first then slowly added the HTML pages in. We then built a home page and expanded to include the side pages. 
 We used the Google Maps API to determine the shortest route between the start and end locations. Using the AccuWeather API, we interpret real-time weather conditions on any locations the user requests. Based on these conditions, the basic Python AI we constructed links the weather to appropriate gear. 
 Using the Unity environment, AR Foundation, ARKit, and XCode together to develop the environment and functionality for the app between two devices. 
 We initially tried building the website using HTML/CSS but realized very quickly that these languages are not powerful enough to meet our demands for a dynamic website. We switched over to using JS and the React library, and utilized the Semantic UI system to make for cleaner code and enhanced functions. We created a variety of components and just imported all those files into the app.js file so that the code was more readable.  
 I built my web app using Javascript, and HTML. I also utilized Google's Map API, as well as their Places Library and Directions Library. 
 The CC2541 is a power-optimized true system-on-chip (SoC) solution for both Bluetooth low energy and proprietary 2.4-GHz applications. It enables robust network nodes to be built with low total bill-of-material costs. The CC2541 combines the excellent performance of a leading RF transceiver with an industry-standard enhanced 8051 MCU, in-system programmable flash memory, 8-KB RAM, and many other powerful supporting features and peripherals. We also experimented with other microcontrollers, but in order to keep it small enough to not be bothersome to the child and still big enough not to be a choking hazard we stuck with the CC2541. Once attached, the CC2541 begins to collect real time data and passes it along into our application. The application side was developed within Evothings Studios and Workbench. This allowed us to create an application capable to run in both Android and Apple Operating Systems using JavaScript and HTML. Libraries such as Easyble.js and Tisensortag.js played an instrumental role in our final product. 
 We use a flask backend to implement the game logic. We use a javascript frontend (no frameworks) to paint an html canvas based on the game state and to display the scores of players. We communicate between the backend and frontend using websockets using socket.io.We are currently in the process of hosting on AWS EC2 with Apache (its a struggle) 
  
 Our backend is built with a REST API in Flask connected to a MongoDB database in Python. One of the advantages of using a non-relational database like MongoDB is that we can represent our database like an actual tree. We calculate the tree structure using WordNet, a dataset that maps the semantic field relationships (supersets and subsets of words) between words in a tree-like structure, binarizing the tree. We implemented a custom scheduler to navigate the tree and find the next best images a user should classify. We also host our static images on-server. Currently, our app is configured to use a subset of ImageNet called TinyImageNet. However, this is easily configurable. This is all deployed on Google Cloud.Our frontend is a React Native app compiled with the Expo toolchain, written in Javascript. The app sends a GET request to our API to get new image URLs, IDs and categories to classify from our scheduler, and sends a POST request back for each image classified with an image id and a boolean. The database is updated in real-time and can support simultaneous users as well as users exiting the app and never classifying images. 
 I programmed the project in c++, using SDL for the graphics. 
 We built this using Unity, Photoshop, Pixilart, and a healthy dose of Google Docs.  
 Our application is hosted on an Azure Virtual Machine, built with Angular and Python it utilizes the Tweepy wrapper for Twitter's API to give us a way to reach out to the world.  
 We used the Adafruit PN532 RFID reader breakout board with an Arduino to read the unique UID code associated with a RFID tag. We transfer the ID# over serial to a python script that identifies and opens the corresponding medical record. 
 Bulit with a Django Backend using Watson and Twilio for call functionality and automated conversation 
  
 We built this tool using rasberry-pi-3 for the hardware of the screen, and nodeJs and CSS for the full-stack development of software. 
  
  
 At first, we attempted to create an Alexa skill and implement the Flutter project with Firebase, however, after many issues we decided to go server-less.Built Withamazon-alexaandroidfirebaseflutteriosnfcTry it outgithub.com      Submitted to    HackTX 2019    Created by  I worked on the firebase and flutter mobile applicationYonathan Zetune"You are not the Techlead" -TechleadI was developing an alexa skill to alert users to actual disasters.Kushal ShahWorked on Alexa skill and integrationszabir031200 
  
 The front end was built in HTML/CSS and Javascript. The backend and database used python and MongoDB. 
  
 We used Python, HTML, and Javascript to make this project. The project is built on a web application, but it uses Python in order to analyze the audio file and give critiques.  
  
 For the front end, we are using a combination of HTML/CSS/Javascript to interact with the user and provide the necessary functions for the interview practice.In the backend, our Javascript functions work with the Azure Speech SDK to provide speech-to-text functionality, and the Azure Sentiment Analysis API to review word-choice and sentence positivity. We are hosting our website with Microsoft's Azure web services as well. 
  
 Our app runs on the Flask web app framework. We used the Google Places API and the Distance Matrix API from Google Cloud to calculate the distance and time needed to get from your location to a hospital. We used MongoDB Atlas to securely store and receive data for each hospital. R was used for processing data before using it with MongoDB Atlas. Python was the language we used to run Flask, send queries to the database in MongoDB Atlas, and send information for use on the web app. HTML and CSS were used to design the web app. 
 Using different API's, GO, javascript, materialU and WebSocket to create the project 
 We use a flask web app for the front-end to retrieve a url from the user. We then extract the content (e.g. text and images) and run those through two separate neural networks, the first designed to determine the probability of text being ai generated, and the second designed to classify an image as a deepfake or not. For the text analysis network, we are essentially reversing a gpt-2 text generation neural network and having it simulate an output based on the context from the website the user provided. We then run a statistical analysis on the probability of the website's text being generated by a similar network.For the deepfake detection network, we use an xception classifier network trained on Google's FaceForensics++ dataset.Once our data is retrieved we use flask's jinja2 template library to dynamically create a results page, then serve that to the user.Everything is running on a ubuntu vm instance on google cloud. 
  
 We get Twitch chat data with the twitch-python API, and then analyze it with GCP's NLP API as well as our own statistical algorithm in Python. We host the backend server with Python + Flask. The frontend is in HTML + Javascript, connected to backend with Socket IO. 
 The web application is composed of the frontend, which is created through HTML, CSS, and Bootstrap. The middleware we used was React.js, and this allowed us to both dynamically change the user interace, as well as connect to the MongoDB in the backend. It also allowed us to create a socket connection between the driver and the student, providing feedback to both parties to ensure that the request from the student reached a driver. 
 Lots and lots of web scraping, data cleaning, web development, and cloud engineering. 
 We built a smart contract & UI that:tracks a list of whitelisted members addresses mints & burns Coven Shares(CVN) based on deposit & current supplyDeposits and withdraws Coven funds into an interest earning protocol (AAVE)let's members borrow from the Coven at an interest rate 
 I built this application with the Twilio and Wegmans API in python and the flask framework. 
 We have a raspberry pi that has a bunch of sensors in the that pushes data to a virtual Linux Machine in gcp. The Linux machine than take the data and predict the amount of people that would need help in that area, and contacts people if the weather is bad. Then there is a front end that creates a way for people to sign up for the service.  
  
 We performed web scrapping using Beautiful Soup on Yahoo Finance to get live data for stocks. We used React, Node.js, and flask to create a web app that takes in financial stock data as a JSON, and uses advanced algorithms and powerful APIs to present the data in a user friendly, and extremely useful manner.  
 We used the Google Maps api through React Native to display a map on the screen and track where different users are. This api allows us to also create polygons, so whenever a user moves on the map, their area gets filled in with rectangles. Each search party has its own "room," and this room has a code that can be distributed to all the members of the search party. 
 I used XCode and the iOS language Swift to build the app and Firebase as the back-end. 
  
 Confidant leverages the most cutting-edge and advanced technologies. Our frontend uses React.js + Next.js to render our website. Our eye-detection technology uses a library called WebGazer that runs on the client side. Our backend API is built using Flask, hosted on Microsoft Azure Virtual Machines (VMs). We extensively use Microsoft's Speech Services (both text-to-speech and speech-to-text) to process the simulated interaction between the human and the Confidant interface. Finally, the Confidant companion is powered by the VOCA speech to animation engine. 
  
 We used IBM Watson Machine Learning primarily. First, we used a Python script to pull images from Google Images to train the model. Then, once we trained the model with the Machine Learning service from IBM, we used a combination of IBM Cloud, Twilio, and Slack to handle the output from the model (a JSON array) and turn it into I/O with the user via SMS texting. 
  
 Using Flask microframework (and Jinja2 templates), Google Cloud Vision API, and Google Translate API 
  
 We built it with a Node.js/Express Backend and a vanilla javascript frontend. 
 Our first task was to learn our way around react and get the default app deployed on to Azure. Then we started to divide up tasks by having Shelby work on the back end, Amy work on the design, Sam to work on the front end implementation, and Anne to design the logo. From then we all knew our tasks and pushed on to our own branches. Sam and Amy worked closely together for the overall flow of the system and Shelby made sure we were able to save all of the data on the MongoDB. 
 We first asked the user for a starting position and a destination. Then, we used the google Directions API and google Distance Matrix API to get the location and time information from the starting position to the destination. Next, we used the accuweather API to get the weather information on the route. Finally, we combined all the data and calculate the driveability. 
 Using IBM Watson studio's Object Detection toolbox, we were able to label a mushroom's features such as its color, shape, and surface. Then, we feed these data into the AutoAI model trained using a Kaggle dataset in order to determine if it is edible or not. 
 Python was chosen as the main language used when building Warning Shot. For the machine learning algorithms the Tensorflow library was utilized. To create a signup form to send SMS messages HTML and CSS were used to create a front end website that was connected to Django and Twilio as backend code.  
  
 We used Flutter as our main framework to facilitate cross-platform usage and high-level prototyping. We used our Google Cloud Credit to enable Google-Maps-API services with Firebase to create real-time geolocation maps and make queries for nearby hospitals and shelters. We pulled our natural disaster prediction sets from NASA, Firecast, and FEMA. 
 We utilized  SmartCar to gain insight into the car's model info, battery, and location services. We then designed our own API that leveraged the SmartCar API as well as another external API that finds the nearest EV charging station based on location. The Chargeo API exports a google maps route based on the charge level and location of the vehicle: when the vehicle's charge falls below a certain threshold, we update the google maps route by adding a waypoint to the nearest gas station. This google maps route is then pushed to notifications, where Chargeo users can click and add the EV charging station waypoint without any work on their part. 
  
  
 Messaging service - Go & Vue.jsCloud server - GoIBM Cloud VPS - Go & DockerData Visualization - Python with OpenCV, NumPy and Matplotlib) 
  
 Hosted on Flask and Guicorn at the backend. Frontend uses Angular. ML library from Tensorflow, Sklearn and python stats_models are used. 
  
 By downloading Uipath Studio and learn the demo providing online, we roughly know how to build it.First, we design the time schedule for the person we want to call; it divides 24 hours into breakfast time, lunch time, dinner time, free time, bed time, etc. Time schedules might be different based on weekdays or weekend. For example, brunch time in the weekend is a few hours later than in weekdays (because we want to make sure that we don't want to interrupt them from sleeping during the weekend). Then, we start new project in Uipath, and divide it into 4 processes: input dialog, retrieve time data, analyze time data, and message box. For input dialog, we basically read the city where the person you want to call, and store it as a variable city.For retrieving data, we enable Microsoft edge extension, and build an open browser UI, set it at www.bing.com. and then take a screenshot of the input. We put it into type in and hotkey UI; And then we get the screenshot of corresponding time and date to retrieve data and set two variables time and C_date. For analyzing data, we build a flow chart. First we check if it's weekend and then divide the time into AM and PM and check what they're doing at that time and whether to call or not. we put the result into variable suggest. For weekdays, we have different time schedules; we check what they're doing at that time and give the suggestions.Finally, we use a message box to display all the variables and let the clients know what the person is doing at that time and whether they should call or not.  
 Using Twilio and Java 
 built using the outsystems framework 
  
  
 ArduinoBuilt Witharduino      Submitted to    VandyHacks VIWinner                Most Outside the Box Hardware Hack, Presented by BNY Mellon                  Created by  Jacob FineLliu11 
  
  
  
 Android studio and java 
 Utilizing MySql and resources provided to us by the Hackathon organization, we took the first steps in creating a landing page a login database and a course database. 
 Writing a variational autoencoder in keras and train it using apple's font library. Then building a pygame front end to demonstrate it 
  
 We used android studio 
 Using scratch and self-made audio 
 We used textpad, JFrame, and Java. 
 R+Shiny 
  
 We built it on cardboard, and wrote the code in C and python 
 We built it using tinkter and pygame 
 We used Android Studio, primarily with Kotlin, to develop the structures to store each character as they are input, then compare new inputs to saved templates. 
 We built it with a foundation of HTML and CSS for the design and implementation of the website (front-end). We then used Javascript and Json as the actual back-end programming of our website. 
 html css javascript 
 