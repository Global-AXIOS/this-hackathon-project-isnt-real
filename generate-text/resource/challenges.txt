Being the only developer for such a large project, I had to manage my time efficiently. From designing to developing to testing. I did it all. Time management was the biggest challenge that I faced.    How can we inculcate motivation in citizens to follow parking discipline through this solutionChoosing the most appropriate way among various ways in which parking in/out and safety can be ensured at a parking siteResolving a unique slot number allotment for requested booking when bookings can be done online as well as on the spot  We ran into several major challenges in taking the functionality of our add-on from the cloud to fully onpremises. Among these were challenges enabling real-time collaboration, using an embedded rich viewer within Confluence to create slide shows, and taking our 100% cloud-based functionality into a format that can be used entirely behind a firewall.What's next for Lucidchart OnPrem for Confluence ServerWe are currently taking the lessons we learned from building Lucidchart OnPrem and applying them to our web-based Lucidchart add-on. These changes will make both products faster, more reliable, and more valuable to our end users. Aside from the business push, we also plan to continue iterating and improving upon the add-on based on customer feedback. For instance, we will be releasing a new version with improved revision history controls shortly.SummaryJust like taking a Server-based add-on to the Cloud, taking Lucidchart to the Server was challenging and required many large and small changes to our existing product to deliver an excellent solution for our customers. We hope you'll enjoy testing Lucidchart OnPrem. Feel free to give us your thoughts as we are always working to make our products better, faster, and smarter. Thanks!Built Withatlassianauiconfluencegoogle-closure-compilerjavajavascriptlessTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist: Add-on HackathonWinner                Popular choice add-on                  Created by  Blaine FarrDmitry PashkevichI am a software engineer at Lucid Software where I work on Lucidchart and its integrations with Atlassian products.Rob ChristiansenOwen Krafft  We did not run into particular challenges but we enjoyed the process very much and learned a ton about the inner workings of Atlassian products and Jira in particular.  implement the most popular API in ours project.  Being the only developer for such a large project, I had to manage my time efficiently. From designing to developing to testing. I did it all. Time management was the biggest challenge that I faced.    How can we inculcate motivation in citizens to follow parking discipline through this solutionChoosing the most appropriate way among various ways in which parking in/out and safety can be ensured at a parking siteResolving a unique slot number allotment for requested booking when bookings can be done online as well as on the spot  We ran into several major challenges in taking the functionality of our add-on from the cloud to fully onpremises. Among these were challenges enabling real-time collaboration, using an embedded rich viewer within Confluence to create slide shows, and taking our 100% cloud-based functionality into a format that can be used entirely behind a firewall.What's next for Lucidchart OnPrem for Confluence ServerWe are currently taking the lessons we learned from building Lucidchart OnPrem and applying them to our web-based Lucidchart add-on. These changes will make both products faster, more reliable, and more valuable to our end users. Aside from the business push, we also plan to continue iterating and improving upon the add-on based on customer feedback. For instance, we will be releasing a new version with improved revision history controls shortly.SummaryJust like taking a Server-based add-on to the Cloud, taking Lucidchart to the Server was challenging and required many large and small changes to our existing product to deliver an excellent solution for our customers. We hope you'll enjoy testing Lucidchart OnPrem. Feel free to give us your thoughts as we are always working to make our products better, faster, and smarter. Thanks!Built Withatlassianauiconfluencegoogle-closure-compilerjavajavascriptlessTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist: Add-on HackathonWinner                Popular choice add-on                  Created by  Blaine FarrDmitry PashkevichI am a software engineer at Lucid Software where I work on Lucidchart and its integrations with Atlassian products.Rob ChristiansenOwen Krafft  We did not run into particular challenges but we enjoyed the process very much and learned a ton about the inner workings of Atlassian products and Jira in particular.  implement the most popular API in ours project.  The initial challenge was to get 6-axis accelerometer and gyroscope data over BLE to Raspberry Pi 3, Since there were limitations on the size of bytes transferred over BLE we had to break accelerometer and gyroscope data and transmit both separately with some milliseconds difference. Next was to get the data captured into a visual 3D graph this was done using the Plotly graph, keeping the refresh rate to 5 seconds. 3D graph won't be visible in selected old version devices which don't support WebGL 2.0, As Plotly uses WebGL 2.0 in 3D charts.  The collection of the dataset. Was able to get MRI and CT scan for brain abnormality but was only able to get a structured dataset for breast cancer, so various other image-based models wasn't tried.    Scraping the data    So far, so good. The hardest part was selecting nice icons :)  As we only found out about the project one month and a half ago, developing ChedliTaxi was not a straightforward job. As you may know, proper Android development takes time especially when dealing with a Real-Time communication application (synchronization of tasks...).In fact, we have received many returns with nonexistent streets and wrong directions that made the Exception Handling process very tiring.     Design and development of the demo model of the linear actuator for Thrust Vector Controlling as it involves complex mechanical assembly and linkages      Certain services and its integration in SWIFT force me for technical deep dive.  Web scraping Facebook was one of the earliest challenges we faced. Most DOM elements in Facebook have div ids that constantly change, making them difficult to keep track of. Another challenge was building an AI that knows the difference between a fact and an opinion so that we do not flag opinions as false, since only facts can be false. Lastly, integrating all these different services, in different languages together using a single web server was a huge challenge.  I didn't have the camera so it was tough as I had to do assumptions on things like how the camera perceives data still the video tutorial played a vital role on creating the app.We intent to modify the app as we get the camera with Open CV modules      The biggest challenge faced was shortage of time for such a grand project. But all challenges were accepted wholeheartedly. The main challenge was to implement it in a seamless way so that the gamers experience is kept abstracted from the Kin Mechanics and feel like every other game.What we learnedImplementing Kin itself was a huge learning curve for our skills. It gives us a chance to experience making a server for the game and learned how the client side communicates with the created server and the created server authenticates the signatures provided by the client transactions and then sends back and forth to the Blockchain server.Build withThe project is built using Unity Engine which provides all the development tools for implementing Kin. Moreover Kin, as stated before, is officially partnering with Unity Technologies which gives Unity Developers a headstart, and the flawless active communities and support gave the opportunity to everyone to try the Kin provoking milestones for themselves.More informationKins foundation is playing a key role in delivering crypto currency to users with the help of communities like Unity Technologies which is collaborating with Kin. This process has become achievable. Kin lets you reward users in ways that matter to them - improving long-term engagement and retention. For players, it enables earning and spending of kin in games, participation in challenges like competing high-scores and keeping track of Kin balances.Built WithandroidapiblockchainkinunityTry it outplay.google.comdrive.google.com      Submitted to    Kin Crypto ChallengeWinner                Honorable Mentions                   Created by  Steve ThijssenArjen van GaalProduct Developer  The main areas of innovation were related to the native synchronization with JIRA objects. A lot of the configuration is created on the fly - workflows, issue types, etc.      Google Map integration and searching radius of 1 km and to provide best and safe route for cyclist and pedestrian.Collection of such data which can fulfill Fords expectation to promote safety tips was a bigger challenge      It was a difficult process overall. It was hard to figure out the perfect display for the postings on the home page. After 18 hours of hardwork I perfected that and then figured out how the locationManager is going to work so it can get the current location of the user. Another challenge I faced afterwards was calculating the distance between the job posting and the current user location.  We struggled initially when we thought after developing an initial part of the application that it isn't running. We solved it and realised that we haven't given sudoers permission to Apache and that's the reason why a few functionalities were initially not working.    Time is so limited, task is so tough. We just can do our best during a such short time.      Google Chrome extensions have lots of restrictions, particularly on what you can do in the UIUsing the native client and encoding video in C++ is non-trivialIt's so easy to clutter and complicate a recording UI with tons of settingsCreating a good interface that works with every JIRA installation is time consuming    Redirection of view page from default selection to another view with selection options to the final report view page instead of directly going to report page from default view page.  We became aware of the hackathon a few weeks before the deadline and naturally getting a working model built was a challenge. Right from procuring beacons and other electronics and putting it all together as a working model was challenging.We further ran into issues with interference from WiFi signals and other remote control based equipment which required adjustments in the receivers to siphon out the noise. While beacons are reasonably reliable as locating devices with significant distance between two beacons, in a scaled down model this was a challenge. The beacons had to be re-calibrated along with the receivers to sense two beacons a few centimeters apart.One significant challenge was to build a single script that would run on each of the Raspberry Pi's to make it a BLE receiver, communicate with Azure over MQTT and control other peripherals (boom barrier, weighing scale, loading bay) over a local MQTT network. We had to ensure that the Pi would be robust enough to handle the multiple data transactions, leaving no room for bugs or events that render the receiver in a locked down state.  Given the time constrains, and some tricks with getting the APIs to talk together, some of the data we wanted to include was harder to connect to than expected.   We think no HDMI input/output feature enabled on ARTIK 10 until the deadline of this challenge was the bigger challenge for us. This led to us having some problem creating and showing dashboard on ARTIK.While the dashboard was running on the ARTIK itself, but we have accessed it on the network on another machine. If HDMI input/output feature would have been there, we would have ran it locally. But we overcame this challenge by developing our own dashboard from scratch and running it on ARTIK server.Another challenge we faced is with Bluetooth connectivity. Initially we decided to send water consumed data from water flow meters to the ARTIK using ARTIKs bluetooth. But due to frequent disconnections from the bluetooth we primarily relied on WiFi.We hope that ARTIK Developer community evolves more quickly and more developers start developing their solutions.  Lack of documentation. Examples are pretty scattered. Although we wanted also to publish this as a Jira add-on, we just couldn't do that quickly.       Little knowledge on Hsuehshan Tunnel and little resources that was available and point to start.Here I had  to study what can be done with the data and hence used this way.  The first challenge we encountered was dealing with limited route options to and from Yilan. The long periods of heavy congestion during holidays are also more significant than what is normally seen in many cities. These limitations actually helped us by forcing us to think outside the box and add a new dimension to our travel-shifting approach. In addition to luring drivers to change their departure times and take new routes, we now also present our users with a list of top destinations that are close-by. In addition to saving them time, this approach goes well with our incentives and rewards program, as were able to offer our users a discount to these alternate destinations in exchange for visiting them that day.  Circuitry IssuesAndroid Studio LogicFinding the right circuit boardCreating a mount for the circuit LOTS OF HARDWARE ISSUES  Authenticating a user offline and SMS delivery issues in some locations.      Identifying what tasks should be automated for the bot to do, and which should be done by a human.Ways to design the interface so the refugee does not feel targeted by the government.We contacted experts of the challenges faced in different countries supporting immigrants.  Signing up for Azure account and getting it verifiedExporting the trained model from Azure Custom Vision to Android studioUnderstanding the original sample code after importing models from Azure custom vision.Developing and customizing my Android application  Integrating KIN into the app. But it was not very tough.  When we started developing Touch Wizards , we had something in mind for it . However,seeing the market , we tried to incorporate some features that we thought will help our game pick up the market . Soon , we realized that this resulted in an overloaded game with no clear goal.We had to drop out some idea and think carefully about the core features of this game  Not clear requirements or roadmap of business and platform.People capabilitiesDeadline short  Greatest Challenge : I had to learn about how to use ASK, how to use lambda and dynamodb, how to connect it to the Alexa device, etc.Alexa's DATE type slot always takes the future dates, for example if I say 5th April, it considers it as  5th April,2018 as the date has already passed by. I noticed this a bit later, and then manipulated it at back end.Experimenting with Google Analytics API.    Coming from traditional type servers where we run and manage the entire system ourselves moving things over to AWS services was a bit of a challenge. We found ourselves constantly changing decisions on what AWS technologies to use and which ones would be the most effective.  It was a challenge to find the right program to analyze the pictures. Even after finding Azure, it was a problem to have the right words printed in the right sequence.  Almost every step of the way was a challenge for us due to the new environment we were coding in. One of the biggest challenges we ran into was being able to convert a user-input address into a co-ordinate which can be stored in the server and displayed on the map for all users to see.  The biggest challenge we ran into was a compatibility issue when integrating both, object detection and speech recognition, software together.Android Studio was only working on two laptop, we used the pair programming approach to efficiently used our resources.Accomplishments that we are proud ofWhat seemed to be an impossible project to do within 36 hours, we managed to divide and conquer by efficient team work(work flow: different phases, list of task by priorities, etc).Working with Android Studio was a pain, but we managed to pull through get the app DONE AND WORKING!!!!!!!!!!!!What we learnedHow to handle threading in Android Studio using Async Task, as well as implementing communication between foreground and background services. Additionally, we learned the importance of dynamic team work and collaborating using agile development methodologies. What's next for Lo-KateThe ultimate goal is to integrate the software into a pair of google glass with which the person can communicate with. It would be able to not only find objects, but as well as find buildings/landmarks and give directions upon request from the user.Hopefully, with further development this software could have a meaningful impact on the lives of those who face challenges greater than our own. Built Withandroid-speech-to-textandroid-studioandroid-text-to-speechjavaspeech-to-text-for-androidtensor-flowtensorflowTry it outgithub.com      Submitted to    McGill CodeJam 2018Winner                Category Second Runner-Up                  Created by  Worked on implementation of multi-threads (tensorflow oject detection and speech recognition threads). Worked on debugging as well.Mia Yuxin ZhouI worked on the android application and making sure that the voice recognition and text to speech parts integrate well with the tensorflow light.Elias Al HomsiSoftware Engineer McGill UniversityDrawing and filtering of the detected objects. Along with determining the relative position of the target objectCarl El KhouryImplementation of the Tensor-Flow AI, the logic of detecting object position relative to camera and creation of the UI in Android Studio.Max Brodeur-Urbas3rd Year Mcgill Software Engineering Student      Getting the proper versions of Cudnn to play nicely with TensorFlow. Getting data-sets of images of people who were high.  A boat load of issues such as how best to create interactivity in the already interactive 360 scene we were providing with the extension. It was exciting though as we enjoyed solving all of the issues.     ** We needed to find partner organization ** to host our website and hopefully the government will work with us to spread the app to the people who want to ask question about reproductive/sexual health anonymously.What we learnedThis is a good idea that we are using SMS features and chat-bots to serve marginalized people that do not have access to smart phones and internet especially those from the provinces. NLP and Chat-bots are difficult to backend as it requires an in depth understanding of different kinds of language models and computations.the NetDoc+ team will continue to work hard to help contribute in solving the HIV crisis in the country and hopefully in the world.https://docs.google.com/presentation/d/10MYWzQjRy8mjcVRLDVN1fZNXoR6ShJYZzhkHvQwQi_c/edit?usp=sharingBuilt Withbootstraphtmljqueryw3Try it outgithub.com      Submitted to    #codeathon 2017: Technopreneurship for Gender EqualityWinner                University: Overall Winning Team (Global)                  Created by  I worked as the project manager of the app. I have a passionate vision towards the improvement of sexual health services in the philippines and eliminate the stigma of AIDS and HIVNandario NandeyoJason DeichmannTommy Botabarakayletiu  My #1 challenge in making the app was to intergrate material design and polish all the app's features in a short period of time!What I learnedI really learnt a lot about Android Studio and working with Java and Parse.What's next for HyperHyper is currently tackling a small part, connecting buyers and sellers, of a potentially huge craigslist. In the short term, I want to expand the app to incorporate business scenarios. In the future, I plan to incorporate machine learning to recommend users what they might like to sell or what they might like to buy. Built Withandroid-studiojavaparsesketchTry it outgoo.gl      Submitted to    Make School's Student App Competition 2017Winner                10 Upvote Prize                  Created by  Esakkivel Esakkiraja    Using the google API for geofencing.Building a decent user experience.    Trying to differentiate each case where the user wants to do a specific thing. Of course, the limit is the sky, or the galaxy... Also, it was not easy to set up the OCR to recognize text images and calculate the answer right away.  Inference in the neural network was too slow for our student-budget t2.micro EC2 instance, so we had to add the Microsoft Cognitive Services stage to have a faster 'happy path' when the item can be captioned appropriately by that service.  As a sole developer, I had to learn to prioritize specific tasks over many, many other ones. In addition, making sure I had good data was difficult - there was much trial and error in this process!In addition, the native Siri API only offers a 1 minute recording time limit - I had to engineer a solution that extends this limit without going against Apple's Siri requirements. I'm proud to say that PlainDoc utilizes an unlimited Siri recording time thanks to the solution I had engineered!  The first challenge we ran into was with the register mapping for MODBUS communication. We soon realized that the documentation provided by most of the Genset controllers in India were either inaccurate or out of date. A recursive method was followed to identify the correct mapping of the parameters.Gensets are often placed in the basement. GPRS connection does not always work as expected. As such elaborate adjustment was needed to queue the message up and deliver when the network was available. We also had to put a ping functionality in the gateways to be able to identify non responsive gateways.      To assemble a working prototype of the IoT device with load cells to be able to measure the weight and post the data. As we progressed we figured out that there were easier ways to post the data to cloud in a more secure manner.  During the build, i only had a difficult time trying pick right resource i need on azure. i had to read a few documentations to get started but that was all.  Implementing calculus in C#, and building in a VR environment.  Developing the algorithm for alertness index was the biggest challenge as it involved extensive research and data from experts in the field and common accident causes analysis . DepartBy feature , thought easier , took longer time to complete because of complexities involved in getting data from various APIs used .What's next for DavidThe alertness index algorithm required rigorous testing from users and the algorithm needs to be perfected after more research , opinions and adding questions that can help us predict a better alertness Index scoreBuilt Withandroidgoogle-mapsm-1railsmartbusthepeoplemoverweatherTry it outplay.google.com      Submitted to    Go Detroit    Created by  Developed every single bug you can find in hereRoyce Raju BeenaLoves travelling , following football( mostly ARSENAL) , bike rides . Plays with android for a living and as a hobby too .  Well, I can write a book regarding the challenges :D ,As challenges are certain while chasing a big target but they teaches us a lot. When we were in prototyping phase, circuits built over breadboard used to cheat us everyday.We lost 2 turbidity sensor who falls into water bucket and died. fWe also faced problems in using some of the AT commands to communicate with GSM but this problem was resolved later.Fund could be a big problem but we are in such a campus where prioritiy is learning.    This being my first mobile app I had very little knowledge of the subject, so basic app development seemed not so easy.  There have been many challenges we have faced in this project. First the integration of a board without an Internet connection.We solved this by using a board based on the ESP8266 as NodeMCU. Perhaps there is a much better solution but the simplest and fastest was this.At the time of performing rear ventilation, the problem that we have found is mainly how to make a fresh air flow to lower the temperature.We have followed different phases to decide when to turn on the fans.In a first phase we have done a data collection to analyze the information.They are then manually activated to check what happens in certain situations. This led us to think that the temperature difference between the back and the front was a good indicator of high consumption..However, as we were collecting data we realized one thing. The best time is when the compressor is activated and at the same time the rear temperature increases. It is much easier in terms of programming to control this situation.Undoubtedly, the back temperature is closely linked to the compressor activity.We need to do further investigation, but first tests show that there may be about 20% energy efficiency improvement activating air flow.We checked that, when blower moves air behind refrigerator, compressor activity and inner temperature reduction speed are significantly reduced.Accomplishments that we are proud ofWe are very proud to have achieved a system that is capable of two tasks. On the one hand control hidden parameters of the refrigerators to the users.On the other hand have been able to minimize electricity consumption in certain conditions.At a first stage, asigning temperature sensors to each point of measurement was difficult. We had to check their hardware addresses to identify them and write it into source code. Now, we use temperature differences to identify sensors automatically. This makes easier to setup the system.There is no doubt that there is still a long way to go in this project, but with the mere fact of having all this information we can help people reduce the consumption of their appliances.What we have learnedAbove all we have learned one thing, how refrigerators work. We have detected that in many occasions it is made an abuse of them.Just by applying a few modifications to the installation, we can reduce its consumption. But all this is not possible if we do not have analytical data to corroborate it.This is where open technologies come into play. They give us the possibility to create devices that adapt to our requirements. Without these technologies none of this would be possible.We have also found that there may be huge differences in the consumption of different refrigerators. This project can help identify the most efficient models.Only if we know the world around us, can we change it.What's next for IoT Fridge SaverThe following steps we have planned are as follows:Improve data storage through Firebase. One database per user.Make summaries of the histories to show them in the home.Make historical data private for each device.Change the device settings on the web through a form.Activate or deactivate fans remotely.Conduct a socio-economic study in different countries.Built Witharduino-101bootstrapds18b20firebasejquerynodemcusct-013Try it outwww.iotfridgesaver.comprogramarfacil.comgithub.com      Submitted to    Intel Hacks 2017Winner                Popular Choice                  Created by  One of the most consumed appliances in homes are refrigerators. At times, you can reach consumer peaks that trigger electric bills.It can be due to several factors but the one that influences more is the little air circulation in the rear part of the same one.By circulating a fresh air stream in the back, this consumption could be reduced. This project aims to investigate this theory with analytical data.The benefits, apart from economic ones, are mainly environmental, CO2 reduction and energy consumption.Luis del ValleIdea, 101 programing and 3D printed partsGalile0WiFi bridge setupGermn MartnCommunity manager Littlepony  There were challenges:Developing smart algorithms due to the limited amount of data available for software projects and the holes in the data that was available. Designing new visualisations and user experiences to fit with the AUI guidelines. Were rapidly prototyping many of the visualisations outside of the AUI and then we will gradually roll them into the JIRA interface once they become more polished.Designing something that can work with both JIRA Server and JIRA Cloud as well as other parts of the Atlassian suite.  The BUS routes is the most challenging feature, because we need to people help us to mapping all the bus routes in the city, and this is the most difficult part due to accuracy of the route, but I believe in people and they gonna do a great work.The great challenge is based in on point in the map, connect all transport options and show it to the user. And there is no an open source to connect about METRO and METROBUS, so we have to mapping one by one the geolocation for more than 300 stations. Maybe I will open this info, if needed for other developers.  First challenge was the technologies. The purpose was also about learning, so we chose the technologies that we would like to get better at. Integrating the app for mobile devices, with push notifications and social media integration was really something special. Some complex issues had to be fixed on a short period of time.  Three years ago we tried to develop an app like PowAgri. But in that time we did not have enough information, as a result we could not complete our project. On the other hand, after seeing a lot of data in USDA datasets we faced a sweet problem: how to visualize the data to farmers for helping in their decision making process. There are several ways we can visualize data by choosing desired options. But it is not very easy way for a farmer. As our main target user are farmers, we spend a many many hours to design the UX of the apps. We studied a lot of survey, reports and agriculture guidelines to design our app. We also spend a lot of times to determine which information is very important to the farmers so that they can use this information to improve their farming. But at the end we successfully mange and visualize these to the user in the easiest way.  Several object detection issues.  Too many features and creating video of it ran us into problem. So we may not be able to submit video or even if we submit it may not be very polished & in sync with steps within video      Making the request to StackOverflow's API endpoint proved to be difficult. In the browser and from Postman the result was a perfectly fine JSON object containing relevant questions and their URLs. But from node.js when printing the response body I got output that looked like this: http://i.imgur.com/FrC6bdH.pngHinal from the Clarifai booth was a lifesaver in helping me figure out how to fix that. It turns out all I needed to do was include gzip: true, as part of the request headers.  HTTP request payloads were not being correctly transported and data was unusable at endpoints.  AI is very promising technology. But, the application of AI/Deep NEURAL networks to text data is tricky. It needs a large corpus of data and training the model is challenging.   I was alone working on this hack. I did not have clear idea and direction till the last moment.Everything was new to me.     As U@Work rides on existing email platforms, the biggest challenge lies in bringing feature parity and same user experience, by hiding from users, technology & implementation complexity, barriers and limitations. The APIs and plug-in integration approach is not the same and varies from provider to provider. This can be analogous to developing an application with same features and user interface to run on different operating systems.Getting to depth, The APIs and plug-in development and integration approach between Microsoft Outlook desk top version and Microsoft Outlook Office 365 is totally different. For instance, MAPI is used for Microsoft Office desk top version, which is different from using the latest & evolving Microsoft Graph technology for Exchange & Office 365, which is again different from using web hooks provided by Outlook APIs to directly interact with Outlook as part of Office 365.Plug-in technology is also evolving and not exercised by many so far. Initially, while examining feasibility options, lot of time and effort went in, even to make samples work. Also, lack of documentation aggravated the problem and debugged to figure out missing pieces and discover remedies. For example, first instance to make U@Work visible in the email window was a significant effort. Next steps to process user actions, when clicked, were no different.U@Work requires to interact with user mail box and email exchange server. As Microsoft Graph is also evolving, very few APIs are exposed limiting functionality that can be realised. Also, had to experiment a lot, due to insufficient technical documentation and usage examples. Another uncertainty is to battle possible changes to APIs, as they are still in beta and not released to production. Already faced a situation once, where change in API syntax broke U@Work. Presently, we use a combination of both Microsoft Graph and web hooks for Outlook APIs.Emails can arrive to any mail box at any point in time. U@Work to have to talk to Outlook and Exchange server and get notifications,  to be in sync with mail arrivals and user actions. Plug-in request to exchange are treated asynchronously and honoured on low priority. So, to synchronise and process in real time, (even when users have not logged into the mail box and U@Work) a separate service was built to run 24*7 to run in the background.With Microsoft Outlook Office 365, mail box can be accessed from popular browsers such as Internet Explorer, Edge, Chrome and Microsoft Outlook desk top versions (2013 & 2016). When it comes to user interface, size and form factor of the dedicated pane exposed for plug-ins to render varies between browser and desktop email clients. Also, there is no feature parity yet between browser and desktop version. For example, in case of Outlook desktop version plug-in can appear in the main menu bar. This feature is yet to be released by Microsoft for browsers. Ended up adding extra plumbing program code to address this.    Design an Algorithm for get nearby trips.Keep the application looking "clean" and "simple"  Real time graph implementation in the Android ApplicationAccurate ECG readingsGetting a satellite lock in the GPS module  Construction and operation of the physical systemIntegration of the various components developed by the team members  It was difficult implementing the GoogleMapsAPI with various Montreal Events, as well as finding and using data sets on the city's many events.    I ran into lot of issues with GPRS / GPS raspberry had to get internet capability to the raspberry PI.I had issue with debugging the custom vision module deployed on edge device using IOT edge run time in visual studio.     Getting more than one input in one intent (like the serving size, drink and no of drinks) e.g. I had a glass of wine. This does work for some cases, but we could see that it is prone to errors. We had to modify our logic and also have fallback scenarios build and thoroughly tested in order to make this happen.  The Arduino 101 didn't work, so we threw it in the trash.The Arduino Uno didn't have audio so we threw it in the recycling bin.The Google Home didn't have an easy way to play audio despite being a smart SPEAKER so we put it in the compost bin.Everything was incredibly easy because we are all geniuses. The most difficult part of this project was tweeting out our logo as a submission for the color war contest. Twitter makes me bitter.Originally we planned to use an IBM Watson cloud service to distinguish the type of garbage. But it turns out garbage can't detect itself. Though we got the app to "work", we couldn't figure out how to get it to sort the recognized objects into categories. In addition, the app does not really seem to identify things in an... appropriate manner.   EfficiencyWe used an object diffing library that should be able to efficiently give us just the changes so that we send the minimal changes to the subscribed client. But due to time constraints we're not taking full advantage of the diff library right now and our DDP updates are not as efficient as they should be. There are two issues:Unfortunately we can't tell yet that an object was inserted into the array and the following items just shifted down, currently all items after the inserted one will appear as changes. Very inefficient.Changes should be just the (top-level) field that changed. Right nowwe send the whole object (all fields). The diff library can tell usexactly which fields changed but we're not using it.UsabilityIt's challenging to design a UI that's self-explanatory enough for a casual user or hackathon judge to grasp immediately. We didn't have time to iterate on the UI.  Gender Inequality is such a major problem that can't be solve in a day. The biggest challenge that we faced during the development of this application is the way that we and how the society can take the first step in getting rid this problem, because a wrong approach from the start will eventually backfire after a long run. Accomplishments that we are proud ofWell "teamwork" and "bonds gained" is clich but it is the truth. After 2 days at Penang Codeathon, we as a team got to know each other more on how we approach on matters or solve a problem. We might argue and even rage amongst each others at times, but all those just made us stronger and managing to release our prototype is proudest accomplishment that we could have ever achieve.  What we learnedBefore this, we never had to come up with something from nothing, there was always a foundation and then we just played ball until we hit home run, but this time was not like that at all. We were amateurs (still are) so we were practically clueless about even starting, like where do we even begin? What are we supposed to fix? It was a huge setback for us in the beginning because all of the sudden, we all had differing opinions on what was the solution. Some of us were being really opinionated and stubborn while others just kept quiet and appeared indifferent. It definitely took us awhile to get our wits together and learn that the only way we were moving forward was if we confidently voiced our ideas and yet, be open to other member's rationale on why it could or could not work, what could be better.One key thing that we picked up was how important it is to remain professional about it all. No taking things personally and then getting offended because it's really not personal at all. And there wasn't any time for arguments with the little time we had to complete our product. We had to learn to trust each other as tasks were relegated to each member of the team. Fortunately, things smoothened out and from then on, we simply focused on delivering our brainchild, TheFriendZoneWhat's next for The FriendZoneTo be honest, we're not quite sure exactly what's next for our baby because, we just don't know how far we can take it. Developing it will need a good sum of resources that we do not have but let's say we do and all goes well, in that case, there's a whole world out there that we reach with TheFriendZone. We can develop a more sophisticated gaming style that isn't just, here's the question, now give me the answer, but more like, get the hero to escape the haunted castle, you know? It can be designed to be more interactive and engaging. Of course, we could host small competitions, akin to a 'big boss challenge', that will be collaborations with Tech employers out there looking for fresh innovative solutions, all the while building a stable platform for players all over the world to play together and learn from each other without being affected by discriminations, unconsciously or not.Built Withmit-app-inventor      Submitted to    #codeathon 2017: Technopreneurship for Gender EqualityWinner                University: Most Socially Impactful (Global)                  Created by  Pumpkin   MasterWen Kai AngMervyn OngDexson DcwwHong Jing YeungJustAPotato  Couldn't use Mains AC (the organization asked us not to) so we had to re-design all our circuitsNo one had 3D modelling skillsDesigning a plug system that was safe to use, even with the 230V AC (we did it, but we couldn't test it)    Implementing an incentive model (WRCs) that incorporates different types of organizations. Coming up with a machine learning model that fits seemingly countless possibilities of fraud.  There were a two main challenges that we ran into while creating Dawn. The first problem was the fact that the APIs we used requires money to be used extensively due to the large number of requests we made. The second challenge was creating accounts for users to save notes, projects, etc. This was a challenge primarily due to the fact that it was our first time building an accounting backend for a website. Accomplishments that were proud ofWere especially proud of our UI and UX. We spent a considerable amount of time pondering the most effective way we could present our idea. What we learnedHandling requests for APIsCreating a Highlight and Define function for the articles textUsing Firebase to create and manage accounts that store dataUsing Modal BoxesWhat's next for DawnFirst, we would like to create a better experience on Dawn by integrating more APIs which would allow users to access a larger number of articles. Second, we would like to create a cloud word processor that would help with academic language, check for plagiarism, and other features that can potentially help researchers output better research papers. Third, we would like to create a system to classify articles as read, important, irrelevent, and unread. This way, researchers can filter out unimportant articles from future searches for a certain project, see which articles were important and require further reading, and know which articles the researchers have read. Built Withajaxaylienbluemixbootstrapcss3easybibexpress.jsfirebasehtml5ibm-watsonjavascriptjqueryjsonjspdfmerriam-webster-dictionarynaturenode.jsquilljsxmlTry it outdawn-search.herokuapp.combit.ly      Submitted to    Make School's Student App Competition 2017Winner                10 Upvote Prize                  Created by  I contributed to the design of the css, creating the transparent theme and logos. I also built the majority of the back-end and front-end scripts that involved animations and Firebase.Andrew WangKevin ChoWaterloo '23J Young Kim  Early on, we ran into numerous software issues as we were all discovering and learning about the different technologies available to us. After consulting with industry professionals and hours of debugging, we managed to successfully get the chrome-extension and our algorithms to work. Additionally, with limited experience with back-end programming, we struggled a lot understanding the tools and the interactions between platforms but ended with successful results.  Accomplishments we are proud of: A major accomplishment we have achieved is being able to meet inspirational individuals of all ages, who are all passionate about technology and helping others. We have made links with many which we hope to maintain even after this event. What we learnt: We now know much more about technology and entrepreneurship and are aware of the fact that age is simple a number and doesnt restrict what we are able to achieve. Whats next for Study+? We hope to become an established environment which students find beneficial and support them into achieving to their maximum potential.       Submitted to    AcornHack    Created by  deleted deletedNasteho AbdulPatricia SulmaZen Chin    Creating the PvP system and integrating Kin in a way that encourages community and sportsmanship.  The biggest challenge was time.   Uploading the code cost me real money, because you need to have the contract read by ethereum miners.What's next for Montreal CoinHaving Montreal officials approve it, and introduce it as a reward for goods and services.Built Withazureblockchainethereumexpress.jsherokumongodbmongoosenode.jssmart-contractssolidityTry it outmtlcoin.orggithub.com      Submitted to    Hackatown 2018    Created by  Did the whole thing :)Alexander ShevchenkoFirst Place Winner at HackHarvard 2018 (Best Overall Hack)  We've had a hard time adding new reports to JIRA. The documentation of the reports module is outdated and not very useful. Also, it was not straightforward to find out how to render the JIRA's project side bar on report views.It was really challenging to put the blueprint module to work properly, creating workflows, issue types and custom fields for new CRM projects. As the documentation hasn't helped much, all the research took a long time.Drag-and-drop features are always a challenge too :D They have to get to the perfect usability in order to be really useful.  Installing python edge modules on Raspbian (Resolving the error - ImportError: libboost_python-py27.so.1.55.0).Training the ML models with the right parameters and increasing the prediction accuracy of the model.Configuring communication with Edge device - To design the entire flow of sending query parameters from HTTP triggered Azure function to IoT Edge devices.Time - We did not implement all our initial use cases.  Setting up Kin backend sdk on my server was a challenge as I took some time to making it work fine without any glitches    The challenges Wheels is running into are to massify our services in all the cities around the world, especially in the cities located in the emerging countries because it is there where there are big concerns with the mobility. As well, another challenge Wheels is running into is to work next to local transportation agencies so that together supplying our data so that we can improve the mobility in each city.  Consuming SNOW API calls        There are so many challenges in starting a product, first and one of the most importants is Team. Bussi needs the right team right now we are a small group in deliver what people wants. we are probably not complete yet but we are still working on it.Second thing is convincing Van owners to try Bussi model, is a hard task we are accomplishing step by step, also we need to have user demand and making growth so both Van owners and users might be satisfied. This is probably a very common challenge in two sided markets.Third is cultural shock, most of people in cities like mexico city are custom to own a car but carpooling or van sharing is something they dont understand at first, as well people thinks spending 2 to 3 hours in transportations services that lacks quality and safety. So comming with a solution that changes the game in order to penetrate these markets takes a while.  After we decide to reduce congestion traffic flow by raising the section toll rate, the first challenge we encountered is to determine the travel speed interval and the corresponding toll charge( how high should the charge get?). It took us a lot of discussion to finally set a 10km/hr interval and an increasing 2,3,5,7,12 times toll rates. Based on personal experiences, we all agree that the current toll fee is too low to force travelers to take different routes. A maximum 12 times of the original rate should be enough to compel most divers to take other roads.The second big problem is the data source. We encounter difficulties trying to directly download the instant speed data from the National Freeway Bureau website, so instead we focus on the downloadable Freeway No.5 traffic data of April, May June of this year(2015). We analyse the data provided by 157 cameras over the three month period first by removing the false/unreasonable data and then averaging the data during the same hour of every day, and finally locating the cameras closest to the current toll gates and use their data as reference speeds. Overall, it was a very time-consuming process due to the large amount of data.  Our first React Native appScraping data for the databaseData collection was successful but we ran out of time to implement the rendering of the visual interface.  The app is currently available on IOS.   Android is being developed to reach the largest possible audience.      We are having some hard time in storing data from Alexa to Dynamo DB and process it accordingly. However, we are also facing some accent issues with Alexa.     Most BMS APIs requires mandatory filters to query data from the system. Also, search on all fields in the object is not supported.  This resulted in additional processing after fetching data from APIs.  Work order priority is not captured in BMS system.  Since work order priority was one of the key parameter in this solution, we had to define a workaround based on promised date to achieve the results  We expected WIP to manage / make available the material inventory. We had to define a temporary inventory in the application and manage. Also location of Material is not captured in system.  Adding data to the system was challenging. We lost time to understand the clock-in API to feed data in work orders.  Objects in different modules are not internally connected. Needs significant to map data fed to the system.            As this was my first time using Firebase, I was quite confused with the JSON structure and how to organize my data. I ended up creating various dictionaries for both the pages and the posts. Inside the app, I converted the downloaded dictionaries into structs to easily manage and access information for each post. Also, after piling up storage data, I had to shrink the image sizes to a smaller size that still remained very high quality for viewing on phones and tablets. Later on, as I wanted to make the app more efficient, I wanted to download only relevant data so the process would speed up. At first, I had trouble querying Firebase to retrieve only selected posts. After several trials, I was finally able to find a method that was organized and easy to download the data.   The most challenging part of the process, beyond the coding, was adding finishing touches to the app. I know that UX, user experience, is essential for someone not to delete your app minutes after downloading it. A few friends tested out the app in its early days while I watched to see how they interacted with the app. There were obvious things that made sense to me, but my "users" could not figure out. So, I took my observations and their feedback and went back to the _ storyboard. _What I learnedPrior to this competition, I was unfamiliar with many of the concepts needed to develop Access Granted. During this process, I learned how to implement UITableView, UITextView, UICollectionView, AVFoundation, and Core Data. I also learned that I can build a functional app in a relatively short amount of time ( ~ 3 weeks), which I truly did not think was possible for me before this process. I gained confidence and skills during this period and I am grateful for Make School and the competition.What's next for Access GrantedAfter this competition, I will continue to develop Access Granted. While the app can appeal to a wide audience, I am interested in connecting with organizations that work with young adults who do not have easy access to career services. I would like to find out what features would be most beneficial for young adults as they prepare for their interviews.One idea I have is to make Access Granted social. Users will be able to log on, favorite questions they like, upload their recordings to the community and receive feedback. I would also like to set up video mock interviews through this app or a sister app that pairs job seekers with professionals.Thank YouI appreciate your support! I could not have done this if it weren't for the love of my family and friends driving me and for the opportunity Make School has provided! Thanks Built Withavfoundationcore-dataswift-3uicollectionviewuikituitableviewuitextviewxcode-8Try it outyoutu.be      Submitted to    Make School's Student App Competition 2017Winner                10 Upvote Prize                  Created by  I designed and coded Access Granted.Hal Simelane  As with all hardware projects, what we built took a much longer time than we anticipated. We ran into several roadblocks along the way in all parts of the process.During circuit building, we were just learning how to use transistors, so we tried many circuit designs and blew many transistors before we were able to use them correctly. During the mechanical design, our biggest difficulty was preventing the device from leaking. There were many points where we had to drastically change our design in order to prevent leakage. We also had difficulty in figuring out how to cut the materials, but eventually found a few methods that worked really well.    It was tricky to setup the infrastructure and environment required for Web App using Azure AD and for Graph API. It was also difficult to debug Office add-ins on devices like iPhone and iPad. In addition, there were some challenges regarding the data visualization too. We have spent a lot of time trying to organize the bubbles in the way we wanted.  The largest challenge we met is how to apply theory into real world. We are students from transportation management and chemical engineering and have some experences in software development. We have to figure out the problem first and apply what we learn in class to solve the problem. It s really a great challenge because real world is not that easy. For example, we want to apply transport formula to calculate capacity. However, the turbulent system do not follow the parameters. When we personally observe in the highway, we found that many parameters are wrong in real case. We concurred a hard time to find out a solution with support of theory.     Training watson was the most challenging part. First of all, we did not have a lot of Bluemix credits, so we had to really limit the amount of queries we made (including training of course).Secondly, there is not a lot of footage to get online where people are about to commit such acts (which is probably good, IMO).  Figuring out the Alexa APIsUsing Twitter REST API from Node JS APIs.  It was challenging to play different music track when the user tap on a different tableview row. Storing things from every self-care trial the user goes through into the CoreData was a key challenge. At one point, I messed up a lot and had to revert to a previous version from GitHub.  Generating "random" words is not an easy task. Taking a sample from the dictionary lead to high frequency of words that the average user never heard of. Then, we've opted for pseudo-randomness and used most common words for a specific language.    The biggest challenge we faced was determining the optimal flow of questions to present to the applicant. We also had to ensure the application was user friendly, and make sure that it was easy to fill out for applicants of diverse backgrounds and technical skill levels.  Getting up and running with the developer rig was somewhat challenging as this was my first foray into the tool. After some getting used to, it turned out to be a great resource. I had worked with Lambda and AWS quite a bit in the past but working with the comprehend API was another learning experience with this project. The biggest challenges I ran into were performance at scale. I wanted to make sure my pack size and response sizes were small so this could scale up. I made some major architectural adjustments to account for performance as well as some potential issues with pub/sub limitations.     I encountered many challenges during the development, most of them were related to the rendering/behavior of the control on the MDI form and discrepancies between what appears during design and what will appear during runtime.Another type of issues I faced is related to XAML workflow invocation.The used of certain packages within the XAML invoked from the form requires to use an activity from the same package before executing the form (ex: If the workflow needs to send an email, it is required to use a "Blank" Get Mail activity before using the form activity).Certain activities used inside the workflow are not functioning (Invoke workflow file, Activities communicating with Orchestrator...) - This seems related to a missing Workflow extension (probably ad hoc one created by UiPath) from the Workflow Instance Extension Manager.I am looking forward to communicating and collaborating with UiPath in order to resolve those challenges.  The most challenging for us was to figure out how to work with geographical data, how to display the polygons of fog and to cut out the visited area.     Balancing convenience vs. security is very difficult.  We wanted our user experience to be as fast and easy as uploading an attachment, but we have to gather so much more information about viewers and security measures.  We did dozens of iterations and settled on our simple dialog approach with as few steps as possible.Encryption is CPU intensive and due to Javascripts single threaded nature, our UI was vulnerable to getting blocked during the encryption process. To resolve, we used workers to perform the complete file reading/encryption process.          from a business point of view it was difficult to find the right balance between getting as much valuable information as possible form colleagues and do it in the most non-disruptive and fun way; this lead us to build the appfun fact ;): AtlasKit is still under heavy development, new versions have been released during our development sprints.we wanted to make a fully responsive version from day #1, custom components had to be developedwe wanted to include Atlassian account login option as well; for the time being only BitBucket is available.  how to help Turtle Conserver against Egg Poachinghow to develop a system that can save turtle from extinction.  Currently CARE app is limited to certain Crops and locations; I may need some data source for different crops historical growth data to provide more accurate information.  We spent a lot of time trying to figure out a way to preprocessing the audio file into text due to lack of existing API that can help our speech recognition search.    It is very time consuming to select the right art, compose music, and decide on the script. The engine was not very well documented and the community around it is relatively small, therefore most of the time we have to find workarounds for bugs ourselves.    Our biggest challenge was integrating the two APIs together with little to no documentation in some cases.  Because of this we needed to hard code our own XML string generator (that was initially written using Node.js) because the other API was not compatible with Node.  well the main challenge was to make the app handy and to the point so it can solve the real problem, so we had to check multiple apps and come up with this imagination of how the email and note can be in the same place, deploying a zend application over a google application engine wasn't easy enough we ran into lots problems because Google has some rules that was violated by most of the php frameworks, so we had to make sure that the framework will run smoothly         Generating a interest based itinerary was the most complicated part of the app as it involved considering several factors like user interests , distance to the place, managing times and preferences to suggest restaurants.    We attempted to incorporate gesture recognition into the service, but it would cause delays in the transaction due to extensive training/inference based on hand features. This is a feature to be developed in the future, and has the potential to distinguish and popularize our unique service    We learned AngularJS during the competition. We spent a lot of time deciding the brand and how we would create the app. At the end, we did a great job and created a MLP (Minimum Lovable Product).    The UI for the android application required extensive use of google search and StackOverflow.    since we are students, most of us didn't have any experience as a software designer, and since we were using tools we had never used before, it was really hard to get the hang of it.   The firmware created by MIT is new and I did not work the PINS library, which blocked the TEMPERATURE library, so use the LED library to run the actuators.The other problem I encountered was with the Voltage Elvador, which did not work, so the part of the battery use and its recharge with solar cells stayed on hold until a new module that if it works.  Integrating google maps with custom markers showing different icons for different parties involved.  We initially had trouble with the drag and drop functionality, but managed to overcome it.Settling on a UI that would be optimal was also a challenge, but we were able to settle on a design with feedback from other participants.Half of the team members had minimal experience in javascript and almost everyone in the team picked up NodeJS within this hackathon.      The code for the multi-view form becomes tricky after several steps are added. I used Lucidchart to map out and visualize the flow of the form before I started building.Storing data to be CSV readable was also new and challenging. All CSV data needs to be organized well and captured in a format that allows for research and study. For example, the exported information should not list an adult as "false" for being a student. Instead, it should be empty, so that the column can be compiled to show correct percentages of student demographics. By making sure these fields are left blank for adults when exporting, the compiled information in a CSV will be more accurate.User TestingI began user testing the moment I completed the flow of the form, with a total of 13 people. I would hand my participants a piece of paper with fake information to use as they filled out the application. I wanted to simulate two scenarios:Distracting participants from the form by having them search for information outside of the application.Simulating an adult who may not remember exact social security numbers, income, etc.I also did a couple of rounds of testing on UserTesting.com. Tests frequently revealed issues with broken functionality and incorrect input types. More than anything, the user testing was beneficial to alter some of the language and messaging that was confusing to participants, as I mention in the video.  Too many. That's what happens when you work with things you've never touched before (and Git).  Interacting with the Google Natural Language API, Cloud Speech-to-TextLearning how to use Node.js & JavaScript with little prior knowledgeInterfacing between the front-end & the back-endIncorporating and parsing video & sound files from a microphone and webcamWhat's next for MIRORImplementing additional APIs to further increase accuracy of feedbackAdding a database to keep track of users and demographics to improve interview tipsBuilt Withcss3html5javascriptnode.jsTry it outgithub.com      Submitted to    BeachHacks    Created by  I worked on implementing Google APIs with Node.js and helped with data analysis.Catherine PhamI worked on the user interface, programming with HTML, CSS, and some Bootstrap components.isaiasramirezI worked on back end logic, functionality and coding, communication of back end and front end, receiving and extracting information from the Google NLP API. Pardis Khodadoustandenilpoudel  We ran into many walls because we all used a technology that we were not familiar with. Will and Chanel started React-Native while Matthew and Tim worked on Nodejs + JavaScript.  The challenges I faced in making this game is overtime working.I am a beginner  in game development. But YouTube helps me a lot. I am pretty sure that will eventually come to real world in the near future. I really proud of what I achieve so far!  Most of our group members are at the beginner level, and we did not have a teammate who was able to contribute any front end development. It was difficult to put together a functional solution without the client-side.  Plotting the readings to a graph and saving the graph as an image to gallery were the major hurdles   We faced few issues with authorisation for trello APIS. When we tried storing the token on the callback URL, it saved the token on the server's domain which was inaccessible from trello.  We had difficulty in finding materials that could support the motor weight and move the windows. Also had trouble synchronize the application data and data from the Arduino.     The major challenge that i encountered was the oauth of context.io, and how to connect it the gmail api        Create a fresh installation of Anaconda3 / Python 3.6 on a Windows 10 machine.Code a python script managing multiple types of algorithm and 2 types of machine learning problems (regression and classification).Link UiPath to Python using standard available activities.Use Custom Input activity to really enhance user experience and ease the use of the tool.Exchange data between Python, UiPath and an embedded web application through Custom Input activity.Accomplishments that we are proud ofGive the ability to create a supervised machine learning model with a simple Excel file as input data.Connect UiPath Studio to an Anaconda environment and manipulate Python objects with standard activities.Develop a user-friendly web application with modern CSS3 and HTML5 along with Javascript ES6.Integrate this web application inside a UiPath Robot and find a way to exchange data with high performance.What we learnedWe learned that UiPath solution is really designed to talk to various types of applications and can be definitely used for a tremendous number of use cases.We also learned a lot about how and when to use Machine Learning algorithms to fix different types of Machine Learning problems, either classification or regression.Eventually,  that mixing different skills can make you produce things you're not able to create alone.What's next for Machine learning for non-datascientistsWe can hope adding more supervised learning algorithms and add more parameters to existing ones.Create an unattended version of HandyML Predictor.Also, why not integrate Deep Learning?Quick installation guide1. PrerequisitesProject developed and tested on a Windows 10 machine.1.1. UiPath versionThis project has been developed under UiPath Studio 2019.4.3. So you will need a Studio/Robot with version 2019.4.3+.1.2. Python versionThis project uses Python 3.6.8+. It has been tested and developed on a x64 architecture.You can easily have a working Python 3.6 environment by installing Anaconda 2019.03 version (download Anaconda). Then you will need to create an Anaconda environment specifying Python 3.6.8+ version by executing the following in Anaconda Prompt (replace [environment-name] by the name you want, eg. env-3.6):conda create-env -n [environment-name] python=3.61.3. Python packagesYou will also need to install the following packages using either conda (if Anaconda installed) or pip to be able to use this tool.If using Anaconda, launch Anaconda Prompt and execute following commands:activate [environment-name]conda install numpy pandas scikit-learnpip install matplotlib scikit-plotpip install win10toast (add options --user --ignore-installed when installed on a Windows 7 machine)1.3.1. NumPyDeveloped with NumPy 1.16.4.1.3.2. PandasDeveloped with Pandas 0.24.2.1.3.3. Scikit-LearnDeveloped with Scikit-Learn 0.21.2.1.3.4. MatplotlibDeveloped with Matplotlib 3.1.1. (Note: needs to be installed using pip, not working correctly using conda)1.3.5. Win10toastDeveloped with Win10toast 0.9. (Note: needs to be installed using pip, not working correctly using conda)1.3.6. Scikit-PlotDeveloped with Scikit-Plot 0.3.7. (Note: needs to be installed using pip, not working correctly using conda)1.4. Environment variablesIn order for the UiPath Robot to know where is Python installed and on which architecture you'll have to create 2 environment variables:HANDYML_PYTHON_PATH, indicating the folder on which python.exe file can be found> With Anaconda installed you can know where the python.exe file is by following this: https://docs.anaconda.com/anaconda/user-guide/tasks/integration/python-path/HANDYML_PYTHON_TARGET, indicating the machine architecture (should be x64 or x86)2. Create and train a supervised machine learning modelJust launch HandyML_Trainer and follow the steps!Of course, to start you'll need an Excel file holding your data, everything is explained at step 1. Where is your data?.3. Make predictions on new data using a trained modelThis time launch HandyML_Predictor and specify the path of the Excel configuration file generated by HandyML_Trainer indicating the model to use, the JSON configuration file associated to this model. A sheet named data is available for holding new data.Important note: New data must have exactly the same format as the one used during training.4. Let us know if you have any issuesGithub: https://github.com/masiref/HandyMLEmail: masire.fofana@natixis.com | chabnichab@eisti.euBuilt Withanacondacss3html5javascriptjsonnpmnumpypandaspythonsassscikit-learnTry it outgithub.com      Submitted to    Power Up AutomationWinner                Best RPA for Fun Solution                  Created by  I worked on UiPath robots and on the web application using HTML5, CSS3, Javascript (ES6).Masir FofanaRPA Solution Architect at NatixisI worked on Machine Learnings algorithms and their implementations using Python libraries like pandas, numpy, sklearn, joblib. Chabane CHABNI    The most challenging task was to match dependencies after migrating to the new MongoDB version.What I learnedI learned how to host and deploy apps with herokuWhat's next for Distributed EstimationI really love solving challenges of distributed teams, so I'm not going to stop on this Add-On. The next Add-Ons are:Relative EstimationRetrospectiveBuilt Withangular.jsexpress.jsmongodbmongoosenode.jssocket.ioTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist: Add-on Hackathon    Created by  I was responsible for the whole project starting from the concept and straight to the shippingVitalii Zurian     Hierarchy of Things Our use case demanded that each truck was a thing which had some sensors as well as there were some things that were connected to the thing in this case tires. Ideally we would have registered the things in a hierarchical fashion but there is no such capability in AWS IOT.To accomodate this challenge we introduced an edge device. We tested a raspberry pi connected to multiple sensors getting the data and persisting on the raspberry pi and then syncing the data. This introduction of edge device was also helpful in building a more tolerant logic that could live with network failures. Issues with Kinesis SQL It took us time to understand the syntax for Kinesis sql. The Kinesis SQL syntax was unlike general SQL and took  a little bit of learning Curve as the Kinesis SQL was case sensitive. Lack of Logging Kinesis SQL  application doesnt provide any log in Cloud watch. This meant the only means for testing was through trail and error when the application didn't work as expected. Debugging the Parsing Errors Kinesis SQL parsing error messages were not very intuitive so we needed to perform some trial and error on sql queries before figuring it out. API Throttling Errors Within AWS we ran into issues when we tried to analyze images using Rekognition at a high throughput and encountered API throughput exceptions. We requested for throughput increase to ensure that the lambda pre-processor output was working.    Our job was essentially to get the user from their starting location and back with an amazing trip in between. This in essence is the Traveling Salesman problem which is a NP problem which does not have a polynomial time solution always. We had to come up with an efficient algorithm which goes around this problem.  Having only software experience and knowledge, we are beginners to hardware - it was intensely difficult to perform simple tasks like soldering and fix the hardware parts all together. There were constant issues arising - either the wires were not connecting properly or the screws were coming out to the extent that in the last minute one of our robot motor just broke. It was a real challenge for us since it was out first time doing a hardware project.    Email syncing takes a long time for large mailboxes. This might have an effect on user experience. Used polling to mitigate the long wait time. Also, we persist personality and tone data to minimize Watson API calls. During testing, a Context IO dev account can have its mailboxes locked due to authentication issues.        The only difficulty we had to face was that one of our collaborator, that had to leave the hackathon early, was so addicted to Polytunes that he actually couldn't stop playing event at home. Because we had no way of contacting him beside drawing pixel art style words with Polytunes, it made it very difficult for us to create the presentation screen capture video.What we learnedWe were all beginners with meteor but we found that it was very easy to use to quickly build a responsive and dynamic web-app.What's next for PolytunesWe'd like to add the ability to create public and private rooms which url can be shared with friends or on social networks. We currently used a lot of default settings that would be settable by user, such as tempo, pitch, musical scale, board size, etc. We also would love to build Polytunes as a native app for mobile devices that could allow both solo offline and collaborative online play.Meteor package usedmeteor-base             # Packages every Meteor app needs to havemobile-experience       # Packages for a great mobile UXmongo                   # The database Meteor supports right nowblaze-html-templates    # Compile .html files into Meteor Blaze viewssession                 # Client-side reactive dictionary for your appjquery                  # Helpful client-side librarytracker                 # Meteor's client-side reactive programming librarystandard-minifiers      # JS/CSS minifiers run for production modees5-shim                # ECMAScript 5 compatibility for older browsers.ecmascript              # Enable ECMAScript2015+ syntax in app codegrigio:babelstylusartwells:accounts-guestaccounts-uiaccounts-passwordcheckrandommvrx:bluebirdgrove:call-asyncmizzao:timesyncu2622:persistent-sessionlookback:tooltipsBuilt Withhtml5javascriptmeteor.jswebaudioTry it outgithub.compolytun.es      Submitted to    Meteor Global Distributed Hackathon     Created by  I worked mostly on the WebAudio API and front web design, but also had a taste of Meteor with the live update of players count and list.Clment BourgoinWilliam Hollacsek  Non-Standard UI-Elements due to our high design expectationsChairs were not that comfy Sorry, guysTime Constrains - often we needed to run ad-hoc crisis meeting to decide how to cut the scope to finish on timeMVP - how much VIABLE does it need to be - to stay useful and sexy  It was our first time using React. And that was hard. It took us several hours to understand how to get basic routing working. But we got through that, and know we know quite a bit about React.      Some of the challenges we experienced was attempting to narrow down our focus from a mountain of ideas and ambitions. Due to the time constraint, creating First Days from scratch in less than two days means that thorough testing and constant adaptations is very limited.      One non-obvious challenge that we ran into while building Fireflies was that our users really didnt know what to expect when we did user testing. The idea of our app was originally much broader, and we gave users lots of control in the app to to order anything and get it delivered anywhere. (One use case was getting problem sets handed in by peers for their classes). We left a text box for the users to enter what they needed and how much theyd pay. However this was challenging in that the users didnt know what to put in the text boxes, and if they did, they didnt know how much to pay. We solved this by eliminating user inputs and giving them a few options with predetermined prices. The app became much smoother, and we noticed a visible improvement in user happiness. This sort of standardization required that we do a lot of work on the backend. When we integrated locations and routes along with pricing, we needed to automate the process to be extremely precise. Alohar provided the information we needed and helped us apply it well within the context of the problem we were trying to solve.     We faced many problems while building this project. Calibration of sensors data was a great challenge to us, there was no perfect way coming out to do so. The methane gas sensor was the hard task to do, it was never giving exact perfect values, if the values were correct on one day the next day it went to random. There was network problem in GSM when the sky was not clear. Also there were problems in interfacing Arduino 101 and GSM Sim 800 ,so we used Arduino UNO in between.//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  The HipChat integrations API was a bit of a time sink just to get the installation plumbing working.  Customizing Elsa's dress, which doesn't follow standard princess dress configuration. We coded a gradient into the dress, by taking the initial RGB value and ending RGB value and having each R, G, and B increase proportionally. We discovered, however, that they can't linearly change or the colors in the middle of the dress will be on a completely different spectrum since the ratios are off.       Setting up the server the serve the app quicklyBuilt WithandroidmysqlphpTry it outgithub.com      Submitted to    KPApps: The Khyber-Pakhtunkhwa Apps Challenge    Created by  Made sure the app was actually completed.Asad SalmanHandling backend data storage and server processing; hence providing an API for the app to access server functionalities.Talha ParachaI worked on the front-end of the app, developing the interface.Taha RazaI worked on the front end of the app (java in Android studio). Handling requests from the server and sending it requests. And then displaying them to the users.Shah Rukh QasimI worked on the Back-end databaseMansoor Jadoon  TIme is a precious commodity, which you should handle with care.  Our canvas (screen capture of webcam) was cropping our image when being sent to the back-end.  All of us participated in our first hackathon. Staying up for 36 hours at a stretch was our first experience and the biggest challenge. We are new to python and all the mining we did for the analysis was through python. We ran into issues while mining the data. These issues took some time to be resolved. Learning new technologies in extremely short span of time. Dealing with data poses unpredictable challenges. It is especially tough for us as psychology is not our expertise and understanding this data took a lot of time.      The biggest challenge for our team was to collect the data of California. Since we are working on the different parameters of crops, and a large number of crops had alot of parametric data which had to be accounted for. Since we live in India, it hard to go over the authenticity of the facts of California, and this made it quite a time consuming process for us.        The most difficult thing was trying parse every possible Confluence page in such a way that it still looks good as a presentation. Also learning two very different plugin architectures from the ground up was quite an interesting challenge.What's next for Presenter for ConfluenceThe add-on is now available on the Atlassian Marketplace and we look forward to hearing from our customers for feedback and suggestions.Built Withatlassian-connectatlassian-plugin-sdkjavajavascriptjqueryphpreveal.jssymfonyTry it outmarketplace.atlassian.comincloud-products.atlassian.net      Submitted to    Atlassian Codegeist: Add-on HackathonWinner                Best newcomer cloud add-on                  Created by  I work for Incloud, which is a software development company focussing on the combination of web, mobile and embedded technologies. Using the best tools available enables us to create high quality software that can share core logic between platforms.Atlassian products are the foundation of our working environment, which is why we start falling in love with add-on development.Lukas Korte      LOTS! In the beginning we struggled to visualize an app that would allow users to post workouts, photos, and text posts in a simple way. Once we were able to plan the app out, we ran into an unreal amount of technological challenges. However, as with all the challenges we have faced, we eventually overcame them.     We had some problems with saving. At first, we tried saving it as a 360 panorama, but there is no available technology in the phone capable of taking a screenshot in the VR headset without an external camera. Then, we tried saving the vectors created from the strokes in an input file, which would be stored internally on the phone. When the user wishes to access the painting, the file would be read into the app.  Keep session state amazon Lex working concurrent with channel either direct and group who has been build conversation with Logibot. Integrating Logibot to other platform for providing actual information.    While building my app, some challenges I ran into were adding read/unread features; implementing cell-phone verification and login; sending push notifications; and designing a flowing aesthetic theme.  Getting started was tough as we had little to less knowledge on Apache Spark learning curve was tough eventually did it.  Since installing the system in tunnel will use lots of sensors and LEDs, it is an important consideration to decrease the cost with reasonable effectiveness. At first, we have evaluated and tried various kinds of sensor, some cost high and some need more time to develop. Lastly, we think the photo sensor is a relatively low-cost and good functionality choice. In practical application, the response time of sensor must be considered to be able to precisely detect the car speed which is over 100km/hr. Because of the large amount of LEDs, it is necessary to select reliable drivers to keep the constant lamination and stable. Moreover, the system has to deal with a lot of information of moving cars, so the hardware core needs good ability of parallel computing such as CUDA. It is also a good way to use multi control units to handle some information respectively, and the host computer or control center will communicate with these control units to monitor the whole tunnel. LEDLEDCUDA  I don't know how to write Java, MySql or use PhpMyAdmin at all and AppInventor isn't that user friendly. All the coding are learned from the scratch. AppInventor codes are not that understandable and repeatable in usage. It's crazy that all the tables are constructed one by one and all of the tables's width,height values are changed one by one.  Early on, we ran into numerous hardware issues with the Particle Electron board. After consulting with industry professionals and hours of debugging, we managed to successfully get the board working the way we wanted. Additionally, with no experience with back-end programming, we struggled a lot understanding the tools and the interactions between platforms but ended with successful results.Accomplishments that we are proud ofWe are proud to showcase a fully-stacked solution using various tools with very little to no experience with it. What we learnedWith perservance and mutual moral support, anything is possible. And never be shy to ask for help.Built Withamazon-web-servicesbuilt.iocisco-sparkhtmlparticle-electronphotoshopTry it outgithub.comgregwoo78.wixsite.comhomescan.org      Submitted to    McHacks 2018Winner                Cisco              Winner                Deloitte                  Created by  For HomeScan, I worked primarily on the frontend. I was responsible for creating the user interface through Cisco Spark. I had to learn how to create chat-bots that would interact with the user using text analysis technologies. The chat-bots would retrieve data from the AWS in order to output the correct information. I then used HTML to implement Spark on our website. I also participated in the preparation of our hack pitch by creating a logo and a video for the project. Creating HouseScan from scratch was such an amazing experience. I learned a lot throughout these 24h and had a lot of fun working with such an incredible team.Greg WooI primarily worked on the hardware side of HomeScan with sensors DAQ, sending data to the cloud and webhook it to AWS. I also helped on the design aspects of the AWS database and lambda functions as well as for the Cisco bot.Romain NithRomain CouperierMilo Sobral    Connecting people to get a tampon.  At the moment Planner is still quite new, so not all API endpoints give use the right amount of information we needed to build the module.The same goes for SharePoint. Microsoft Graph now includes a SharePoint API endpoint but does not allow you to call the search API. Therefore we went for the standard SharePoint Online API endpoints.  We get noise from the frequency channel of our RAM. We had to do a lot of signal processing to reconstruct the data we send to our RAM.The main challenges were involved with the RF processing (we had no prior RF experience) and decoding of the signal. We are dealing with extremely noisy data with no synchronising clock, and therefore the decoding algorithm was complex. However, we are comfortable with how it performed at the end - in several tests it managed to transmit 40 bytes consecutively without a single error.    Databases are hard.  I'm happy to say that this was a rather pain free project.  The biggest challenge which I faced to write the code in such ways that can be use in any existing android application seamlessly. Which can help millions of android developers.  Making the Raspberry Pi behave as hotspots was really tricky, interfaces kept conflicting for a while. We also really struggled to understand the serial communication between the Android app and the Raspberry Pi through bluetooth.    It wasnt easy to trigger tests in ALM from CI tool straight away. HP ALM tests can be triggered with the help of Open Test Architecture library of HP. Which means creation of separate program in C#, integrate it with Bamboo plugin and parse ALM test set results in a format bamboo understands and integrate results with JIRA.  Due to some of iOSs restrictions, setting the alarm to trigger at a certain time and forcing the user to only use the app was technically impossible. So we could only display a notification for the user to open the app in order to disable the alarm. Additionally, the UILocalNotifications that we used for the event triggers were uncooperative and sometimes would not fire  so we also built a standard timer to alleviate this issue.Unfortunately, due to the lack of an Objective-C wrapper/library for Plaid, we were unable to complete integrating their payment processing system.  The JMeter codebase is a pure java GUI, quite different from the usual projects we work on. We were not sure where to start.Hopefully, the JMeter team gave us some guidance, many thanks to them!  Integrating BrainTree, persuading Gradle to stop whining, keeping devices synced together via GCM.  We wanted to challenge ourselves by using a new tech stack for HackMIT 2019. For most of us, it was our first time developing a native app for IOS. Therefore, gaining sufficient context on the IOS programming practices was an important challenge for all of us.One specific challenge that we overcame was deciding how we would provide the Custom Vision model with enough pictures for it to be trained properly, without requiring the user to provide an absurd amount of pictures of their memory. Our solution was to require four pictures from the user, then manipulate the images to create new pictures out of them. This not only solves our problem of needing more data, but also increases our data variety to cover different angles, thus creating more robust classifiers.  The main challenge was testing everything on kovan the istanbul hard fork broke kyber which broke the fulcrum P tokens.Accomplishments that we are proud ofWe are proud that we were able to create a user experience which is quite easy for non tech savvy people to use and integrates with multiple DeFi protocols in one place.What we have learnedWe have discovered the power of DeFi recipes. These allow to abstract otherwise cumbersome multi transaction interactions into one simple action by a user. We will take these lessons to improve dDAI and future projects.What's next for dDAICreate a more generic approach to DeFi recipes.Implement dollar cost averaging on the UI sideAdd GSN support to the frontendAudit smart contractsMainnet launchDexWallet integrationBuilt WithopenzeppelinreactsoliditytypescriptTry it outddai.dexwallet.iogithub.comgithub.com      Submitted to    Kyber DeFi Virtual Hackathon    Created by  I designed and styled componentsdimarconicola Di MarcoI worked on the concept and dApp integration.Alessio DelmontiTech entrepreneur, blockchain researcher.I worked on the smart contracts and the user interfaceMick de Graaf    One of the main challenges is that we need to built our tools in a way that they help the team work better while not being a distraction in the conversation.What I learnedWe have learned a lot along the way with Convergely, and we are already adding more functionality around team communication.What's next for ConvergelyConvergely its planning to integrate better with the Atlassian ecosystem, we want to be able to schedule projects, messages and tasks.Built Withnode.jsredisruby-on-railsTry it outwww.convergely.comhipchat.convergely.com      Submitted to    Atlassian Codegeist: Add-on Hackathon    Created by  I worked on the backend. I also planned the architecture of the application and the API of Convergely.At first we had to understand how ACE for hipchat worked and this was intimidating, but everything started falling in on place once we finished reading their API's and code.miguel perezI worked on the backend for this project, It was the first time I used node and have to admit that it was intimidating at the beginning but I learnt a TON from this experience.Carlos ContrerasJonathan TarudGustavo BazanJuan Mejia    We had to come up with a low-cost solution which can effectively solve the problems of the pedestrians and cyclists. We all know that in ordered animal movements, every agent tries to optimise the movement of the team. The existing traffic solutions try to imitate this for humans by ordering their movement. The challenge we had was to gamify the existing system in a way which can ensure that pedestrian and cyclists obey the traffic rules.We believe that the current reward systems can easily be transformed into a redeemable award systems. This would give people more incentive to obey the traffic laws.  Le principal dfi a t l'utilisation de JavaScript car c'tait une premire pour tout le monde. La slection des feux concerns a galement pris un peu plus de temps que prvu: pour cela nous avons d rutiliser des concepts de gomtrie.    More validations and versions of RDPs  We need spend a lots of times to collect, design sprites for pet. Building game in blockchain quites different with casual gamesWe used Ropsten testnet to build Dapp so we met some interupt by migrating Ropsten to istanbul    To start with I had little clue avout usage of API key and Beacon Data as nothing was pulling up.I had to find different ways of getting an API key from 511.org to use.    It took me some couple of hours to conclude the design of the application, unable to get committed developers around me as i was trying to meet up with date of submission. Choice of payment platform and integration of payment platform etc.  Slow internet so I had to code the whole night at the hotel to catch up.    The Atlassian Connect JIRA and JSD APIs are new and evolving quickly, the documentation isn't always keeping up with new features. However, being able to talk directly to the API developers at Atlassian Connect Week was a great help.  ZOI was initially planned to be an Android app, developed using Java in Android Studio - however, there were inevitable bugs and unexpected crashes. Hence we migrated to a pleasantly familiar platform to provide an optimal solution on our part.  SchemaFront End DesignIncorporating API    Time constraint: One major challenge was completing the app and all necessary parts for submission prior to the deadline. We heard about the competition and applied late and had less time but solved this issue by splitting up the tasks and working together.MIT App Inventor language: A minor challenge we faced during development of the app was learning the block language that we were unfamiliar to. To learn the language quickly, we spent some time looking through tutorials and creating the sample apps to learn the functions of the app inventor and the basics of how to code. Once we knew these, we began to program our app and used the guess and check method to overcome other programming hurdles.Sorting: When developing the application, the sort function was a problem due to the complexity of the algorithm and the required knowledge of programming. To solve this problem we used the guess and check method. We developed several algorithms, and learned how they processed the information. we finally were able to make the current algorithm for sorting using a if...then case structure with variables programmed to change values for every run based on the given index.  Pie Charts: The last major programming challenge we faced was with the Pie Chart APIs. MIT app inventor does not have a function for charts and diagrams so I had to learn how to program and recreate APIs that I plugged into the show charts section of the Compare pages. These APIs take real time data from the app and convert them to show the information. I had a hard time formatting and setting up these charts but learned through online resources.Scalability: The scalability of the app was a minor setback in the design development process. The app was originally designed for mobile use since we were only able to test with the Samsung Galaxy S4. The text and buttons sizes and placement was based around this device. Towards the end of our process, we began testing using the Lenovo Yoga Tablet. Due to the large variance in size, the app was not able to format properly and we had to rescale everything using percents to match both tablet and mobile devices.  We had never created before a connected app using bluetooth with a hardware device and it took some days before we could send data between devices. Plug-and-play hardware are not always easy to set up! We had some trouble with some parts of the hardware, that took us more time that expected to make it work properly. For example, the stepper motor was not working as what it was expected from the documented examples and it was a lot of trial and error to make the piezo vibration sensor to get the petting feeling that we wanted. We had some trouble when we had to fill everything inside the plush toy, as the components are rigid and we were fitting in a soft space, that continuously was being pressed, which led frequently to malfunctions and cable disconnection when we were testing the "live" prototype.  We ran into a few challenges while developing this project: The data-sets which we utilized were very dense and overwhelming at first but as a team, we were able to attack them together and succeed in analyzing the data. Another challenged we faced involved some additional factors that needed to be taken into account in order to effectively complete this project such as Pedestrians, cyclist, left/right turns etc. Although these were challenges, they did serve as a sort of acceleration toward our main goal  One of the challenges we ran into was integrating the speech to text capability function in the web application, as well as finding innovative ways to teach this information. We spent a lot of time brainstorming ways to combine education with community. Using the power of Facebook and modern web design we built a unique solution.     Work in Progress with Token Sets: The Major Challenge that we ran into was working on creating a ZAP on top of TokenSets.  Sadly that is still work in progress.  Sadly their entire smart contract infrastructure does not simply accept ETH to mint the necessary rebalancing Sets.  For this purpose, we had to create a Smart Contract that works with the Provable Oracle to call for the price of the Set every time investment had to be made.  However, even after solving that major hurdle, we learned that every investor who had to interact with the Sets Smart Contract had to have not just WETH (for which we were using Kyber Network) but also any other component asset that they have defined in their smart contract - adding to this, there was NO way to determine how much quantity of either of the component assets were required.  This threw a major ball into our direction.  We are now working with the Token Sets team to resolve this issue so that we can add that as a product offering really really soon.Lost USD50 worth ETH due to wrong interaction with the Compound Protocol: While doing our first round of testing on the mainnet, our Invest2cDAI function interacted with the Mint Function of the Compound Contract and accepted uint as a return.  This uint should be the cDAI tokens that are minted.  We then straight-away asked for the Compound Smart Contract to transfer the output uint received.  We realized, the hard way, that we were wrong.  The returned uints were not being picked by the transfer function thus locking our assets.  Thus, we had to re-do a new contract which would first check our updated balanceOf and then do a 100% transfer of cDAI.No sync on Test Nets:   We could not, unfortunately, Test our contracts on Test Nets due to main reason that the DAI and WETH tokens that the Kyber Network, Compound Protocol or bzx Protocol recognized were different (their own custom addresses).  Due to which the interaction between multiple protocols on the Test Net was a challenge.A Global Team: Our team is spread across 5 cities from 4 different countries with 4 different time zones. Singapore, Mumbai, Pune, Peru, & New York. We all come from various individual backgrounds, but ended up working as a team for this hackathon. We range from Accountants, to Entrepreneurs, to Developers, to recent Computer Science graduates. Despite all of our variations, we acclimated to seamlessly communicate and actualize the product from the ground up effectively.  Installing the engine, especially on Mac OS, coming up with mini game ideas.  Detecting the audio levels and providing speech-to-text translation simultaneously without creating additional latency was one of the major issues that we had to overcome. Finding the anchor point for the text without any local SDKs available was a challenge. From a consumer perspective to enable accessibility,  we built two separate versions of the application, catering to both wearable and handheld devices.    Initially, the first hurdle we had to overcome was Wacom's SDK and trying to run the samples.  But once that was done, we found the Wacom, although could still use a few improvements, had much to offer.Built Withclarifaihtml5imgurjavascriptwacomwillTry it outwww.github.com      Submitted to    Inkathon    Created by  I put together the front end and the machine learning with Clarifai!Cassidy WilliamsSoftware engineer & developer evangelist at ClarifaiJoseph Song  The biggest challenge we faced was the deadline of this project. As we had around 24hrs to complete the project, we had to cut down on several areas which could have made the app more efficient and asthetically pleasing. Futhermore, with existing apps such as foodpanda and deliveroo, we had to find a way to make others understand the difference in terms of concept, and potential of our project compared to the apps that are already in the market.   Optimizing performance on as many devices and browsers possible.    There were problems with keeping the structure for the project composed and minimal since the problem has multiple dimensions of solutions. The smart contracts are made highly flexible in terms of handling all the cases of the problem statement and that was the major challenge faced. Lastly the integration of react and web3 also popped some problems here and there but was nicely tackled.      Not having any experience with the Google Cloud Platform, computer vision, or image manipulation.      Throughout the project, our team ran into several challenges regarding the hardware aspect of the implementation of the helmet. As this was our first time using the IOT framework, we found it difficult in the beginning to find a method of connecting to WiFi. When we initially tried to use an Arduino Uno board and connected an ESP8266 WiFi module to it, we found that the setup required to connect to Google Firebase required several external frameworks that we were unable to implement. As a result, we switched over to the NodeMCU board which had the ESP8266 WiFi chip already built into the board. However, since this is a much smaller microcontroller, the number of input and output data pins was half of that of the Arduino Uno. In order to counteract this, our team members learned to use the I2C protocol which allowed us to connect several sensors to only one bus and save the input and output pins.   Our biggest challenge was lack of time.We also were working in a distributed team and socializing at the Seoul hackathon, so there was lots of noise and socializing going on. It made things more fun, but also made it hard to focus on the project.None of us slept. :)  There were many challenges we faced during the creation of this project. The most frustrating was to arrange time from our hectic university life. we were going through quizes and assignments than sessionals, it was really hard to arrange time for it, but we had scheduled and few hours from each night and discussion with teachers in the morning, only for this project. Secondly, the first idea and modeling of that idea was really a challenge because we discussed thousands but can't get to finalized one. After facing all these challenge the reason behind the selection of this project was also the ease in creating it because we had our back in experts of the field.   We had some challenges integrating the UI and the backend. One major problem that we encountered is that we were unsure of how to convert command line arguments into simple buttons that the user can press to use the application.  Making the plugin work for cloud and server was biggest challenges. Lack of documentation for Atlassian conect addons further contributed to challenge.      Confluence uses its own "flyingpdf" for page export, which is a proprietary version of FlyingSaucer media converter (but has no exported API). We had to recreate an exporter to provide PDFs to DocuSign's API.  As mentioned before, the learning curve was the challenge. Coming from no experience with Arduinos or micro-controllers, it was very intimidating at first to begin, however, online resources and ease of use of the Arduinos on board capabilities such as the accelerator, made this manageable for us.  Trello is surprisingly flexible, and from the early start it was obvious that every person uses in their own way and wants to automate different things. So a big problem was how to provide all these options without creating a monstrous and hard-to-use user interface. I solved this by using a natural-language user interface, in which the user basically writes English sentences describing what she wants to do.  Some people have a lot of content in Confluence, so indexing it all in browser can be a bit of a resource hog. We've found a balance by indexing most of the recent data, and checking with Confluence for any extra matches (via API).Unfortunately Chrome extensions don't have very fine-grained permissions, so even though Thunkfluence is pretty secure because it's all on your computer, we have to ask for more permissions than we need in order to get the smoothest user experience. None of your data goes anywhere, though.    Every member of our team was a first-time hacker, with little web development experience. We learned how to use JavaScript and Sketch on the fly. Were incredibly grateful for the mentors who supported us and guided us while we developed these new skills (shout out to Kush from Hootsuite)!   Getting the excel data from Citrix environment.   Mobile integrationLCD message flash          We mainly highlight the problem of the age of the children. Our robot can be used for a wide range of ages, from 0 to 8 years old, but not excluding the other ages, where games might be redesign according their age.Besides that, understanding children was another interesting challenge we dealt with.      Everything was new to us. Also we both are from different city. We worked together using GLIP chat and meeting.  The biggest challenge we ran into was managing the time optimally in order to make up for missing one of our team mates! - who got sick on Friday night. We planned our tasks and organised our schedule to complete the project within the deadline, allowing a slot of time for polishing and debugging - which, we realised, was fundamental.A big challenges was coming up with an original idea and an entertaining gameplay. We had plenty of ideas but some of them, for a reason or another, were not feasible for a game jam. We still decided to aim high. In the end, we managed to complete our project, even if ambitious.Another one of the challenges was putting up with the difficulty of more people working on the same Game Scene given that the project was set up in a repository. We split up our own changes into personal Test Scenes and our Game Designer managed to merge everything together.    Time. Are there ever enough hours in the day when you know you or on to something beautiful that will enrich and help people in their daily business dealings. Also balancing operational expectation and what can technically be delivered in a specific amount of time. Keep the focus. There as so many great ideas and ways to assist business owners one can easily dive down the wrong rabbit hole. Technology. Technology can be temperamental. All in all, we have had a great experience so far and even the challenges we have encountered has enriched our journey.  We have an impeccable team that tackled every challenge head-on.   One of the major challenges while conceptualising the platform was that it would be difficult to make the platform popular as people would need the corresponding ERC20 token for the option they choose, restricting scalability. While exploring Kyber, we realised this problem will be solved successfully by integrating Kyber with our platform and allowing users to pay through it.One of the implementation challenges was writing the contracts such that no bad user input can get through, i.e., learning basic auditing to create a secure implementation. Also, another challenge was testing on public networks since we needed data from public networks like Kyber and Chainlink.  We didn't have much experience in building a webpage and we had a lot of difficulties such as properly aligning the elements together or having the elements display as we wanted too. We also had trouble understanding the javascript codes as we do not have much experience with the different languages. Despite of the lack in experience, our team persevered.  We need someone to help us to develop our app with objective-c/swift.  the geo local sponsoreshipsponserd by ScreendyWhat I learnedHow to solve some social problems What's next for Taxicoming soonBuilt Withapiazureccsscss3html5javajavascriptjqueryjsonmapmysqlphpscreendy      Submitted to    Ford Code for Taxicabs Mobility Challenge    Created by  Graphic design , css , javascriptIssmail Taoussiismail 18 years old from morocco , one family one dream (Geeks Family)I worked on the front end and some of the backend Ilyass SemlaliMostafa IdmachicheAbdeLaziz EL arassi  Help riders choose their taxi co-passengers, as well as where riders are picked up (on route v. via detour).Improve safety for women riders - by offering a way to ride only with other women, etc    As Spark is still quite fresh addition to IBM Bluemix, it has certain limitations. Currently, the code can only be executed as part of a notebook, thus there is no way of scheduling the runs. This was quite a discovery at the end of the time I had for the hackathon. What it means for My Perfect Weather, is that the presented weather days will slowly get out of date if Spark notebook is not re-run manually. I hope that IBM will address this shortcoming promptly.  The biggest challenge I ran into is balancing schoolwork, working on DogLog, and the other extracurriculars I'm part of, such as volunteering and playing piano. On a technical level, making sure the UI and UX was user-friendly was challenging for me as a developer. Thanks to the help of my sister, as well as getting other people's advice, the app ended up looking nice and is easy to use.  This was the first time we work on a such project that involves chat bots. The logic dialog in wit still in beta version and is restricted to some scenarios.  We were challenged by not having access to the target population to do user research with. So we did the next best thing - we performed web research to find forums where users were talking about their experiences (this is documented in our presentation.) There we found the frustrations and challenges users had navigating to find the case numbers they needed to fill out the form. We also conducted user research and testing with as many people as possible to remove further usability issues.       To identify the parameters that were required to be considered and to make sure that the time complexity of the algorithm is as less as possible.    Coming up with ideas. We finally came up with one after midnight. Productivity crashHad to use Google Translate API because we couldn't figure out how to get the API key working for Microsoft's Translator APIAPI Quota Limit  We need spend a lots of times to collect, design sprites for pet. Building game in blockchain quites different with casual games    Finding reviews was the hardest. Due to all the website's business models, none of the data was readily available to take from the APIsSpecial FeatureWe added a #HackRice Mode that enabled us to make "cooler" suggestions that would cater to the student crowd. :DWhat's next for WayfarerWe could only do so much of analysis over the weekend, so to make this the absolute Travel guide, we're hoping to implement more effective Machine Learning models to make better recommendations at higher accuracy.And who knows, the places recommended might even scare you.Domain Challengewww.IveBeenFramed.orgBuilt Withalchemyapiamazon-alexaamazon-web-servicesbluemixcssdomainhtmlibm-sentiment-analysisibm-watson-apilambda-functionmysqlngnixnode.jspersonalityinsightsphppythonreact-nativetwitteryelpTry it outgithub.comgithub.com      Submitted to    HackRice 6Winner                MLH: Domain.com - Best Domain Name                  Created by  Idea generation and product development.Built the algorithm of the recommendation system model.I also worked on the front-end design of the mobile application, user experience with Amazon Alexa and UI Development of the ivebeenframed.org domain.Rohit JacobDeep Learning ResearcherI worked towards integrating our app with Amazon Alexa. Took care of developing the complete skill. Also worked on getting the tweets data from twitter API. Also made a logic to get verified user account by name from Twitter API.Manoj PariharWorked on developing react native application (ios) for the above project. Also contributed to get data from API that was generated by performing the personal insight on the twitter and yelp database.Mahendra MhatreWorked on Yelp data cleaning in Python. Created PHP application API for mobile app to use the Yelp data. Worked on React Native Mobile App to show the recommendations. Performed Personality Insights on the Yelp Data and user's twitter profile data for better recommendations.Udit Desai    Generating models, 3D math computations, making sure that the phones are generated with their correct size, placing input directly into AR.  Because the skeleton extracted from OpenPose can be noisy, processing the signal and detecting events in a robust manner that works with anyone and in any conditions is quite tricky. We applied signal processing techniques to filter the signal and improve the detection of events.    Our biggest challenge was learning Python. None of us have any experience coding in Python, but we knew it was similar to Java, something we all have experience with, so we wanted to challenge ourselves to code in a new language. Another one of our biggest challenges revolved around working with Trinkets.io and having it crash continually. We ended up having to use different IDEs. Additionally, working collaboratively on one game and website can be difficult. As far as specific challenges with the game, we struggled with creating the methods and loops for the game logic, tracking the players score throughout the game, and developing a looping win condition to play again or leave the game. After determining the best way to go about creating the methods and loops, we were able to create a framework we applied to all of the questions.   I had to build this on my own without teammates, because we are having an exam tomorrow and everybody is busy with that. So I worked straight 24 hours to implement this. Meteor makes is great and most importantly fun to develop. I was familiar with C++ and Java before, so this was a real fun to do. It took me time to make videos and stuff because I did all individually in a limited time. But the hackathon was one hell of a fun.      Before getting the eCar solution, there are lots of great ideas during discussion. The thing is how to narrow down the scope and figure out which is the best. While research shows that auto-pilotted cars can be used to reduce safety concerns when driving in an increased speed. Before we really have an auto-pilotted eCar system in place, GoforCar team focuses on utilizing the current technology to provide the best integrated option and relieving the traffic jam during the peak hour. Besides, the other challenge we have met is how to simplify the process in reserving an eCar and carpooling with others. "GoforCar" team member came up with QR code from various solutions to make sure users only have to provide few information to get their eCar easily.  We had issues with setting up our WebRTC STUN and TURN server, transacting data through WebRTC efficiently, and we had a lot of frontend bugs and issues (e.g: navbar was very cheeky and animations were annoying). Accomplishments that we proud ofThe integration of the blockchain, dashboard, and chrome extension marks a programming milestone for the teamcredit to our perseverance, teamwork, and surplus of caffeine.What we learnedSohit, throughout the course of 48 hours, learned React.js and developed the entire dashboard.Both Alan and Yesh learned how to implement a WebRTC server and create a blockchain from scratch.Sarthak learned how to develop complex Chrome extensions with persisted data.As a team, we learned that we work excellently together, and that we hate JQuery. What's next for CommitCoinBeyond this hackathon, we see true potential in the future of CommitCoin; we're planning to redevelop it with a scalable blockchain on WebRTC and a better mining script. But further, we think there's great promise in CommitCoin's platform. Whether rewarding upvoted Reddit posts, StackOverflow answers, or any sort of online contribution, the possible applications are limitless for our technology.Built Withblood/sweat/tearschromenode.jsreactredbullTry it outgithub.comgithub.comgithub.com      Submitted to    Hack&Roll 2018    Created by  Yesh ChandiramaniSarthak NavjivanAlan ChangSohit Gatiganti  Installing packages and making the packages rasberry-pi compatible  Support of all custom fields on issue transition screen. Sophisticated approach for each field type is necessary. Web items provider for cloud version of the plugin.What's next forBoard Sharing, QuickFilters, DetailedView for issueBuilt Withactive-objectsatlassian-sdkcssjavajavascriptjqueryspringspring-bootspring-dataspring-mvcspring-securityTry it outmarketplace.atlassian.commarketplace.atlassian.com      Submitted to    Atlassian Codegeist 2017    Created by  Idea, Market research, Coordination of effortsAndrey DekhtyarAndrey SurayAlexandr Matyukhin    The most interesting challenge was to set up and fine tune the web socket based sessions as communication channels.      Even in our demos, lag consistently hampered our efforts to sound euphonic. The best way we found to mitigate this was timing the delay so that it is always behind by a factor of the beats per minute of the violin composition. This is a fundamental limit in all sampling based approaches and requires a more sophisticated predictive model to completely eliminate.The neural networks also had issues. The most basic one was training time since the neural networks took 10 hours to train on all of our datasets. The most crucial challenge was out of our hands though. We had several problems in making the char-RNN learn musical structure, as it would often play awkward combination of notes, spend large amounts of time not playing anything and then compressing dozens of notes in a few seconds. More training epochs helped remediate this problem, but the architecture still needs better forms of representing musical structure in order to be more effective.  The first H-bridge (SN754410 Quadruple Half-H Driver) wasn't suitable for the planned operating voltage of 5V. So we had to change to another one: L6203 DMOS Full Bridge Driver. Furthermore, we changed the motor voltage to 12V.Furthermore, we found out that developing for different Meteor versions is nearly impossible.  It was difficult to change directory, since processes terminated after a single command, but we decided to store the directory and track changes.  Error handling with incorrect commands was also difficult, since error messages have different formatting.    Communicating the user's actions to Alexa effectively and creating a syntax useful for gameplay.  Python parsing    The hardest parts were coming up with a logical encoding for patterns and tuning the HTM parameters to produce sensible output.    Building an interactive grid system with only javascript, html and css, smoothly updating between browsers.  Using a new device from scratch : it was an honor to be one of the first persons to play around the artik board, although it was kinda confusing at first trying to get things running I managed to make it work in the end. I had a lot of trouble with the initial configuration (I was moving the jumpers when I wasn't supposed to, for example) or trying to make the neopixels work (which I couldn't succeed, so I moved on using a normal RGB led to display the different thresholds).Creating something useful : the idea of creating something that anyone could use in a easy way, in any house anywhere, was one of the biggest challenges here. Trying to go in one direction when you can find a shortcut or an easier way to display your results (I'm talking about my frustration trying to use neopixels here!) - so sometimes it's better to think simpler and let the upgrades come later.   Finding a dataset to train our model.  The commercial weather API I use has a free version that limits requests that can be made per minute. I had to workaround this by caching results in DynamoDB.  Timing: Our team didn't discover the project until early January, so we definitely lost some potential time to implement bigger ideas. What's next for LunchlineOur interview with a school specialist revealed that many households fail to fill out the application because of illiteracy. Ideally, there would be a visual icon that would allow participants to "listen" to and fill out the application with audio-technology (screen reader, Google Voice Typing, etc.). Applicants should be able to track their application status online (similar to a very famous online pizza tracker...). Progress bar updates could be steps like Received >> Opened >> In Review >> Status Update Successful!At the end, the application can surface nutritional recommendations based on location, income, and household size.TurboTax authentication for easier income reporting/editingForm field validation: Users can receive an error or validation every time they finish completing a form field to give a tiny sense of accomplishment at every step. It also decreases the time and effort spent on fixing errors as they appear, rather than waiting to find out what went wrong when trying to move on. Optimized form fields: In-field, top-aligned form fields can provide quick scan-ability and visual clarity, as opposed to other types of form fields.Built WithcsshtmljavascriptTry it outgithub.comwww.lunchux.com      Submitted to    E.A.T. School Lunch UX ChallengeWinner                First Prize              Winner                Best Creative Design Aesthetic              Winner                Best Behavioral Design Elements                  Created by  User Experiencewww.laurenmacguidwin.comLauren MacGuidwinTechCody LandstromStrategy and Product ManagementKeaton BedellStrategy and Product Development at RazorfishVisual DesignJessica Zhang  It was very complicated to get a proper segmentation of the pupil because of how small it is. We also ran into some minor Google Cloud bugs.    Much of the technology stack I chose was new to me (I intentionally chose bleeding-edge), and the learning curve was pretty steep. This was my first exposure to server-side JavaScript, to functional style programming in general, and to Redux and RethinkDB. Attempting to learn and correctly apply sane architecture, idioms and best practices was demanding and often frustrating. I ran into numerous technical issues along the way, including the need to fork and modify both Atlassian Connect Express (improved compatibility with Express 4.x) and a storage adapter for RethinkDB (added authentication support).Technical challenges aside, the largest challenge I grappled with was deciding on the feature scope of my minimum viable product. I have a laundry list of features that I wanted to include in this first release, and found it painful but necessary to aggressively cut scope in order to make the hackathon deadline.     De nombreuses capacits techniques ont du tre dveloppes, telles que la programmation sur un microcontrleur, la cration d'un serveur sur apache, le dveloppement en quipe  travers GIT, etc.      we know the quantity of data we'll be dealing with and that's why we architectured our application in a way that is optimized for data read & write.using Lottie animations got our application to run a little slow, but that's a tradeoff that we are ready to pay when it comes to UI/UX.handling our application state became tricky as the code growsmessenger API Manual isn't good at all!    There is a need for activeness from UMKM to keep updating the product so that the content is always fresh and interestingThe biggest challenge we faced is when determining the closest tourist location with the best rating. Some locations that offer local products are also a challenge because we have to contact owners of local product in the form of snacks, heavy meals and drinks. This is a very interesting challenge because by highlighting features like this, the tourists who come to Banyuwangi can find out their destination without having to be confused looking for a place to sell typical Banyuwangi food or natural attractions that only exist in Banyuwangi.What I learnedLearn the literature needed to make the applicationLearn the culture of Banyuwangi so that the information provided by Ayo Banyuwangi is accurate and realWhat's next for Ayo BanyuwangiWe hope Ayo Banyuwangi becomes the solution to the problems that exist in Banyuwangi. Ayo Banyuwangi is a new thing that can advance the people of Banyuwangi with a touch of technology in the fields of tourism, culture, art and everything that is characteristic of Banyuwangi. Ayo Banyuwangi can promote Banyuwangi so that it can improve the regional economy as well as the surrounding community.Built Withbootstrapcodeignitercssgoogle-mapshtmljavascriptlaptopmysqlphpsql      Submitted to    Hackathon Pintar 1.0Winner                Hadiah Pintar                  Created by  johan bagaskaraBima AhidaRakha Hidayat  There are quiet of few challenges we have faced while developing this solution and some of them are:  We faced challenges while dealing with AWS Lex duration slots values  AWS Lex, does not provide an option to create copy of existing bot   Facebook has a default timeout where a response from bot is expected, hence there could be more option added in utterances (like wait) to notify front end to keep waiting if processing is taking more time then expected   There were occasion when AWS Lex web UI was not responding and we had to wait for almost an hour to resume our work  We observed, technical error occurred from Amazon Lex few times, which given an indication that some is wrong at AWS side. This user experience can be improved.    Integrating the front-end to the back-end, building the layout and design since our members are mainly focused on backend development.  I encountered various challenges such as figuring out a scoring system, how I would implement the enemies as well as how the user would control the spaceship.  The cocoapod we are using to allow us to take pictures and select from the camera roll at the same time stopped allowing us to take pictures so you can only add from the camera roll with it. We plan to fix that as soon as finals are over.  Controlling the electroluminescent lightson the helmet using Alexa commands, securing the Pi to the back of the helmet, and putting the microphone and speaker inside the helmet. Getting the Alexa voice service to work with the Raspberry Pi.What we learnedWe learned how to function without sleep.What's next for Alexa JARVISMore voice commands and a smaller and more mobile system that can be adapted to other helmets.Built Withamazon-alexaangular.jsapachefirebasegoogle-directionslambdanode.jspythonraspberry-pirelaytwilioTry it outgithub.com      Submitted to    WearHacks Kitchener Waterloo 2016Winner                First Place              Winner                Community Favourite              Winner                Top 5 Projects                  Created by  I worked on the Alexa voice skill service, hardware, and the backend using node.js and firebaseMilan DimicProduct design, helmet hardware, cordova companion app, nodejs backend hosted on AWSBill ChenI set up Amazon Alexa on the Raspberry Pi and contributed to the backend in node.js. Veljko Dimic      Accessing the mail server. Getting around the uwaterloo ip subnet.    Working with a backend solution that is new on the market and exploring the features can be hard. There is not much material available as well as other projects to use as examples. We've sorted it out using Google's amazing documentation as well as Google I/O videos and sample codeAcomplishments that we're proud ofWe're really happy to have acomplished building an app in a new ecosystem in a short amount of time.What we learnedWe've learned to work with the Firebase's core functions and integrate them in our codeWhat's next for HootChatIOS Notification Support1 on 1 conversationBuilt Withhootsuite-engagementiosiphone-sdkmvcswiftTry it outgithub.com      Submitted to    VanHackathon    Created by  Leonardo DeleonMatheus RuschelVictor Cotrim de Lima        There were many obstacles in building the faceband but one of the interesting challenge was deciding which actions we could use, since the actions were limited to the head. When we tested 'shaking your head at the beat' we all got dizzy in a few shakes and thought it would be more like torturing, so we changed the action to slightly tilting the head.We wanted to make a multi-play AR experience, however currently Spark AR Studio doesn't have any feature and resource for an (independent camera user's) multi-play effect. If coming updates can provide creators with such assets and resources, faceband team will definitely use them for the next milestone.      Working across languages: Python and JavaMerging main algorithm with UIlearning about complex machine learning algorithms  It was a lengthy process as I had been a house wife and I have a wonderful kid and much more wonderful husband.I had a zeal for Software Development but somewhere managing my family I lost the touch .This Challenge allowed me to reinvent that zeal for my work and I dedicate it my son Soham. Day went by studying Spiceworks SDk and how to use the api.I asked questions in the Challenge and substantially it cleared my doubts.The D day came now was the time to upload the project and time was less I started uploading the project on Azure but some how it didn't work out.Next came in my mind booking the domain and alas I am proud I did it.The project was complete and here I am writing the descriptions.My friend Abhishek helped me in this project and was a motivator.  We had a hard time navigating through the Atlassian Connect framework docs.          Export dynamic data using XML to MS Excel Dynamic step flowWrapping my head around the presentation of the formWhat I learnedDynamic Wizards are tough to develop; especially, when populating data across different tables in a UX friendly manner.Built Withcss3html5javascriptjquerypostgresqlruby-on-railsTry it outfood4students.herokuapp.comgithub.com      Submitted to    E.A.T. School Lunch UX Challenge    Created by  I was responsible for all phases of the project. Anees Merzibuilds pleasant user experiences  Setting up a serverSetting up asp.netSetting up rubySetting up herokuCollecting and deploying contentA site built from scratch  We ran into design disagreements based on personal preference and practicality disputes. In adition to this, our 3D model, which we could only print once, was too thin in the wind mill side parts and therefore fell apart.  Getting Open Data related to Disasters and Epidemics was a big challenge.Also, making accurate recommendations based on the responses was a bit challenging.    Overall the API was easy to use, but we had some difficulties with the implementation of the attachment API. We've filed a bug for it on the GitHub project. In the beginning there were some synching issues with my inbox, because of which we couldn't fully test the body import function.Accomplishments that we are proud ofWe've finished the Context.IO module in under 2 weeks.The site that we use in the demo video was built in less than 4 hours.The module is really powerful and plays nice with Drupal's contrib modules. We are really excited about it's potential.In 5 days we had 85 downloads for the module, and it's already reported in use on 3 sites. This is really fast adoption, that I haven't previously seen in my 10 year long Drupal carreer.What we learnedWe had a dependency bug with the Context.IO library and our developers had to freshen up their knowledge about dependency management with libraries in Drupal.Prototyping in Drupal goes really fast and is super fun (I guess that wasn't really news, for us at least)What's next for the Context.IO Drupal moduleWe've published a blogpost about the competition and the module to the Planet Drupal (an RSS feed for the Drupal community). We will be following up with another post about the demo video (also here attached). We'll do another video that is targeted at site builders.We would like to talk with the Context.IO team about community outreach programs in the Drupal communityWe want to still implement attachment handlingWe want to set up a beta invite signup page to check for interest for the prototype that we built. We believe there might be interest for it as a customisable B2B solution.Maildispenser demo implementationSelectively share emailMaildispenser is an application that helps people selectively share emails. With it you can set up filters that will automatically upload emails into an online archive space. Each archive space can be shared with a group of people of your choice. With it, you no longer need to forward your emails or give access to your mailbox. Access permissions are granular: other people only get to see the emails they need, and when they no longer need access you can revoke their permissions. Unlike the native gmail filters that will only forward when they arrive to your inbox, Maildispenser can also help you share old emails. Invoice sharingEarlier this year my assistant who helps me processing my expenses went on pregnancy leave. In the past years I had set up a large number of email filters in Gmail that forwarded invoices to her, now I not only had to redo those filters, I also had to forward a number of invoices that hadnt been processed yet. If you work with one assistant this is a bit of a hassle, if you work with a number of different people from odesk for example, this can be enough hassle to undue whatever time savings you made using an admin in the first place. You could maybe solve the problem if you forward these mails to a separate mailbox, but then you have to keep changing your credentials for it.With Maildispenser, you can set up a space for your invoices, you can either define a catch all filter, that will upload any email that mentions the word invoice. Or you could define individual filters that only upload emails that originate from a certain address and have a specific type of title, to upload invoices from each of your vendors separately. Once the system is set up you can then give your assistant access to your invoice space. When they are finished with the job, you can revoke their permissions.Email archiveA few months ago we fell out with a customer, as a pre-seed startup they had run out of money. They threatened us with a law suit and played other tricks to try to make us do more and more work for ever shrinking amounts of money. When we didnt give in, they deleted our sprintlogs and kicked us out of their ticketing system so that we wouldnt have proof of the work we did for them. Luckily we had kept the notifications we got from their ticketing system. Email threads spread over a few different inboxes, however, are not all that great when you need to piece together what happened in a project.With Maildispenser you could in this case reconstruct the comments history from the ticketing system. You can set up a space and add emails from all the developers that worked on the project. With a special field that extracts the ticket number from the email title, you could add the comments in chronological order to the right ticket.Who we arePronovix is a consultancy that supports developer-focused software companies:We integrate APIs from SaaS services like Context.IO, Livefyre, Gitlab and Brightcove with open source software (primarily Drupal) to make it even easier for developers to use those services.We provide maintenance and community support for the integrations.We help promote the integration in the developer community through blogposts and community specific outreach programs that play nice with the community and that are built on top of sometimes less obvious communication channels.We build websites for these companies that integrate documentation into their marketing site. This increases their search ranking, reduces support costs and improves ROI on sales.Built WithdrupalphpTry it outwww.drupal.org      Submitted to    Context.IO App ChallengeWinner                1st Runner Up                  Created by  Kristof Van TommeBalzs WittmannSoftware Engineer, Drupal DeveloperTams NagyJan MashatDina LakatosTechnical Writer  The time frame was short for us to build a perfecting website that is tailored to what we actually planed doing     Using my computer.  Every camera is different, and almost every variable is non-linear.  This makes creating solutions difficult.      While add-in commands support task pane apps, they do not yet support content pane apps yet.What's next for XLTools SuiteWe have submitted XLTools Suite to AppSource and waiting for it to be publishedWe will continue to migrate most successful tools from our VSTO add-in to XLTools SuiteWe would like to improve our marketing and sales in corporate sector, we hope Hack Productivity 3 and Microsoft can help us with itBuilt Withazurecsshtml5javascriptoffice-js-apioffice-ui-fabricoffice365resttypescriptTry it outgoo.gl      Submitted to    Hack Productivity 3Winner                Best Office 365 Add-in                  Created by  I worked on the script for the promo video and some of the materials for the submission.Maria BalobanovaI helped to develop UI for Mac version. Also added new functionality for the official xltools.net site.Vlad SereginI worked on the idea itself, architecture and the add-in development. I also did promo video and some of the materials for the submission.Peter LiapinA pessimist sees the difficulty in every opportunity; an optimist sees the opportunity in every difficulty.  Winston Churchill  Getting the initial serverless stack setup with AWS Cloudformation proved trickier and more time consuming than we thought. We investigated several solutions like serverless but couldn't find something that fit so we ended up building our own stack.  Cloudformation coupled with Cloudfront can be very time consuming with every small configuration change sometimes taking 20 mins to run.  On the product side, getting the design of the rule builder right was also very challenging and took a number of iterations to get right.     For half of our team, this was our first Hackathon and we had no idea what to expect. We spent the first few hours going through many different ideas and getting overwhelmed with everything. Laravel, the main component of our project, was also completely new to most of us.       The first challenge is to formulate the matching criteria. Additionally, the second difficulty lies with the text analysis and finding APIs that are most relevant in determining the level of empathy. Accomplishments that we are proud ofEmpathy Match does not simply matches people based on similar interests and personality. Instead, I am proud that our system is able to read peoples experience and connect them based on their state of emotions and intrinsic needs. What's next for Empathy MatchWe will be submitting this project to Betwixt Festival: Art & Bytes, a digital and interactive art exhibition held at the Art Science Museum on 26, 27 and 28 February 2016. During the exhibition, we will be connecting real users through our Empathy algorithm.  Built Withibm-watsonjavascriptmeteor.jspersonality-insightsTry it outgetbimy.meteor.com      Submitted to    Hack&Roll 2016    Created by  UI/UX Designer ying ling tanYan Sheng ChooTechnopreneur and product developer on a variety of professional, freelancing and startup projectsRen Sen HoTong Wei Teo    The first and the foremost biggest challenge we had to face was finding meteor developers from Pakistan. Which was the fact that led us to develop "DevPost", we thought long and hard about the idea which would pay off and can also be converted into a sustainable business.What I learnedHow difficult it is to find developers having the same interests as yours around you.What's next for DevScoutIt is going to get bigger, faster and smarter. :)Built Withaccountsaccounts-githubblaze-layoutbootstrapflow-routergoogle-mapsjavascriptjquerymeteor.jswebstormTry it outgithub.comdevscout.meteor.com      Submitted to    Meteor Global Distributed Hackathon     Created by  Shehroz RashidEbtihaj KhanCivic Hacktivist | User Experience | Program Manager, Code for Pakistan | Football fanaticMuhammad SayyamMohsin Tariq         We had several cosmetic problem in building the IOS app, mainly due to a lack of familiarity with the language. Also, linking front-end and back-end had a lot of miscellaneous work to do. Also, there were a lot of configuration to be set hosting the app using AWS.  At first, figuring out whether or not to use Sendgrid was an issue but since they don't support AJAX calls, we stuck with mandrill.  Secondly, deploying to bluemix wasn't too easy either.  But once we figured it out, it was a lot smoother than initially thought!What's next for FoundWe would like to incorporate GPS locations as well as better support for found and those unfound and aid those in rescue missions with a phone app to help them take pictures on the go.Submitting ForFamilies, friends, colleagues of individuals impacted by the disaster.First responders and backend support teams, including healthcare organizations.Built Withbluemixclarifaicss3html5imgurjavascriptmailchimpTry it outgithub.comfound.mybluemix.net      Submitted to    Bluemixathon: Operation Rescue & RecoveryWinner                Bluemixathon Category Runner-up Awards  Families, Friends, Colleagues of those Impacted:                   Created by  I deployed, did the email sending, and submitted to bluemix!Joseph SongI built the front end and integrated Clarifai!Cassidy WilliamsSoftware engineer & developer evangelist at Clarifai  Understanding the Office365 API, properly catering for async tasks, designing the architecture of the addin.      Short allocated time to develop application, meet new people to build a team with different knowledge, experience and skills , time management. get the data of shelter site in Holy Places     The first challenge we ran into was the time frame, we are lucky enough to complete our registration before it closes but the time frame took it cause on us which made us work overtime.Secondly, materials gathering its not as easy as we thought.  We searched for materials from numerous journals and surf the web for information.Thirdly, only the API provided by the organizers are to be used of which we don't have language translator API. More so, the available API's are not free. But all effort were made to make the current app and website good enough to         Nobody in the team speaks Finnish, so we could actually face the problem ourselves.At some point, an intermediate version of the app was often asking if a user likes to party, though no relevant jobs were eventually found.The dataset is quite noisy so extracting keywords was a challenging task.      The tricky part here is training the LUIS datasets and using them inside the dialog chain. Yet another task is to authorize the changes made on the site, certainly identifying the Logged user details and verifying that user's privileges in the site.   We had a number of challenges we had to overcome as a team. Firstly our hardware IoT devices were not behaving very well when being connected to the cloud. This did delay our efforts on the hardware side at the start. Another challenge was to make sure that all the developer machines were up to date and could be used for development. As we had a mixture of operating systems this caused some issues. Finally the challenge of staying awake enough to try and develop as much of the application as possible was an obvious difficulty we did our best to cope with.   We had no previous familiarity with android and the SDK as well as performance issues with image processing (frames freeze).    Finding and processing imbalanced datasetsRaising the accuracy above the previously researched level  Currently Microsoft Team Bot only receive messages that are directed to the bot with the mention tag @Live Chat. This makes the dialog on the thread in teams a little more cumbersome than it had to be, because every reply the customer representatives makes, they must start with @Live Chat followed by the message they intend to send to the customer.   While our code functioned effectively as individual components (database management, algorithm design, API and front-end), integrating them into a full stack with the flask interface turned out to be our singular defining problem through the hackathon. There were often inconsistencies across the data types and models that were used, making the internal transfer of control and information difficult. This integration issues were exasperated by difference in versions of softwares as well as difference in syntax across the programming languages combined in the stack. Furthermore, we also experienced issues in adjusting the dimensionality of our neural network input and the time complexity of our relevance algorithm (we had to restrict our articles to 7 to ensure manageable speed).   Because we decide to create FOUR games in 24 hours, it took us a lot of effort to design the user interface and find the appropriate libraries. We were tired, but we are also glad to finish all of them!Facing two similar libraries, swing and processing, we had to decide when to use one of them and not the other. We realized the advantages and disadvantages of each library. We learned how to collaborate these two libraries!Accomplishments that we are proud ofWe provide you with a content user gaming experienceWe can provide you with your own feedback when you finish playing the game.What we learnedWe learned a lot from using similar but still different libraries. For example, our teammates argued about whether we are going to use Java Swing or Java Processing. We understand the strengths and weaknesses of both these libraries.We gained a better sense of how to provide a better user experience!We also learned to be more sustainable by developing this game!Having a strong team collaboration is always the BEST thing!!!What's next for Sustainable BadgerDesign a more beautiful user interfaceProvide more up-to-date information in the gameSpread it to the world!Built WithgamingguijavaprocessingswingTry it outgithub.com      Submitted to    MadHacks Carbon 2019    Created by  I made the map user interface using Java swing package. My teammates mainly used processing, a third party graphics library, so I also learnt how to integrate processing into swing.Zhaoyi313ahalxq LiCarrieChen0524Lingzheng He    The main challenges we faced were related to Augmentation and Realtime collaboration using Wacom. We did our best to overcome those in the time limit          Model selectionLogic for model  Working with micro controllersCollecting information of the vehicles and how we can implement this technology.Cost reduction, how can we minimize the cost of production of this deviceFixing the slightest bug  The first challenge was knowledge - most team members didnt have prior knowledge of Facebook APIs and had to learn on the go. The second challenge was teamwork - beginning stage was difficult as people came with different ideas and different cultures, and communication issues arose later into the project because we didnt distribute work between teams well enough.  Unity does not support hosting an HTTP server that can handle incoming requests and it was extremely difficult to transfer information between the phone and the intermediary server.To establish a link between the Unity application and the Alexa skill, we needed a way to know which instance of the app was making requests in order to delivery information to the correct device.We had trouble with Unity resolving some Vuforia components, preventing us from working on multiple computers and merging our changes.When creating 3D models of all the planets, we had trouble resolving their textures and did not know how to create .mat (material) files that could be used.During testing we had issues using a tunneling reverse proxy (ngrok) as it repeatedly crashed and was unstable for our uses.The C# environment that Unity uses caused us some troubles as we had never used it previously.Our Heroku requests often failed because the servers were set to fall asleep after a set time periodChallenge 1 Fix:We solved this by creating two different servers that could handle HTTP requests and allowed for the information from the Unity application (Planet) to be passed on to the Alexa skill. This was then processed through the various intents for each piece of information we wanted to deliver to the user (planet description, orbit time, diameter, and distance).Challenge 2 Fix:We used a server to bridge Alexa and the app in order to generate a unique UID that paired both of them in their own isolated instance. Alexa creates a one time alphanumeric seven character sequence that it then prompts the user to enter. This initiates a link between the two.Challenge 3 Fix:We were forced to use only one computer to run Unity on as it would compile properly on others.Challenge 4 Fix:We fixed this issue by learning how to convert satellite jpg or png images into that specific file type so that the .obj files would accept them as an overlay and so the planets would appear in color.Challenge 5 Fix:We deployed to Heroku to mitigate the issues we had with ngrok.Challenge 6 Fix:We learned the nuances of the language and restarted Unity which fixed a lot of the issues.Challenge 7 Fix:We used a Heroku App called Kaffeine to ping it every 30 minutes and keep it always running.  The most obvious challenge would be coming into this hackathon with just under a week to go! We knew that would be hard but had hoped to attract developers and coders a lot earlier in the process.As we are in our infancy in the tech/hack space a big challenge was knowing how to even articulate what we want to do and achieve. We were fortunate enough 4 days out to find the most amazing and tirelessly committed word press guy in Kenton!Naturally with a team across the globe, and me having to travel in the middle made it difficult but the team were so committed and motivated, I was humbled to see how far they have gone and how much they have put into this. We are a leaderful team and that was key when we handed over from HK, New Jersey and Sydney!We used @slackhq throughout and it was very helpful keeping us in touch and on track. We tried to reach out to seek more support from the LGBTIQ travel community but alas our deadline was just too tight.  We have limited skills as a team in design and coding so that has taken much time, we would hope moving forward we could benefit from having quality production but hope our work is still able to convey the larger vision that we have re coding and production value etc.Another challenge given Planet Ally is so information-driven was access to updated information from NGOs and official government websites etc. We used tweets and DMs both on Twitter and Facebook to crowd source information and access to data and reports etc.Going forward I would like to take the time to verify information and accuracy with trusted local partners, consulting with our global advisory and conducting interviews on the ground etc.Overall, we tackled a big project with a large scope, over 3 continents, so consistently raining in and focusing on making something that worked and was presentable was hard, but we did it! I am so proud of every one of our team, who absolutely went above and beyond and now are all keen to take Planet Ally to the next step!    Making things work across Windows, Linux, and OS X: all of which were represented on the team.        I have faced many challenges in the development of Free Feet, as being a young woman in STEM, I have always been in the minority of my peer group. I have faced the challenge of overcoming the barrier of youth, as in professional situations, such as networking events, it is hard to ensure that you and your ideas are taken seriously and that your work is appreciated for its full value. An example of this I like to share is a networking event which I attended recently, and because of my age, I was asked if I was looking for one of my parents. I thought that this was certainly a situation in which I was not being taken seriously, and overcame that challenge by explaining my presence and work. I have also faced the challenge of balancing my work and school life, whilst working on the development of Free Feet, I have completed the A-Level equivalent exams (Irish Leaving Certificate) and earned my first choice course in University (Biotechnology). I thought that this was quite a challenge, as I had to balance my time and resources very carefully, but it worked out for the better in the end!I also faced the challenge of learning the technical skills necessary to build the prototype of the device. Free Feet is a device which I built myself, at my kitchen table. In order to do this I needed to learn electronics, soldering and stitching skills, and they were quite the challenge to master!  The image size that our GPU can process is very limitedThe cloud processing time is not stable because the hosting's bandwidth is not so high.       Using Shareabouts, Wilson and I have discovered the woes of hacking into a platform that uses many levels of abstraction: Django, Handlebars, Backbone, Postgres, Postgis, etc etc...Also, VTA data is not very easily accessible. A lot of it is directly stored as variables in javascript files and not in any template forms.    Nothing at all  -Building the simulation environment-Using the NFC function for our purposes-General programmers stuff  The biggest issue I ran into was browser support. I initially had an issue with the code becoming overwhelming with callbacks. I decided to use promises to clean up the code. Which worked well but while testing I found that IE didn't support the promises API. To solve this I added in a polyfill and it has been working very well since.  Both the accuracy of location and sending out the call quickly were important to get emergency services to the user as rapidly as possible. Seconds can make the difference between life and death in some cases, and we had to ensure we could balance these two components to minimize response time. By allowing the app to send new information as it became available, rather than all at one time, we were able to allow near instant calling when the app is opened.    Multithreaded Python programmingBuilding a phone emulatorDeveloping our own network protocolEnhancing reliability and data consistencyDesigning with UI with high degree of usabilityWhat I learnedBuilding large scalable systems is hard. Design is hard.What's next for SmartifySecurity, Traffic encryption, Horizontal scalability through automated server provisioning, Redesign to further enhance UI/UX, Full-integration of MMS.Built Withazureexpress.jsfirebasejadejavascriptmongodbnode.jspythonsasstwilioTry it outgithub.comsmartifyapp.co      Submitted to    MHacks 6Winner                2nd Place              Winner                Best Use of Twilio                  Created by  Sean BaeI love building thingsColin King    The wallet.ethereum.org front-end is built on Meteor, which was troublesome to use in a serverless Brave extension context. We had to patch it to work in chrome-extension:// contexts (since Brave extensions use the same protocol as Chrome extensions). We also had to insert some hacks to make it possible to launch the 'Send Funds' page from a button click with a pre-populated address, since Meteor doesn't allow navigating to an endpoint unless the main app page is already loaded. Figuring out how to bundle geth and the front-end in Brave was also challenging. We didn't want to check any binaries or obfuscated javascript bundles directly into the source tree so we had to figure out ways to download these components at build time.  We had some trouble at the end with launching geth wallet creation as a subprocess and passing in the user's wallet password securely. We also couldn't get ledger nanos to work with testnet.      There were many challenges in this project. Some of the more challenging ones that took way longer than I anticipated were;Developing a LUIS Model for the types of queries and intents that I wanted to be recognisedGetting Device to Cloud IoT Messages from the IoT Hub through to Table Storage and Power BI. Lots of reading and lots of failures until a configuration on the Event Hub saw everything fire up.Working with the available memory on the Respeaker Core embedded Linux device and the version of Python offered meant that I had many failed attempts with different Python Libs to be able to send Web Requests to Azure with the differing payloads and authentication methodsAccomplishments Im proud ofIm very proud of the entire end-to-end solution. As an experienced Identity Architect Ive built many Identity Management solutions for customers over the last 20 years. The concept of being able to talk to an Identity Management backend and have it talk back to me still amazes me even though Ive been working on this for a couple of months. Approaching the crazy concept initially I thought my ability to actually build it was low. After working through a high-level solution and breaking it down into functional components and then working through them individually my confidence grew and I built a working solution.What I learnedBuilding this project exposed me to numerous Azure Services that I hadnt previously used as well as using some services in different ways. New services I leveraged and got my first exposure to are;Azure Cognitive Services Speech to Text - Used to convert the voice query to text before being submitted to LUISAzure Cognitive Services Text to Speech - Used to take the search query result and convert it to Speech to be spoken back to the requestor Azure Cognitive Services  LUIS  (Language Understanding Intelligent Service) used to identify the entity to query and the entitlement to evaluateAzure Event Hub - Sending IoT Device to Cloud messages through he IoT Hub to Event Hub to Stream Analytics. Azure Stream Analytics - Taking the Device to Cloud Messages and outputting to Table Storage and Power BIAzure Table Storage - Storing events for auditing/reportingAzure API Management - Integration of Lithnet MIM Service REST API Python - This was my first ever use of Python. The Python script on the IoT Device listens for the Wake-Up Word and interfaces with Azure Cognitive Services for Speech to Text and Text to Speech along with the Azure FunctionWhats next for the projectThe project is currently essentially read-only. The information that can be retrieved and the pre-requisites to make it available mean that it doesnt consider context or authorisation. Id love to implement voice-based authentication into the solution so that the speaker/requestor is identified by their voice pattern (using Azure Cognitive Services Speaker Identification API) and authenticated to Azure Active Directory. Based on the authorisation level of the user higher privileged requests can be initiated. (e.g. Disable User David Bowie, or Create a new user account for Colin Furze). Id also like to enable the solution using Azure Bot Services. The way the solution has been designed allows for this. This would replace the need for a device to speak too, and provide an easier implementation path for the Authentication and Authorisation functions. The Bot could also be enabled via Skype for Business/Microsoft Teams. Built Withazureazure-api-managementazure-cognitive-servicesazure-functionsazure-iot-suiteazure-key-vaultazure-language-understanding-intelligent-servicelithnet-mim-service-rest-apimicrosoft-identity-manageropenwrtpower-bipowershellpythonTry it outgithub.comgitpitch.com      Submitted to    Azure IoT on Serverless Hackathon    Created by  My Voice Assistant for Microsoft Identity Manager is an Azure IoT Business Solution that Ive designed and built using Azure IoT, Azure Cognitive Services, Azure Serverless Services and other Azure Services along with Microsoft Identity Manager that empowers IT Support staff to query Microsoft Identity Manager for common IT Service Desk, IT Admin requests.Darren RobinsonExperienced Identity & Access Management Architect  Independent Craft Beer Brewer  Thought Leader & Trusted Adviser  This was a very complex hack, and we ran into obstacles across every form of technology we worked with. From the first night, we realized we had to SSH headless into a Raspberry Pi and got stalled with running even the most basic scripts or checking that the photoresistor circuit was even working (it definitely wasn't, and had to be redone three or four times with help). Claire ran into issues with multi-threading and controlling the flash on an Android device to communicate the light data, and getting the photoresistor circuit to work. She had to also find a way to package the information and pass it to Ben's portion of the hardware. Ben ran into issues such as researching Track 1, and Track 2 data for credit card magstripes, and coming up with the technology to generate the electromagnetic field, and doing a lot of prototyping and testing to ensure the data is being emitted correctly.  I am still struggling with the login animation, apparently Safari on iOS doesn't like to animate widths the same way every other browser does.  Fitting into the form factor, debugging code on a Raspberry Pi A+    It was my first project built with this technology stack and it wasn't easy to get started)Handling differences in JIRA Server and Cloud API, like license checking and entity properties  Converting the EDI file to human readable format was a challenge. Since the output of any EDI gateway is a file of a type *.edi and of a format which is not readable by any user due to its complexity of structure (ANSI-X12), parsing through them is a challenge within UIPath. To overcome this challenge, we developed REST API code that retrieves the invoice data from the database and translates it into UIPath readable data format as described above.What's next for EDI ConnectorTangentia Automation and Tangentia EDI consultants are looking at beta customers for the UIPath EDI Connector and will be looking at expanding the scope of the connector from the existing Accounts Payable (AP) process to cover the entire gamut of Order to Cash and Procure to Pay processes within the enterprise. With Tangentia's deep experience in EDI and B2B, we should be able to lend a lot of value to UIPath implementations especially in Retail, CPG, Government and BFSI.Built With.netansi-x12apic#ediedifactjsonmysqlrosetta-netTry it outgithub.com      Submitted to    Power Up Automation    Created by  Afaq QaziVijay ThomasAneesh Swaminathan      We ran into many challenges, first we started late but thank goodness the deadline got extended . We could not create working dispute mechanism, which will be next steps for our project.  Too many.        Mostly just the typical Android development issues - targeting multiple devices/screen sizes, and dealing with the sometimes finicky Android Development Studio IDE.    Getting spark to work locallyUploading codes onto bluemix and integrating backend and front end  Our first step was figuring out how to detect lead in water. Through research, it seemed very difficult to find a chemical that reacted with lead safely and in a clear way. Fluorine reacts with lead (but vigorously). Chlorine reacts but it is endothermic. We thought about Sodium rhodizonate, which police use to determine bullet residue, and it causes a color change. However, that is not easily detectable by arduinos.Also, we had to figure out how to process that information. Luckily, we took classes on Arduino and The Internet of Things, which gave us the idea to use the NodeMCU with the Arduino coding language.Lastly, we had to figure out how to display it to the users. We created a website using HTML, which I (Sunny) learned for the first time today. Making the graphics and links work took some time, but we overcame the challenge.There was difficulty publishing the code we wrote for the website, even though it works on my computer. So, we included the raw files that can be downloaded onto another computer.  Database not ready  Being a Mechanical Engineer with no software background making an app was the biggest challenge i ran into      Initially it is hard to decide what to build which actually help user to assist in traffic related decisions but then IIndore Traffic Friend concept comes into mind. This application is under continues development. Phase I development is already done. Phase II development contains lots of features which requires hard mathematical calculations and image processing features as well as some features requires us to collaborate with traffic department which currently seems to be challenge to us.    Working on This project alone was one of the hugest challenges for me. Entire time I was doing it I kept thinking how can I leverage machine learning and different things to create a recommendation for travelers. Since that something that I really struggle to create that. For that later Im using right now I entire time I was doing it I kept thinking how can I leverage machine learning different things to create a recommendations for travelers. Since that something that I really struggled to create that. For that later Im using python  but since its not fully fleshed out yet I havent integrated it into my application. For now Im doing recommendations by hand. And many more.            The path was not at all easy. At the beginning, We implemented the whole concept on arduino uno and everything went just perfectly. When we switched to arduino 101, Problems kicked in. It was heartbreaking to know that 101 does not support Software serial which means we cannot use GSM Module on 101 and our whole project came to a hault.Then finally we decided to connect the GSM Module on arduino uno and connected the uno and 101 through I2C protocol.  Originally I needed somewhere to host the images, sounds and JSON file. Eventually, I found that I could use a git repository through GIT hub to host these files.  Scoping down the problem to deliver a proof-of-concept that accurately illustrates the potential business value within the given timeline  We fought a little with the JavaScript and CSS Confluence ships with. But overall it was very smooth sailing.What's next for Reader for ConfluenceWe plan on gathering customer feedback internally as well as externally and want expand the functionality from there.Built Withatlassian-plugin-sdkjavascriptreactTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist 2017    Created by  I (as the new product manager) am very proud of the team and how much they accomplished in such a small amount of time (barely a week).Jens BeckerLukas KorteTim Kolberger  After about 8 hours, we had to really simplify, basically redo our program, because we were getting too deep into something that we couldn't figure out. It was also hard to keep everyone target.  We collected so much data for 24 hours we couldn't process the data in real time in the browser.We struggled with javascript because none of us specialize in it.We wanted to collect the geolocations of tweets about cryptocurrency and map them in real-time, but crypto enthusiasts are averse to geotagging their tweets :/      Since I am trying to code it by myself my ideas kept changing so different things were getting implemented and also it was hard to find the flow of the whole app.      Stdout is slow. Super slow. Really killed my FPS.     It is hard to find royalty free sources about health symptoms and conditions.    Neither of us were very experienced with socket programming so there was some learning curve with that.We also spent a lot of time trying to host on AWS with Apache and it turned out to be more difficult than anticipated.  Not enough power supplies to power up our Raspberry Pis. 2. Not enough hardware knowledge to build a wireless card. This demo is a proof of concept.    getting the server running but was aided by a mentor from Google to get it going.Creating a decent looking rapid prototype on an iPad (Self-Challenge) with no experience with the app.Learning python, Rest architecture, JSON.Realized we can't outfit the entire school. Figuring workarounds for interferences.       Having no example data or mocked API from GE Healthcare, we built an API simulation based on the data given.On a non-technical level, we were forced to translate medical terms to easy-to-understand descriptions. The mapping of different treatments to colourful storylines required a lot of imagination and creativity.  Well, yeah there were a few but it was a part of the learning curve.In building this app, i learned loads of stuff core to iOS file management and using file extensions.        Initially, it was little bit challenge to understand how the Alexa Presentation Language works and how I can use it. But once I figured out and understood, it was a pleasant experience.  Originally at the start of the hackathon, our team was set on creating an AR app using the Unity platform. We attempted to use technologies on Unity such as Vuforia and Mapbox in order to create an AR camera and to keep track of location. However, after 15 hours of hard work and setting up almost every component of the project including database set up and UI, we failed to make progress towards to the core idea of our project - the ability to stick marks to a geographic location. As a result, we scrapped what we had done on Unity and decided to just use Swift to perform what we needed instead. After making the decision to switch to Swift, we encountered many other challenges as well. One main issue was figuring out how we wanted to store our data. In our application, marks were sets of 3D colored spheres which were too large to store into a database. Therefore, we had to spend a long time making design decisions, because we wanted to keep efficiency and scalability in mind to make it easy to scale the product. For example, some troubling decisions included deciding what properties to store for each mark the user creates and within each mark, how the structure of the data should look.Another big challenge was that the hackathon room itself had no GPS signal. This made testing difficult, since one of the main features we wanted to implement was how a user could only see a mark if the mark is near them. In addition to frequently going outside of the main room to perform tests, we also had to figure out a way such that marks are presented locally so that it is demoable.Unity gave us a hard time before we had to switch to Swift    Alternative messaging applications contained rate limits that prevented effective communication from working, thereby limiting the options of messaging platforms that we had. Also, due to the high load of our communication, the Whatsapp module frequently encounter obscure errors.    The largest problem we faced was the Connect framework itself.  The ways you can integrate with Confluence is limited compared to what is available in the server version.The good news is that Atlassian has been working hard to provide more integration points within the framework and responding to requests for improvements.  There is always more to do, but we think Atlassian Connect has advanced enough now that we can provide a solution that meets our customers' expectations.Accomplishments that we are proud ofWe assumed a risk of changing the technology in the client, moving our stack from Backbone/Underscore/Browserify into React/Webpack, and we're pleased to say it was great!  Even with large experience building client rich UI's, we were faster and more productive: we dealt nearly all the time with business issues instead of technical issues.What's next for Comala ApprovalsThis is just the 1.0 release, our work has just begun.  Ultimately what we build next will be driven by user feedback.  So let us know what you like/dislike about Comala Approvals and help make the next lot of features even better.Built Withacebeer&goodwinenode.jsreactrediswebpackTry it outmarketplace.atlassian.com      Submitted to    Atlassian Codegeist 2016    Created by  UI/UX developerMichael RegaladoShannon KrebsCTO @ ComalatechJuan AriasGorka PuenteProduct Manager @ Comalatech    Disrupting a traditional industry was never going to be easy. The main blockers we faced revolved during the idea generation phase, where we were trying to formulate new ideas that could revolutionise the accounting industry while staying within scope.  Live streaming video is super hard  Trying to capture the cheerful Meggie in an App!  We had to study the underlying design of Atlassian Connect framework. We also have to master the JIRA Server plugin framework to provide as much as possible the facilities that Atlassian Connect provides for developers.          We ran into one main challenge, how to interact with the MySQL server from Java. This was because they are both different programming languages. After hours of research we discovered Java Database Connectivity or JDBC and were able to use the .execute() method to insert data in our table to send queries to the server.Accomplishments that we are proud ofThe two biggest things that were are proud of are the MySQL database and the machine learning algorithm. The MySQL database is a large accomplishment because it not only interacts with our Java program and makes our idea scaleable, because it can hold a very large amount of data, but it is also hosted by Amazon Web Services. Because it is hosted by AWS it is redundant which means that it will never go down and it can be accessed from anywhere in the world because it is in the cloud. What we learnedThe two biggest things that we learned is how to work as a team on the same code project and how to utilize multiple resources together. This was our first hackathon for all of us and we learned a lot about teamwork. For example, we learned to listen to each other's ideas, we learned how to plan and create separate objects that fit together, and we learned how to appreciate each others skills. Also, we learned how to combine multiple resources together to create a great project. The ProBot is a combination of MySQL, Amazon Web Services Relational Database Service, and Java.What's next for ProBotthe technology behind ProBot is extremely robust, scaleable, and applicable towards many real world applications. For example, the MySQL database part could be used the store users data, the machine learning algorithm could be used to teach computers how to complete tasks with patterns own their own, and the Java automation program could help users with mundane tasks across their computing environment. The main vision for this technology is to be able to have computers interact with web elements and be able to learn from their experiences.Built Withamazonclassconnectivitydatabaseeclipsejavamysqlpro-profsrds-relational-servicerobotweb-servicesTry it outgithub.com      Submitted to    TeenHacks LI - Spring 2019Winner                People's Choice Award                  Created by  I did a lot of research on web scraping and I also learned about web crawling. I gained a lot of experience all around during the process of this project and worked on the logistics of the quiz and assisted with the Devpost and google slideshow. James EdomI studied all the capabilities of the robot class in order to find an applicable application to our project, after determining that it is viable, I used the robot class in order to preform tasks using the mouse and keyboard to accomplish our goal of going thought each question on the quiz and storing into the database.Logan BaderI worked on the MySQL database which allowed us to develop our back end server, as well as researched and started the code for future advancements of our program. I also created our slide show presentation and configured the screen recording and editing software we used.Emre GuvenilirI initialized the MySQL server in Amazon Web Services and focused on connecting it to our Java program so that we could send and receive data to and from the server.Lucas Ahrens  We had some trouble with building the website with React.js because it was our first time working with it. We also learned Figma which we weren't used to, and also other design techniques on our group member's iPad.  Our biggest challenge was using python's sql orm to store our data. In general, integrating the many libraries we used was quite challenging.The next challenge we faced was time, our application was slow and timing out on multiple requests. So we implemented an in-memory cache of all the requests but most importantly we modified the design of the code to make it multi-threaded.  There were a couple difficult algorithms that we had to write, involving some difficult mathematics to figure out.  Trilateration functionality on the java server was a difficult algorithm to implement. To get an accurate location out of the trilateration algorithm, we had to figure out how to convert latitude and longitude GPS coordinates to useable distances in a pseudo-cartesian coordinate system. For the mobile app, getting the distance between the user and the wifi router proved to be a challenge. We had to manually calculate the distance using the strength and frequency of the connected wifi signal. In terms of networking, matching BSSIDs to IP addresses in order to find unique connections over the entire network was a big task, given the massive size of the network. It required a lot parsing ARP information and finding suitable _ insights _  from that data.    Scope creep was a big one. It was hard not to try to build everything and then some. This was an extremely interesting use case to develop and we hope people who see its value will reach out to us to discuss it further.Also our front-end guys took this opportunity to learn react-native for the first time (great job X!)  Using react was fun, but also a learning experience.  We used radium for react styling.Inserting PDF information into mongo documents.React styling seemed cumbersome, but resulted in very modular components.                openweathermap.org limits us to use their APItweeter limits us to use their APIdifficult to extract information about relation between tweets and airportgrouping Mentions into Events (how to know that 2 tweets is about exactly the same thing)  The first challenge we took on was to write our own Connect framework implementation in Clojure. We kind of reverse engineered the existing Java and NodeJs frameworks into a Clojure version. We are glad we did this for the full control it gives us for that part.Another big challenge, that is completely unrelated to technology, is the place where the data lives. Suddenly the data does not live inside a customer's JIRA, but we are responsible for it. We must do the best we can to make Relations for JIRA a safe place for our customer's data. Legislation from all around the world will play a big role in this, as we are storing sensitive and personal data.Other challenges include the limitations the Connect framework imposes on our work. This is of course all relative to what we are used to in writing for Server add-ons. Connect is great as an idea/architecture, it just needs a bit of work on both Atlassia's side and the add-on vendor side...Built Withamazon-web-servicesatlassian-connectauiclojureclojurescriptdatomicre-framereactTry it outmarketplace.atlassian.comaddons.avisi.com      Submitted to    Atlassian Codegeist: Add-on Hackathon    Created by  I worked on the user experience design and user interface design. My main task was making sure that everything looks and feels integrated with JIRA cloud. I basically translated AUI and the JIRA interface to wireframes and visual design.Maarten ArtsPassionate about UX Design, Product development and Start-upsCreated the AWS Cloudformation scripts and the continuous deployment scripting using Bamboo and CodeDeploy. Dockerized the Datomic layer and deployment via AWS ECS. Implemented the daemon code for the Clojure jre process and also implemented the onboarding wizard. Currently working on a legal framework and the drip feed marketing.Gert-Jan van de StreekSoftware developer, founder and owner of Avisi. More ideas than time. Love to work for the best, with the best.I played client, critic and cheerleader alternately...Yanne VeronneauThe first challenge was creating a Clojure connect framework. Then created the base for the application with Clojure, Clojurescript and Datomic. This was my first time working with Datomic and I learned how awesome this database really is!Mitchel KuijpersStarted learning Clojure and ClojureScript and developed parts of the add-on.Casper KolkmanSoftware Developer at AvisiWorked on the deployment to Amazon AWS using Bamboo and helped with the configuration of the testing and production environments.Tomas TheunissenSupport the team with some front-end development skills. Mats StijlaartWorking as Visual Designer, designing logo's, splashes and other artworks!Kevin TaiVisual Designer    The major challenge we faced was accessing the NIN (NIMC: National Identity Management Commission) API.      Firebase and Alexa don't play well. The desktop we brought could barely run the Vive, so Unity crashed frequently.      Not having really done a proper multiplayer/networking game before definitely made it a challenge to figure out how to connect and sync multiple users across the world. It was even more challenging to figure out how to allow anyone to run their own AI bot on the fly. I also spent a lot of time thinking about a game that would be simple enough to implement but still provide enough interesting challenge/strategy to be played by humans and AI.    It was not possible to re-use any of the Server backend code so we had to re-implement it.Maintaining AUI and ADG3 compatibility in the same codebase is something we haven't solved yet.Having a single page application inside an iFrame was definitely a challenge. We needed to synchronise the iFrame URL with the browser's URL and vice versa, in order to have the same behaviour we have on Server version. This way we can make use of permalinks as well.  As expected and as usual we ran into a lot of problems. Especially obtaining information from the hook theory website was difficult. This is because no database on chord progressions exists. Therefore we had to write up a script to obtain the data. Additionally, learning about tries and applying them proved to be very difficult. We also had to write up threads in order to play notes in different channels.   I am not an experienced developer. Therefore, I found it challenging to develop the websocket. I had to learn by myself and master its key functionalities.Also, I had to develop my 3D design skills in using Blender software to recreate the items in my virtual home.    Internet connectivity.Access to healthy drinks and food.Access to sanitary facilities.      There were several challenges involved in getting the front end components working properly with the back end API, particularly using the Azure Maps, as custom javascript and CSS had to be written to get the functionality we wanted.Initially, we were facing issues making the IoT DevKit send telemetry data to IoT Hub. Later we found out that the issue was because of a bug in the older version of IoT Workbench Extention for Visual Studio Code. Updating that extension to the latest version Fixed that issue.We also faced some challenges while making the Function App trigger automatically when the device sends the readings to IoT Hub.  Adding utterances into intents, and entities into intents were tricky. It's hard to predict the exact sentence the user would type for a certain problem so I had to put as many utterances as I possibly could.  1) Capture the kinect photo with the least noise and incorporated Arduino-based trigger for the photo 2) Integrate the local image capture, python web server, google cloud platform, and twilio together and make them work flawlessly. Specifically, the challenges include the following:Image format conversionImage compression and processingHandling HTTP POST/GET requests between Local and web servers for images as well as web servers and twilio for sending and receiving textsCreate appropriate database structure to store images and item labels3) At first, it was really hard to pick the right label from about 10 labels returned by cloud vision api. We used KnowledgeGraph first to narraw the list down to 3-5 labels, and then manually process them according to how general or specific they are.4) There were some misleading parts in the documentation of cloud vision api in Python. The URI stated in the doc is not the correct format required by the actual function. We finally figured it out by looking into the C# version of that documentation.    How to make the login procedure as simple as possible          A shortage of parts drove us to change our design a few times. The available sensors did not enable us to detect the size of the leaks and integrating a web page into Arduino code was also a challenge. Trying to get live numeric data from the robot was difficult. Building a drive train to maneuver the robot was taxing. Prototyping robotic arms was challenging too. Determining tensile strength for robot parts expended a lot of fabrication time.    We had many issues getting the data sent to somewhere where it could be processed.  We tried using ROS, SPI, and sending the data over serial, but ran into intermittent issues in each case, where not all of the data would be sent.    Only two weeks passed between the time I found information about Codegeist Hackathon and submission of the add-on. Major challenge was to build MVP in really aggressive timelines.Being really just back-end developer, I also had lack of UI experience which was really challenging!  Back then we didnt have a good visualization tool for eye movements on screen. However, there did exist a lot of experiments such as Dwell-Free that allowed us to trace a succession of actions based on heat maps. Whilst innovative, it wasnt really suited to gaming. We needed something less intrusive and based on what we experienced, heatmaps were a step in the right direction in terms of achieving fluency.So, we accelerated the decay of the heatmap and that created a kind of a waterdrop effect in the screen (The first bubble!). We then created a different variation of that technique that ended up being the customization feature in our extension. After that things gathered steam. We showcased the functionality to Eleague and we were promoted in a  "Getting Technical" segment that became quite popular in the show. After that (by content creators/streamers request), we made this a streaming tool. Accomplishments that we are proud ofThe 2nd challenge came when we had the option to transform this streaming tool as an extension, by that time twitch was working in a new version of their extension store showcasing a new way of visualizing extensions in categories,  that will boost discovery for newcomers. Of course, we did not want to lose this opportunity, but we had 3 weeks to research how to build this and actually do it!Also, we have no branding at all, we call it internally the "Gaze overlay", so we had to have that in mind too and what was more difficult by then: "How we can help our users to explain to their viewers that the bubble means eye tracking?". It was a huge challenge, a very exciting one with a lot of office hour but also with a ton of fun since we were doing something that no one has seen before in gaming and/or eye tracking :) We have seen an amazing reception that been translated into a great exponential organic growth. This has not only allowed us to expand in streamers numbers but also has open doors in esports events as a broadcasting tool: Rainbow Six US Invitationals  What it doesEmpowers the viewer with the ability to see where the streamer is looking in the screen. Watch DemoHow we built itThere are various components involved in transferring streamers gaze on their screen to a stream on Twitch. The easiest way is, of course, to burn the gaze bubble into the stream. However, this won't offer the best user experience for the stream viewers as they don't have any say in the whole setup.We started to develop the Twitch extension based on the Tobii streaming gaze application which we already had. It now needed to push the streamer's gaze data to the viewers, luckily Twitch's PubSub API fitted that purpose. From the viewers perspective, they would get streamers gaze data and could customize it according to their preferences or turn it off altogether.Tobii Ghost windows application passes the gaze point data to Twitch using products offered by Amazon Web Services, then Tobii Ghost extension on Twitch will receive the gaze point data and will try to visualize it with the help of WebGL.What we learnedThat eye tracking is a great addition to streaming for 3 main reasons:a) To allow viewers to understand the streamer decision-making process while they play. b) To allow viewers to see streamers reaction time, been able to measure Streamer eye-hand coordination with their own. c) An extra tool for educational streamers to showcase their game awareness and what they focus is in the different stages of the game. in addition, being able to have access to all the data that streamer generates with AWS has made us learn a lot about our users. What we initially thought was a great product for the battle royale and overall shooter genre, has given us an amazing response in some other communities. That has been the case with  OZU!, Tetris 99, Guitar Hero, Chess and Poker and IRL. We have also learned how to create a community out of this specific extension. What started as a support channel for new users, today is a place where streamers are constantly discovering new ways to use their eye tracker to connect deeply with their viewers. We are very aware that we have only just started and there is a long path to follow in terms of development, but for now, we have achieved great things and one of them is celebrating user content. We have done that not only by placing them into the spotlight.What's next for Tobii Ghost - Eye Tracking Twitch ExtensionThere is a lot cooking, so going forward we intend to make sure we continue the development of this tool by listening to our community feedback since we believe that user-centered design is the best way to explore the future of eye tracking in streaming.Based on that we are looking forward to working together with Twitch in order to fix time sync. Right now that is a process that needs to be done manually for every streamer. We believe that the outcome of this will be a big revolution since we will be able to provide the social aspect of eye tracking allowing viewers to make clips including Tobii Ghost.We are also looking forward to extending our experience to more devices, with mobile as a big target.Lastly to be able to give streamers way more control and data over how their viewers interact with the extension, so they can see what decisions boost engagement.Try it outgaming.tobii.com      Submitted to    Twitch AWS Extensions Challenge    Created by  Back-end DeveloperMohammad MousaviContent strategy, promo. Ignacio della MaggioraEye Tracking Enthusiast - Working for Tobii Eye Tracking. Software architecture, client application, webgl renderingAlexey BezuglyDesign & UXJuha_S  I could not find a working cross-platform camera solution in react native surprisingly so I had to find my way around different tutorials and stack-overflow threads to find an amazing package called react-native-image-picker. This saved me loads of time.   Since we did not know about this hackathon till about 2 days ago, we had very little time to build a product that could compete with people that had been working for maybe months. In addition, our team is fairly new to iOS development, and had to learn a lot of things as we went along.  Trying to create "randomness" in the blockchainMaking cute CryptoKitty crate pictures        This project involved overcoming obstacles in project and time management, collaboration on marketing and logo design, and the development of new skills (web development, CSS, difficulties interfacing the backend, working in Dart).   One of the biggest challenges I encountered with solving the issue of the Closed Captioning being positioned in a location that was blocking a viewers experience. I grappled with a couple of ideas of having preset locations a broadcaster can set to position the CC where they would like it. But not all streams are created equal and having presets will not solve all peoples issues. I then realized, why not put it the power of the user to move the position of the CC text? So I added the ability to drag around the text on the viewer side so they can tailor the experience for themselves. This simple solution made me happy since now its in the power of the viewer on how they want to see the CC text.      It was challenging to get some of the UI components done toward the end of the evening, so we decided to cut some components out for submission. A lot of code did not get refactored to save time. Despite challenges towards the end of the evening, building the core codebase was a relatively smooth process.The game was designed to look like a Windows95 Mobile App... (if Mobile Apps were actually a thing back then). It was oddly very challenging to achieve the styles of a non-styled HTML4 table (on the Game Win screen), since table attributes are not supported in HTML5. (lol). It was fun to purposefully design and build the game to feel and look old...on purpose.    Generating usable dummy dataSplitting tasks equally to leverage the teamFormatting and rendering the Slack postSlack throttling our messagesDon't always trust a Slack API wrapper  Finding the right pattern to recognize ASL and to get a better accuracy of using tech    Couldn't find tag creation in Youtube's API for tagging in timelines, which was kinda disappointing. Had to do it the old fashioned way by converting the audio into text, mapping the audio with screen time and then searching and highighting it on the timeline. n    We had technical challenges and we almost switched to using python as a backend but we managed to overcome them.           We nearly fell asleep in our chairs.Countless git merge conflicts :(    We are not well-versed with hardware. We are software coders :) Yet we chose to pursue a hardware project. It was certainly tough to plan how to to get started with the various components and to integrate them. Putting the boards and wires together was hard as we spent a significant amount of time to configure and test. It was a crazy amount of time (almost 23 hours without sleep) putting everything together and making sure the code works.      Architecture Setup of interwebWe tried different tools available for sharing localhost over the same network, finally stuck with the wifi sharing over the same network.Coming up with different use cases / utilities using the interwebWe did extensive discussions and brainstorming over the different utilities we could provide using this locally connected interweb.Content ModerationTo moderate the content being sent by users over messages, we have designed our own customized admin dashboard, where admin can see all the requests that have been raised so far, or any kind of messages that people have sent to the admin, specifically. Admin also possesses the ability to delete any offensive content being transmitted.What's next for GramConnectAutomated Content ModerationAs of now, there has to be one person controlling each central hub. Going forward, we can even eliminate this person's role to certain limit by adding automated content moderation using machine learning algorithms. Training of the model can be done by observing the patterns and user actions taken by this person when using in manual mode of moderation.2-way content sharingWe can later enable 2-way sharing of files in which users can even upload files which they want to be accessible on the interweb (this content will be suitably moderated, of course!).Built Withajaxbootstrapcss3htmlhtml5javascriptjquerymysqlphpphp5Try it outwww.github.com      Submitted to    MHacks NanoWinner                Top 12              Winner                All Submissions                  Created by  Developed UI,and a part of Backend Using php(Laravel Framework), also worked on the Web Crawler that cache's the websitesPurujit BansalShivam SinghalJavaScript Enthusiast. Open Source Contributer. Seeking for Internship  Once the code became more complex, I realized it was no longer maintainable in its current form. It needed to be re-written and reorganized in order to be more sustainable. In my experience, voice apps commonly face this challenge because of the endless possible user states and the constant need to anticipate and evolve with them. As you add more questions, the app gets exponentially more complex. The next challenge we faced involved obtaining the data for our skill. Housing data is incredibly fragmented and locked down within the real estate market. It took a lot of time and negotiation to get access to the markets we have now. We currently provide listings in Albany, NY, Boston, MA, Staten Island, NY, Denver, CO, Phoenix, AZ, Virginia Beach, VA, Naples, FL, and the state of Michigan.   Currently Google Assistant doesn't support Swedish, which would have been the primary target for this concept. It is readily available for several other languages, so those regions could benefit today, and Sweden could be added later.What's next for Angels of KhanIt needs to be developed further and connected to real data from the Khan Academy translations database.Built Withgoogle-assistantgoogle-homeTry it outconsole.actions.google.comconsole.dialogflow.com      Submitted to    OpenHack    Created by  iotnerd  Yes.  Started to build a project just a few days before hackathon submission. Almost no sleeping at all for 3 days  We ran into many challenges. First of all we ran into performance challenges. The app is opening in Microsoft exploerer and not Microsoft Edge that could speed the performance up we think. Secondly we used a lot of time on finding the right way to show pictures inside the body text on all devices. Thirdly the design should be uniform when the receiver is opening the email on other mail platforms than Microsoft Outlook.  There were quite a few challenges, actually. The biggest two of them were handling image attachments (adding and removing them) in frontend code and creating a text tool that lets users type directly on the attached image.    Efficiently shattering the player object without exhausting the CPU. Ended up batching to save energy. I had a lot of fun messing about with this...Another biggie was adjusting the Accelerometer readings to give you a game experience that's a smooth as butter.    Escalation of complaintsManagement of Groups  Our biggest single challenge was time. We found out about the contest at the end of January 2019 when we were developing our concept for an Extension. Besides time, pulling together technical, artistic, creative and business talent from all over the world in short order was extremely difficult. From a technical standpoint, the biggest challenge was integrating so many different components together without complicating the end user experience.        The Amazon APL is off to a great start but the games visual sequences will most likely be easier to accomplish once there are more APL Commands and Component options.      Speed is an issue, since we have to collect users nearby, then collect tweets for those users on real-time. Twitter also has very rigid restrictions on usage. We ended up using ~150 tweets per user for this demo to solve both the speed and twitter restriction problem. Automatically extracting user interests from twitter profile is also challenging. We tried a few of Alchemy API's services, but the problem was they were not very good for tweets. Then we used the keyword service, which returns some garbage, but we did some additional filtering and it ended up being very useful. iOS is a difficult platform to prototype on    SDK integration: switching between simulated device and real device, debugging with no connection to XCode or Android Studio (USB port used by the device).    Network Failures.JSON Posting. Building the backend. Getting all the APIs to work together.     -prioritizing time to tackle technical issues-learning 3D modelling on the fly-learning new APIs (BlueMix and Back&)  Face recognition doesn't really work well - different camera configurations, different lighting or even haircut and glasses screwed up the process. Thus, in the future, we need to add more reliable identification techniques like Fingerprint recognition (when the devices are equipped appropriately) with a fallback to techniques like NFC, Bluetooth or QR Code scanning.Having a centralised server for all the functionality means that we can roll out updates and have total control over the core of our app. However, it also means that the app can't function offline or if the server goes down. Meanwhile, doing Face recognition on the device is difficult due to all the SDKs out there (some of which are limited to the processor chips, e.g., Snapdragon SDK).Using Ionic and Cordova for making an app was a really good choice due to rapid view design process, but the framework sometimes behaves unexpectedly on the actual device. Lack of on-device JavaScript debugging makes development quite a horrifying process.  Getting everything to synchronize properly.        Storing common state between clients and server.Using SignalR for the first time and understanding its flow.  Programming in C++ for WindowsBuilding the VM on Zihack dayThe feedback for the donations app made us reconsider our approachBroken RFID writer (x2)Broken the login process on a laptop (and then fixed it)  Lots of configuration issues! Starting with the project setup all the way to infrastructure setup on digital ocean. Learning to use graph database for the first time. Revision of several design decisions for better user experience. SSL configuration and setup (also first time)Working hours on end as a solo developer, plenty of sleepless nights.          3/4 of the team members never developed a Blockchain application before. They've learnt so fast and have had to adapt their minds to the way of working with the internet 3.0. Writing applications that only run in client side is more difficult than it seems, you've really limited resources and code and concepts should be really well structured.    We came up with several ideas initially but the biggest hurdle we ran was hardware to test the implementation. Also, there are no online simulators and very limited API for devices.   We had problems with compressing the images to send to the API.We also had problems to train our model with IBM Watson API.  Android Studio.    Some of the challenges include:Integrating Firebase and their cloud messaging platform to invoke notificationsCreating material design components for iOS because they are not natively supportedWorking with poorly documented APIs and libraries  Difficult time to locate the coordinates of the mesh to import new objects onto the mesh.A lot of time is spent to create the contents for VR e.g. 3D mesh, scene, sounds, videos, images...We also had problem when trying install  keras and tensorflow on Google Cloud platform.     i started working 2/09/2019 so Time was the biggest challengei'm no web developper so i did max effort to learn JS and react fast importantit works better on FireFox WebBrowser   e-learning : http://learn.safir213.com/   astronomy Training : http://spacet.safir213.com/   Public Speaking Training : http://publicspeaking.safir213.com/  Unable to get neural network's propagation algorithm to converge at a low enough error, overriding the home button on Android to get an actual lock screen.    Implementing location based filtering and chat   Our team as a whole, lacked Android Development experience but we managed to built an app within the allotted time. We all have different areas of expertise and it was difficult for us to find a common point.  The circuitry was cumbersome. As we evolved the implementation and capabilities of our technology, so too did our hardware have to transform to support it.   It was our first time using Flask (not much of a challenge, but still you have to learn something new) but also our first time with TensorFlow. It's been a pain in the ass getting to work with this new thing but it's soooo awesome to see the final songs :D  We only have one iOS developer so his workload was heavy and we had to design the architecture to minimise the processing of the mobile client app.From a UI/UX perspective, it was challenging to design the user experience without sufficient time for testing with real users, and for multiple iterations. However, we worked around this problem by using general design principles to guide our prototype.   One big challenge was to establish the scope of the project and what to focus on in the allotted time. We also used languages and frameworks (React.js) with which we had very little experience in order to learn. This felt like a fun challenge up until yesterday night at 2 in the morning.  TCP communication between cell phone and computer, serial communication between computer and arduino, DDNS, Hardware  Currently working only with Google Maps and would like to make it work with Baidu, Bing, Here, Apple etc etc etc  Conversion and rewriting Confluence CSS and content to be suitable for a static website is the biggest challenge.      Once we didn't know whether dockerizing the restful API on Compute Engine VM instance or using App Engine. Considering the time limit, we chose to use App Engine. We wanted to allow the user to use Microsoft account or Google account to log in so we had to use two different API's. Microsoft and Google used totally different ways to implement that it took us a while to figure out. They both needed to use AppDelegate.swift and had some conflict, luckily, we solved it before the deadline.    Actually I'm a mechanical engineer, but I took it as a challenge to built this app. So learned how to develop app by seeing YouTube videos.  Finding the right idea that would turn us into millionaires, rendering Knuckles in the right places or at all, using the right API's and using them correctly, learning how to make HTTP calls on Android, having to deal with sync and async calls in different threads, getting the AR from Unity to work with the Google Maps application and essentially every aspect of the app was challenging but we pushed through!    Understanding the different ways a user can ask same questions. English is ambiguous :)Understanding different JIRA APIs to query data.  At first I have faced some problems in implementing whenhub API. I am very thankful that "Whenhub Support"  has helped me to solve those problems. Picast is one of my first web-apps. So, I had to learn different web languages and their formats from the beginner level. And due to limited time I couldn't implement some more features.    We ran into a lot of issues with the machine learning. We were all beginners/had never touched anything in this realm before today. We investigated word2vec models, GLoVe models, Siamese networks, and read a ton of papers before we even had an inkling of what was going on. Throughout the hackathon, we ran into tons of permission issues, trying to install things on our dev machines, network permission issues, and restrictions such as a lack of access to our Slack API and Hubot.   Getting it to work on all client and devices. The Fabric UI    How to make the best use of visual memory. How to make the community train the AI as they use the app (challenges do this). Torch on iOS is not easy. Visual genome images are not perfectly captioned.  Accurate location handling; pushing to heroku.    One of the challenges that I ran into was finding resources for learning how to work with these tools. They're both fairly new services (API Gateway was just announced at the summit a week ago), so finding resources online outside of the official docs was not easy. I additionally ran into some issues with the Grunt tasks I was using from grunt-aws-lambda since it was producing empty zip files for me to upload to Lambda, but I eventually just zipped the files myself.              We had a lot trouble with Github at first. Sometimes Github can't resolve the conflicts when two person modified same file at the same time . We solved the problem by always asking other teammates what changes they have made before update their own versions to GitHub.It's our first time to program in Swift, so we had to learn everything from scratch and develop this program in 24 hours. We had a lot of difficult times trying to create objects, connecting code with UI design, and switching from different view controller to maintain a good functionalities for our app.      We had never worked with hardware before, so it was a challenge learning how a "continuous motion" servo was different than the servos used in every tutorial online. We then had to design a trashcan that had the ability to funnel items into two different sections using only cardboard and a base trashcan while supporting an overhead camera.  It was also a challenge getting a non-internet-connected Arduino to communicate with the web sever to receive servo commands, which we accomplished by using serial communication between the Arduino and a wifi-equipped sister computer.Our lead developer was also sick, so that hurt.  Getting Geofences to work properly and getting the eventHandler to fire.    After deploying to heroku, we realized that certain functionalities had been destroyed. As a result, we tested the code on local only to find it unfaulty. After several hours of troubleshooting, we were able to (with the help of a Twilio developer evangelist) ascertain that it was due to heroku's attempting to print out unicode characters.Try SMSiri out at (714) 500-7689! A full list of commands / what it can do can be seen in the devpost app screenshots.Built Withazurebing-mapsbing-search-apibing-translatorexpediaflaskherokumicrosoft-translatoropenweathermappythontwiliotwitterwit.aiwolfram-technologiesyahoo-financeTry it outwww.smsiri.cogithub.com      Submitted to    MHacks 6    Created by  I created the landing page and worked on the features to generate results via wolfram, step by step navigation, fetching of latest twitter updates, and stock market updates.Nelson LiuI worked on the backend features, including language translation, retrieval of top news headlines, and activity suggestions. I also worked with machine learning to train the AI to recognize natural language requests.Katie LuangkoteI worked on the feature to check for the Weather. It uses natural language processing to take a string such as "Weather in Ann Arbor" to display the weather condition and the temperature in Farenheit.Matthew LinUCLA CS 2017  The biggest challenge is designing the algorithm to interpret the sentiment analysis data in a meaningful way. This is proprietary and will not be revealed.Getting the web app and servers to work right is always tough.What I learned-Wolfram is incredible. Seriously.-Sentiment Analysis is a really powerful tool, but you have to get your dictionaries and algorithms well tuned.Built Withbeautiful-soupcssindeedjadejquerynode.jspythonsentiment-analysis-onlinewolfram-cloudwolfram-technologies      Submitted to    MHacks 6Winner                Best Use of Wolfram Tech                  Created by  I created the node.js backend and middleware layers in an attempt to seamlessly blend our multiple APIs into our webapp. Additionally worked on our optimization algorithm to produce the highest quality results on the job page.Sai NaiduBuilt the web scraper, sentiment analysis and ranking algorithm from scratch. Integrated with Wolfram technologies to supplement sentiment analysis algorithm and do the data visualization.Mark GeeInterest in computers and sensors with majors in Biological Engineering, Biochemistry and Plant BreedingI worked on the front end.  I coded the website in jade. I used flexbox in css to make it fully responsive. I also added in jquery to add functionality to the drop down menus. Mitchell LeeWayne Shengwei Ge              There were at least two areas which you might think should be covered by some community made smart packages, but they weren't. Those are:recording sound in browser and storing it as a WAV filetrack player and in particular MIDI playbackTurned out there are some solutions out there but they're not configurable enough to fit our needs. So in the end we had to put a lot of work into custom integration of 3rd party libraries. For MIDI, we usedhttps://github.com/mudcube/MIDI.js/and for sound recordings:https://github.com/daaain/JSSoundRecorderBronchitis of one of our members didn't help either ;-)What's next for "Star Song"We hope we will have time to at least make parts of our work available to the community in the form of smart packages. Of course a lot of things require polishing.Also it would be great if we can implement some features we didn't have time for:more options for tracks customization, e.g. volume, sound effectsallow different MIDI instruments, possibly uploaded by the usersrendering the end result into an easy-to-use formatdisplay MIDI track in a form of notesshare links to a read-only version of your compositionsimprove collaborative experience, e.g. discussion, locking tracksBuilt Withmeteor.jsTry it outmakestarsong.comgithub.com      Submitted to    Meteor Global Distributed Hackathon     Created by  I was responsible for implementing timeline synchronized audio playback, and the playable MIDI keyboard.Tomasz LenarcikA web developer, working with revolutionary technologies like MeteorJS.In this project i was responsible for making things work together. So I made places for other components, User accounts system and simple design and UX.Szymon PaluchI was responsible for recording , uploading and storage of audio files. Grzegorz PociejewskiMy key part, apart from the idea, was the tracker and MIDI editor interface. I also did some other parts of the UI, scaffolded the db model and architecture etc.Hubert Orlik-Grzesik    One problem we noticed is that scalability for this kind of system is crutial.We detected many anomalies near the airports because we didn't have the enough sample size to learn, this leads to false positives.  Initial plan is detect motion of the user's hand and to flap the bird only when the user is stationary. However, after several trials with OpenCV we have learned the hard way that the library is so slow that it is no longer practical to use along with the game. We opted to checking the objects position against preset boundary area.  We are doing compiler work, enough said.    Teammate not familiar with Laravel. Short time to implement USSD featureAccomplishments that we are proud ofWe did not allow the distance barrier and poor internet connectivity to limit usWhat we learnedWe learned how to work effectively as a teamWhat's next for Easy Retail PayWe plan to proceed with the project after the hackathon and incorporate blockchain.Leveraging on GeoTagged transactions, we can build various products on our platform (e.g loan and Esusu systems) We believe consumers should be able to spend their money wherever and whenever they want to.Built WithjavascriptlaravelmysqlTry it outgithub.comwww.krystanet.com      Submitted to    NaijaHacks 2018    Created by  I worked on the product design and backendOyetunde JohnI am an ambitious, innovative, results-oriented software developer with keen interest in blockchain technology. I worked on the project logo. Also I am working to add language translator geotagging and auto communication of data.Oluwadamifogore Daramola      Working with new technologies always seems to present challenges.  I can't think of anything we did that didn't have some challenge associated to it.  Some of the more relevant challenges include understanding the API content, integrating with the APIs, figuring out how to associate and integrate API information with a 3D simulation and building the runtime editing capability for the 3D configuration.  There is no public oAuth2 Service or anything like this for the VoiceMail System, so I build this as an intermediate.There is no URL of the VoiceMail System to stream the voice record as MP3, so I have to build it as well. This works on the fly (no voice record is stored at my intermediate server), to strengthen privacy. German language is more diverse than English. So Alexa does not get all tweeks, when it comes to ordinal numbers by gender (see what I learned).  Some of the challenges we ran into included: patient research, gesture movement accuracy, and connecting scripts. Motor neurone disease is a rare disease and has multiple different forms (ALS being one of them). Each patient can suffer varying degrees of this disease and would thus be able to use their hands in varying amounts. We had to set a standard amount they would be able to move their hand muscles. We also had to understand how to translate simple hand movements into desires/needs of the patients. We boiled them down to the bare basics to keep it simple: food, water, and restroom. Different movements would belong in one of the categories (we certainly plan on adding more when we receive more data). Perhaps one of the hardest challenges we ran into was connecting Arduino, Python, and PHP scripts into a single project. We had to figure out how to efficiently use libraries to create a single unified product.     Without 3D printing the parts, our mechanical frame is a bit flimsy                                            Developing: Api's Integration. Zoning for Holy Places. "Mena, Mozdalefa and Arafat".                                        Launching:  Hajjy AR mobile application doesn't appear on the play store for Android Version 4.3 or less.     Understanding Klarna's architecture and SDKDeveloping Android app using GradleDeveloping our system using new tools in a short amount of time     Takes us time to get the messenger Id, to send the messages, because FB login uses different Ids for the users (the one retrieved on login process).Until now, there is no way to create a messenger group through API.Time to time little issues with the tools used, like miss configurations, due to mental fatigue.  We developed this app on three separate OSes which complicated the development environment. Also, the Google Cloud Speech-to-text API only allows for ~1 minute of continuous audio streaming so we had to work around this limitation in order to provide a consistent and continuous stream of audio.    Only two of our team members came from a coding background, and the others were well versed in Humanities and Arts. One problem in development was hosting the website. We could not get the website to host on Amazon free AWS as it takes 24 hours to be authorized.     Working with this kind of database was quite new for us so we had a great time learning about it.  We had some troubles pairing the frontend and backend which we troubleshooted using Postman.  1. Don't break our existing submission flow.This feature affects the most critical point of a hackathon on our platform: submitting your project. Because we always have hackathons live with open submission periods, we couldn't release this like a normal feature (where it's deployed and enabled sitewide) because it would break hackathons with open submission periods. So we deployed this improvement using a "feature" that we can enable on a per hackathon basis. All new hackathons get the feature enabled by default, and we'll migrate existing/old hackathons one by one when it's safe to do so. This also meant we had to continue to maintain the old submission flow, while building out this new flow.2. Merge 2 project concepts (portfolio projects and hackathon submissions) into 1 project concept.One of the technical issues that we ran into was the fact that hackathon submissions and portfolio projects have been stored in the database as two separate entities, and have diverged over time. This meant that we could not just map one to the other due to differences in column names and associations. 3. Our system must know the state of the project at the hackathon deadline (for fair judging), but the user can continue to edit their project after the deadline.Portfolio projects are designed to be a living entity that evolve over time. In contrast, hackathon submissions need to be a snapshot of a project at a given moment in time: the submission deadline of the hackathon. We had to set up our system so that hackers can continue to edit their project forever, but hackathon judges/managers can see a "snapshot" of the project at the time of the deadline for judging.What's nextContinue to convert all hackathons on Devpost to use this new submission flow.Review metrics  are a higher % of people completing the start a project --> submit project flowGather qualitative feedback from users Have feedback on the process of adding a project to your portfolio or submitting to a hackathon? Let us know in the comments!Built Withcss3html5mysqlrubyruby-on-rails    Created by  Gave up months of my life to save our hackers multiple minutes during the submission process.Matthew GerriorSenior Software Engineer @ DevpostyoloStefano BallabeniWhat about your short game? :frog:I made it look awesome.Elle MundyHacker  English major. Freelance web developer.If anything about this new submission flow is confusing, you can blame me. Please tell me how we can make it better :)Holly TiwariProduct Manager & Designer at DevpostI mostly yelled a lot during code reviews.Ross KaffenbergerI write code and race in triathlons.Jon Panelmix  The Nuance library was not something we have worked with before, and took plenty of trial and error before we could eventually implement it. Other difficulties included successfully developing a database, and learning to recycle movements to create more with higher efficiency.  Server communication to the client, As we were trying our hands on Meteor for the first time, it took a while.Some serial port libraries did not work.  We faced several challenges while building Pixel Kanvas. The biggest was learning new APIs, including Firebase, a canvas scroller, and a custom color selector tool. Redrawing such a complex canvas following pan and zoom events also proved tricky.         Finding updated appliance data; building assumptions for appliance use.    There were many challenges we ran into, finding an air conditioning system that meets the needs for this project was one of them, we looked around standard AC units but some of them didn't fit our needs, we kind of used and old one at the end, but are looking to implement the Econditioner with a custom one, so that we can have an smart AC from start.It was also hard to use our own servers, planning databases, mqtt brokers and others was a challenge that we overcame at last, and of course keep improving the platform so that we can make Econditioner the new smart AC in your house.At last the final challenge was to save as much water as possible. We keep on working on improving the condensation progress of Econditioner so that we can save more water, trying to optimize this task to a limit is hard, but we are up for the challenge.  MongoDB and Node.js! I needed to learn a fair bit here. I now know what Node.js developers mean when they talk about 'callback hell'.I also ran into some challenges getting my game client to operate smoothly when making web requests to my node.js web API. I needed to figure out how to get these calls onto background threads, which is traditionally quite difficult to achieve with the Unity3D engine. I did manage this in the end though, and web requests now play out smoothly on background worker threads and don't lock up the UI.    Unity learning took a significant chunk of time. If we were to do it again, we would use web-based solutions, for example, we could write a web-only app to show demo.  We had to learn how to integrate the telegram API with our meteor framework.  Being inexperienced with C++, many of our hours was spent looking through countless pages of documentation and Stack Overflow forums to fix simple bugs.Setting up sockets along with a connection from the C++ code proved to be very difficult.     We originally started by trying to write our own classifier using OpenCV. As we mentioned earlier, we realized very early on that building and training an image classifier that would identify various foods would be too difficult, time consuming, and computational expensive, as there was simply too large of a training data set to be processed and evaluated. As a result, we decided that using the CloudSight API would make our lives much simpler. We used the CloudSight API's Visual Search APIs to get context about food-related images and then later grab nutritional data. In addition, we also ran into latency issues with the CloudSight API, as the image recognition was a long, expensive process. We realized that part of the reason why the process was taking so long was because the images that were being sent by the iPhone app were very high-resolution, and also very large. We learned that bringing the resolution down brought the size of the image down significantly, and as a result we were able to get the CloudSight classifier to speed up a slight amount. The latency issues became less of a problem when we decided to change the intent of the classifier, being that the classifier would look for the main object within the image (in this case, the food), and not try and analyze all details of the entire image (e.g. the classifier would recognize additional, unnecessary, features such as the color of the table or plate that the food was sitting on).Accomplishments that we are proud ofWe are most proud of the fact of how well-designed and efficient the app is, as well as the fact that Kenko is now able to identify almost all foods, as well as retrieve the relevant nutritional data with a fairly small margin of error.What we learnedWe learned many things throughout the hackathon, but the big lesson of the day was that image classification requires a lot of computational power in order to train accurate identifiers (especially when it comes to training against a data set as large as the amount of food on Earth). What's next?We are thinking of continuing further with this project, as well as possibly shipping this app to the App Store in hopes of improving the lives of many.Built Withazurecloudsightcsshtmliosjavascriptnode.jsnutritionixobjective-cshellxcodeTry it outusekenko.co      Submitted to    PennApps XIIWinner                Best Cloud Based Mobile App (Sponsored by Goldman Sachs)                  Created by  I organized communication channel between front-end and back-end, and wrote the front-end iOS appKevin FransheyI wrote the Node.js backend for the app, in addition to mocked up the app's design (color palette, fonts, layout, buttons, etc.). I also wrote the landing page. This app was a great learning experience, and a lot of fun to make!Gautam MittalI like building and breaking things.      The whole process was new to us. It was our first time attempting to plan realistic mobile app development according current knowledge and skills, in order to at least make a product. We spent a lot of time trying to find and figure out the new tools and API, and figure out solutions to errors we never encountered.     I ran into a lot of small challenges while building this app, one big challenge was figuring out how to get the video from YouTube directly to an S3 bucket and figuring out which two A.I services to use. Another challenge I ran into was the 5000-byte text limitation of Amazon Comprehend.  We had problems with coding the part that calculates the total sum of a client's purchase. There were just too many variables of different value to code all of it in 24 hours.     I had trouble with Glare Input for the simulation.  The length of the copper wire required an exact length to properly collect the WiFi signal, which transmitted at 2.4 GHz. This was calculated, cut and soldered to the RF connector. Different lengths were tested to ensure optimal collection. The Raspberry Pi was set up to function as a WiFi access point. This required extensive terminal commands and changes to existing functions already in place using the Linux based OS. The algorithm used to interpret the data required data to be heavily cleaned and exported into a format that could be interpreted autonomously by the executing program. The challenge presented was that the data needed to be averaged and manipulated to determine an optimal search direction.  The varying lighting conditions throughout the day affected our image processing.The relatively lackluster performance of the EV3 forced us to find workarounds for performance limitations.Some libraries used were made several years ago and were outdated, requiring us to update several methodologies in the libraries.    The biggest challenge I ran into was figuring out how to do Optical Character Recognition. I was first looking at tesseract which was another free but old Google library to do OCR but it never performed up to par with images that weren't preprocessed. This is why I switched to the Google Cloud which provided higher accuracy OCR to many images which were taken under many conditions. It still doesn't work perfectly and you need to take a somewhat solid picture but it does work pretty well.  Noise on the line on some of our circuits. Other hardware issues. From a developer experience perspective, the Alexa part was pretty easy (except maybe the documentation on account linking, but that wasn't that hard).  The main technical challenges we had during the project were mainly related to the fact that none of us had any prior experience developing Trello Power Ups. Basically we started from the very beginning doing the Hello World tutorial. We had to spend time analyzing the Trello APIs.We also had to kind of rethink the normal development process we normally use in Adaptavist (one repo and different git branches), due to the fact that we needed a server hosting all of our front-end code. To do that each of us had to fork their own personal copy of the main repository. We then used those to host code for dev testing purposes. The main repository was where we hosted and stored stable code.Figuring out what type of module we should use to display the links was tricky. There is no module for adding a general panel to the Trello card, so we spent some time choosing a solution.Some of the APIs to store data on the cards were limited at the time and didn't allow us to do operations on different cards and boards. During the Connect Week in Austin, talking to the guys from Trello, we were informed that these API are about to be more powerful, what is really good news.What we learnedIt is very easy to start building power ups for Trello. The documentation is comprehensive and it is possible to start developing even without a backend or any remote services.What's next for CardLinkIn true agile fashion, we're releasing V1 for Codegiest with plans to grow the Power-up's functionality in the months to come. We're looking for user feedback to find ways we can provide more value and help users truly harness the power of creating relationships in Trello.Built Withcsshtmljavascript      Submitted to    Atlassian Codegeist 2017    Created by  I am the project lead and one of the developers on this project. I worked to prioritize the backlog as well as working to help develop the product.Stephen CheesleyCoding, coding and coding...Andrea RosatiMore coding, coding and coding. Also, I did make sure no one has run out of coffee :)Vitor PelizzaInfrastructure work using Terraform, S3 and CloudFrontMatt SaundersMotivation, support, freedom and the time to make it possible.Jari WorsleySuggestions and dev environment hacksJon Mort  Since excel formulas do not follow any standard coding syntax of modern programming languages (that we know of), we had to create our own code formatting rules and ran into a significant number of edge cases. We were limited by the lack of two-way data flow in the add-in model.  We came up with some clever and unique solutions to make sure that when changes were made in the Excel client, our add-in was aware of these changes and could automatically update the appropriate features dependent on these changes.(Microsoft, please add some event listeners for us!!!)As with any application, performance was at the top of our priority list.  While the installable version of Excel performed well, optimizing our app so we had equally good experience in the browser, had us coming up with some creative solutions. We would have loved to build some end to end testing into the app, but we couldnt find accessible libraries to help us.  Since the app is required to run in an office environment, we couldnt find a way to simulate this to automate our end to end testing.If we missed something, please comment below, we would love to know!  Empower thai people to be aware of green impact           Our biggest challenge was setting up the work environment with the General Motors sdk and learning to develop with this.   The data of untoward effects of medications on a population, interactions of medications that do not affect the same pathway, and the ratio of medications prescribed is not public.  We had to extrapolate data from the information provided on the Internet about the untoward effects.  I ran into challenges integrating the CustomVision ML with my actual xcode app.  I wish I started this earlier. I wish I had more time. I wish I wasnt just a one person team. CORs WebGL issues. Ugh.BUGS!!!I could swap in a better tracking SDK, but  I am familiar with JS, Node, and Mongo databases, I have however never made a mod for a game. Rimworld uses C# and I have very limited experience with it. After the initial struggle of debating architecture and designing our authentication flow (How do you link the game itself to a server when it has no idea about the Twitch streamer?), our next challenge was learning how to actually trigger the events in the game.Luckily we have a well rounded team, and one of our developers (InfinitySamurai) was able to carry the bulk of the weight when it came to C#. Without his expertise this project would have been dead in the water.  We came up with a lot of ideas, and had a hard time choosing which one to spear head towards to. But later we decided to go for the one that had the most social commercial impact. Another challenge we ran into was figuring out how to implement the computer visual algorithm and hooking it up with the FLIR One SDK in a very short amount of time.      Our team has never worked with React-Native (building for mobile apps) and GraphQL. Both technologies make up almost 70% of our application so the learning curve + actual development time was very difficult and very fun at the same time.     Some difficulties using the Shopify API with the authentication process and with the Google Maps API markers. In the case of the authentication we found that the packages that were ready to use in the NPM system wasn't quite enough fitting our needs. So we recreated the flow to make our app works. Our second biggest problem, was the integration with the Google Maps. We decided to use a lib to create clusters with the markers. That was a real challenge, since none of us didn't have used it before. We didn't give up and we managed to make it work.  We took some time to define the initial project architecture and we believe that a little bit more time to plan the whole project would be good.  We had a lot of trouble deploying the web app to AWS. We then chose to use IBM's BlueMix, which along with the help of their engineers, was much easier.  The biggest challenge was the lack of time. We came up with this idea during Junction, so we did all the planning during Junction, which decreased the amount of time available for coding, so we didn't have time to add all the features we wanted and improve the user experience and usability. We also had problems with getting our mockup sensor to work with our backend. The problems were mainly focused around Windows's serial port drivers. Nevertheless this won't have an effect on our programs functionality when the sensors have an API.      Our team consists of 2 experienced guys and 2 absolute beginner. A challange was to train them within about an hour the basics of Meteor. At the end everyone was productive, depending on his prior knowledge about js.  As we were working toward the Athena Health hackathon deadline, starting only from 2 months prior to the deadline, the team had very little time to design and build the system.We made hard decisions on balancing the features/functions we wanted to show, and the levels of depth/refinement we could achieve.  The biggest challenge is to stay within scope, we all have many good ideas, and need to make hard decision on which features should be pushed back and focus on our core value.    Demo account scalabilityHigh quality climate data is available for nearly 10,000 locations and available on a daily basis. The original idea was to compare all locations, all 366 days of the year (includes leap day). It was quickly determined that the demo account provided for the Sparkathon could not reasonably handle the nearly 91 trillion combinations that result from daily data at the full location list. The locations were trimmed down to only locations in the U.S. Historical Climatology Network (HCN) and monthly data were used to bring the scale of the demo into line with the provided infrastructure. I have no doubt the methods scale well with increased resources and such a large problem is still well suited for the Apache Spark framework.Limited Object StorageWith the intent of creating over 167,000 images, periodic transfer to an external system was required to keep the storage usage below the limit.  One of the biggest problems we ran into was performance, after 3 rewrites of the engine that handles rendering of data layers and different libraries we have found something that fitted our needs, yet it can get a bit slow sometimes.Meet the teamNick Vernij - Nick is a 16 year old developer and student from Rotterdam, the Netherlands, creating websites since he was 12. He mostly codes in PhP, Javascript and javaLem Severein - Lem is a 16 year old developer and student from Rotterdam, the Netherlands, he can do anything with his skills in css, html and javascript, and PhP. Sufi gaffar - Sufi is a 17 year old webdeveloper and student from London, England. He is constantly learning new things, and has a fair knowledge of some languages.Daniel Mizrachi - Daniel is a 17 year old webdeveloper and student from London, England. And is definitely in love with open data and APIs.Contact:hello@nickforall.nlBuilt Withclusterpointcss3javascriptopenlayersopenstreetmapphpTry it outunderground.nickforall.nl      Submitted to    Maps as Art | Summer JamWinner                All you can EaaS              Winner                Clusterpoint Grand Prize                  Created by  I have been busy with research, mostly writing the javascript for the map rendering, and converting/collecting data.Nick VernijI'm a 16 year old programmer from the netherlandsI designed the page and wrote the biggest part of the CSS.Lem SevereinI helped collect data for the projectSufi GaffarDaniel Mizrachi  We had difficulty integrating all the different components of our system together. Namely, connecting our back-end to the database hosted in Azure required a bit more than just a simple connection string. Additionally, creating high quality models for the AR component was a challenge due to difficult lighting conditions for photos, which were needed to generate these models.      Our three-person coder team were new to the platform we use (Polymer)Research and data massaging consumed a large portion of time due to lack of national centralisationWe found an excellent resource for registered rooming houses on www.data.vic.gov.au but there was not enough information (no contact details)    Literally our first hackathon and first time using android. It was pretty painful to figure everything out. Accomplishments that we are proud ofLearned android as well as we could in 24 hours and no sleep.We wrote a haiku for NoQ:Four men was her crew, Lines she can skew and undo,NoQ, app long due.What we learnedFirst time use of Android, you hustle or you don't demoWhat's next for NoQWe'll work on it more and consider adding more featuresBuilt Withandroidgoogle-mapsjavaspringsqliteTry it outgithub.com      Submitted to    McHacks 2018    Created by  He Qian WangAlexander HarrisAbbas YadollahiFilip Bernevec  We are ALL beginners in game design and Unity programming. Designing a game for the Hololens is quite different from designing a game for the PC, as the control mechanism is by movement and not by WASD.    Setting up the web server, for one, took a very long time!!! We initially tried a Python server, but it didn't work. Thus, we opted to use a Firebase Realtime Database instead. None of the team members had experience with speech to text before either. The IBM Watson SDK for Unity was a bit outdated, so it was difficult to integrate at first.What's next for reVIVEDue to time constraints and for demonstration purposes, we decided to focus on ADHD as the primary use case of our app. However, we plan to expand the use cases of this app to a book of other mental disorders as well, such as anxiety. We would also love to implement a machine learning algorithm to quantify the user data we receive from the app. In the future, we want to make the app more refined and start working on bringing our solution to real patients for testing.AccomplishmentsWe are honored and excited to say that we were the First Place winners of the 2017 TechCrunch Disrupt NY Hackathon. We hope to further develop our app and continue on the journey to making ADHD diagnosis more accessible and effective.Built Withbootstrapc#cssfirebasehtmlibm-watsonjavascriptnexmounityTry it outtechcrunch.comtechcrunch.com      Submitted to    TechCrunch Disrupt NY 2017Winner                Overall Winner              Make School's Student App Competition 2017Winner                Grand Prize Award                  Created by  I built the VR simulation using Unity and C#, integrated the IBM Watson Unity SDK into the app, and built the Firebase Realtime database for the web back-endAkshaya DineshAmulya BalakrishnanSowmya Patapati            Using the JIRA Java API, and the deadline.  A primary challenge we ran into, was that we wanted a way to ensure our users that the information would be hosted forever and find a way to make that financially sustainable. We were able to develop a onetime fee pricing model that allowed us to calculate the cost of host information forever and by determining the net present value of the information stored and charging based on that number. We believe that once history is online it should be free to view for future generations and friends.  Adapting the design to mobile created a lot of little overlap and CSS errors here and there.         We had a difficult time converting vanilla javascipt to react native.Retaining the same voice when we do the accent transformation is one of our biggest challenges. We currently use a different voice which does not provide the same experience as replicating the users voice. To solve this problem we would need to train our model on the user's voice. We could also prompt the users to create a vocal avatar using Lyrebird api.We also had to make sure that we rendered the video so that it matches the accented voice.    We face a number of challenges, for example how to effectively stimulate people to share indigenous languages, how to design a creative environment to solve problems, how to improve our user experience, and how to engage relevant organizations into our Guageland community. We're very proud of our teamwork and accomplishment!What's next for GuagelandIn the future, we are going to further develop and polish our application by making it more responsive, interactive and developed. Deep Learning, Blockchain, and more cutting-edge technologies will be applied to our app!Built Withaiamazon-dynamodbcss-in-jscss3githubhtml5ibm-watsonjavascriptnode.jsplanetaryjsprogressivepwareactrestful-apiserviceworkersslsusi.aitrelloTry it outgithub.comfosshack.netlify.comdrive.google.com      Submitted to    FOSSASIA Hackathon with UNESCOWinner                Cloud Prize                  Created by  Project Management, UI/UX Design, PrototypingApril, Yuting FanI made many pagesMingLiwen LaiKaishuo Zhang   The most serious challenge surrounded training our meme-generator model, so that the generated captions would be sensible as well as fresh and funny. To accomplish this, another hurdle we had to surmount was aggregating a sufficiently large meme dataset to train our model, which involved web crawling as well as leveraging existing datasets.   We find mostly challenging the electronics, because our main objective was to get the optimal energy out of our battery and avoid draining it too fast.Another point worth mentioning was the data transfer between the experiment section and the Sat section, because we wanted to isolate each part as much as possible from the other, so the experiment just need to tell the Sat to send the data and nothing more.Accomplishments that we are proud ofWe are very proud to have accomplished the objective of making a viable prototype, even though we have faced some issues during these days, nonetheless we managed to overcome all of those issues and as a consequence we have grown wiser and our vision has become wider.What we learnedDuring the development for HackSat, we have learned a lot about radio transmission, a huge lot about serial port and how to communicate data between 3 different micros, using 2 different protocols. What's next for HackSatThe first improvement that should be made is fix some issues we encountered with the measures of our designs, which have required some on site profiling.Another obvious improvement is update the case so it is made of aluminium instead of plastic, which is the first blocking issue at the moment for HackSat to be launched.Finally, we would change the hardware so it has more dedicated hardware which most likely will allow us to optimise even the battery consumption and global lifespan for the Sat.Built With3dprintingarduinocenthusiasmvirtual-wireTry it outgithub.com      Submitted to    HackUPC Winter 2017Winner                ShapeWays Challenge: Best 3D printing hack                  Created by  I worked doing the profile for the majority of the printed pieces to correct the minor deviations occurred during printing, once that was finished I have started to lend any helping hand I could to my two colleguesDaro BlascoI worked on the satellite structure, arduino code, battery life optimization and telemetry.Javier Lopez CalderonI worked on the satellite structure, communication systems, project code and electronics.Pau Gallardo  It's first time we write some music processing app on OS X, and many feature, such as pitch recognition and voice recognition takes us a lot of effort to implement.Integrating many libraries from many sources (and even writing our own) needs a lot of effort, especially those about NLP and transcript.Built Withcocoapodsgooglelatexlilypondosxpythonswift      Submitted to     HackIllinois 2016Winner                HackIllinois Top Software Hacks                  Created by  Original idea. Audio algorithm. Score generation.Yifei TengHan YanCode & DesignYu WangCode & DesignYilin Ma  Initially I wanted to develop the python script to run on a raspberry pi and connect to the insulin pump directly but realized it was too much of a technical challenge to deal with. Hence I resorted to using a third party app and also used my laptop instead of the raspberry pi to run the python script.The prediction engine needed to be tuned to my daughter's numbers to enable it to make sensible predictions. To make the system more universally usable we need to replace the current rules based engine to a Machine Learning driven algorithm.The text messages were initially schedule to run every 5 minutes which ended up annoying my wife who was one of the recipients of the messages, hence I had to reduce it to every 10 minutes.  We faced with challenge to match predicted pose with light we want to control, after few experiments we decided to use a line between a shoulder and an arm, computing the length of the shoudler-elbow and shoulder-arm we can understand that a person wants to switch the light on/off.What's next for HandyLightAs technology achieves great progress in machine learning, the idea of smart homes is starting to materialize. Nowadays there is a lot of amazing project for making our home easy and cozy. Obviously, the light is essential part of every home. Thats why our idea is to make light not usual process of turning on/off the lights, but simplified version of it wich works with hand motion.We hope that during the judging  we will get inspiring suggestions or feedback on our project.Built WithdockerflaskhttpnumpyopencvpythonpytorchTry it outgitlab.com      Submitted to    Junction 2017    Created by  I built and ran Docker services for server and client and implemented communication between them.Pushin AlexanderPavel VoropaevI enjoy hacking and meet new people!Anton LebedevDmitry LevinKarina GasparianInspired by new challenges!  We could not find any publicly available annotated dataset for eye images affected with Cataract. The idea behind having a labeled dataset was the ease of usage of powerful supervised learning algorithms. Since, we were not able to get the labeled data, we scraped images from Google Image Search manually and built a non-annotated dataset, for Machine Learning purposes. We also tried to extend this work over another eye disease Conjunctivitis, but we could not get good results owing to the dataset that we had. Since this is a Health hack and we do not want to recommend random suggestions to the patients, we decided not to include it with EyePhone.    We had trouble interfacing the Google database with Python. This was our first time using RFID, so we took some time to learn the basics.  Time. Time. Did I mention time? I worked on development over winter break, so coding between holiday events, parties and just everyday life made for quite a sprint. Don't get me wrong, it was a ton of fun. Also, working on UX for voice is really challenging  Things you take for granted on a website, or in an App, really present unique scenarios with voice. On a whole, I loved every minute of it. I think there are a lot of improvements to be made, but we all think this is off to a good start.          Making the API work , understanding django, merging frontend and backend , dealing with the extreme slowness of the app because of the amount of DATA our app has to go through.  Lack of adequate hardware. Missed the 3d printing deadline.    Creating a REST Endpoint the router can post IP -> MAC address mapping associated to public IPmaking sure ARP table is updated for all MAC addressescreating recurring cron job on deviceSubmitting and looking up [local IP, MAC address, public IP] on requestsCANDRA must also deal with the delay in the update of the IP -> MAC association (we know the local IP, but don't know the MAC associated yet)single organization can actually have multiple physical spaces with their own Internet routersN members * N devices * N IP addresses * N access pointsProblems connecting to external API to retrieve MAC specific information (unreliable service)Fetching and matching information between actually connected devices and ARP tableWhat we learnedMeteorJS is awesome out of the box:Fast development turn aroundEasier to teach new comersFast, responsive, reactive interfaces that update in real-timeEasier management of complex dependenciesWhat's next for CANDRAWe will continue working on the community management system in future hackathons organized at agora-space.com and as a teaching tool for codersfield.com in addition to continuing the development of it as an useful app that can be integrated to our communities. Built Withgithubjavascriptmeteor.jssemanticuishelltomatoTry it outcandra.meteor.com      Submitted to    Meteor Global Distributed Hackathon     Created by  full-stack dev of the main appJulien Chouletrouter script, requirements, Github setup, devpost project pageRicky Ng-Adamrequirements, router setup, script.Frederic BAZIN  While we were able to complete implementing most of the features that we had designed in the beginning, we found it difficult to strike a balance between simplicity and complexity. Simplicity is always welcomed in terms of user experience; however, we felt that there were certain relatively 'complex' features that benefited the user experience. The only way to figure this out is from user feedback and we hope to keep incorporating data to maximize the user experience.  HTM parameter tunings.=> Let Swarming find==> Takes long...  HTM input scale.    Working out the demographic we were catering for was challenging due to the lack of available data on the detailed skill level of migrants.Limiting scope, and figuring out a commercially viable solution was an interesting process.  Light and Electricity to workCollecting data for our statisticsInadequate Internet to workIssues setting up hosting providers  Although Uipath Orchestrator API are well documented, it took some time to explore and choose apt API call for the job in hand. Managing adn dispatching queue items, bots and enabling the framework work across multiple orchestrator/tenant came out as bit more challenging than we expected.  Callback hell. Version chaos.  Getting started with Twitch developer-rig was a bit of a challenge it is still in beta but I see it getting better in the future.  Time series data is notoriously picky and we ran into that issue a lot. Because we were using customer click data, it was absolutely enormous and we had quite a time crunch on actually producing hive SQL that could run efficiently to load the index in time. Nick ran into a lot of issues configuring logstash to cooperate with his order queries directly from MSSQL.   Defer the HTTP GET request for the images until images have been categorized.Since our Javascript runs as a background process, it was difficult to synchronize the function calls. Keeping ourselves up and running for 24hrs, looking at the ugliest photographs on web. :)  Network availability and context based analytic to identify the nearest object at the emergency location.  Wi-Fi was unreliableSome of us had limited experience with web designNovel topic which required some brainstorming  A major limitation of this bot is the fact that it is a user-based application. Without accurate users' contribution, the reliability of the information provided by the Telegram Bot may be questionable. We also find it difficult to incentivise more users to make use of the bot, so as to keep the machine status updated and accurate.Accomplishments that we are proud ofWe added a nudge function to nudge the last user of the machine to remind them to remove their clothes quickly. We find that this is a necessary function because some people may not remove their clothes immediately as they are preoccupied with other tasks. When users receive this nudge, they will know that there is someone waiting to use that machine and would go and clear their clothesWhat I learnedWe learned how to make a working Telegram Bot using node.js, linking to firebase and constantly thinking of better ways to optimise our code. What's next for Tembusu LaundrobotA possible improvement will be to install a sensor on the machines that detects whether a washer/dryer is in use. For example, it may be possible to detect a jump in the electricity current when a machine is in use. This will eliminate the need for user input to update the state of the machine, effectively overcoming the challenges stated above. We are aware that there is another group of Tembusians working on using light sensors to detect when the machines are being used, and we are planning to collaborate with them for future developments.Since there may be a possibility that there are faulty machines, we plan to input a report function into our code, such that users can report to the bot that a certain machine is faulty, so that other users would be aware of the number of machines actually available for use. The report function will probably change the status of the machine to unavailable, until the college is notified and fixes the machine. There will also be an undo button once the machine is fixed, to change the status of the machine back to available for residents' use. Built Withbest-freshmen-hackfirebase-sdkmost-socially-useful-hacknode.jstelegram-botTry it outt.me      Submitted to    Hack&Roll 2018    Created by  Kay Heen ChongumiumiuuuuuKevin ChanRonald Santoso  We initially wanted to implement the app functions but we had a hard time getting API codes and putting in plugins and additional functions into the app. After speaking to one of the mentors, we decided to build a working prototype of our app using Marvel as we could design the app's layout, functions, and buttons using hotspots.    There are many challenges I encountered. Most of them are found by my brother because every time I updated game, I let him try this game and find bugs. He helped me so much but it doesn't mean that all the bugs are found and fixed :) There are still bugs too but I'm working on to fix this with best solution. Mostly I encountered challenges about Unity physics. For example If I made another game which is similar to it, I would change some of code. Because I added some long codes. At that time I can shorten them. I was stuck with the arts too because I couldn't draw arts that I want to use. So I use some free licensed arts and some music.   We believed that TruClinic would be the solution that providers would flock to. The thought at first was to get customers, regardless of size of practice or length of sales cycle. We were not charging enough either. The positive side of the challenge was that we got a birds eye view into all different types of healthcare providers. The negative side was that we did not have a cohesive sales strategy and it was very hard to predict revenue. Small practices were easy to sell to, but they had high attrition rates because they did not have the infrastructure or budget to implement a successful and scalable telemedicine strategy. Enterprise customers have the infrastructure and budgets, but the sales cycles can be anywhere between 6 months to 2 years. This experience helped to shape the customer acquisition strategy and that helped us focus on hone in on our sales process.Fundraising was also a major challenge. As a small company where everyone is wearing multiple hats, the time and resources needed to fundraise were taking away from other tasks that needed to be completed. The executive team decided to raise the least amount of funds possible and to bootstrap for as long as possible. The whole team learned to operate using lean methodology and kept the growth in line with revenue. The company was not able to grow as fast as it had projected, but it gave us time to work out all the bugs before it was ready to scale.  I could probably have written this without jQuery, but it would have taken me so. much. longer.  That said, I ran into a lot of issues figuring out how to access the inner text of my DOM elements. I think I used all of the following at some point: .text, .val(), .value, innerText, $(this), etc. As you can see on GitHub, this is some of the dirtiest code I've written in a while, but it does work.Edit, 12/9: Changed around sentence/title casing behavior so it's triggered onblur rather than onkeyup. Even with debouncing it was causing issues.Edit, 1/13: Moved email & twitter to their own subpages, created separate datastores for all three content types, massively cut down on JS, and eliminated sentence / title casing (was driving me NUTS). Edit, 1/22: Integrated spammy to flag subject lines that contain common spam triggers.        There were so many challenges involved with carrying out this project, and some of the most difficult were not even software related!In the beginningI like to divide my projects up into individual tests that demonstrate the different technologies needed to complete the project, and then bring them all together. One of the first tests, reverse engineering the motor controller, proved to be quite challenging. Since the controller was designed to be controlled via a radio receiver, not much documentation exists as to which input produces which output. I had to sit down and write some test programs that looped through different analog outputs and tested different PWM pulse lengths in order to find which ones caused the motor to run at different speeds or spin in different directions.Simple electronics can sometimes be not so simpleProbably the most significant problem that took up entire days of work on this project was caused by the power supply. Initially, I discovered that the controller had a 5 volt output pin, supplied by the motor battery, and attempted to use this to power the vehicle components. However, I soon found out that with many sensors, a GPS, an Arduino, and a servo motor, the output was simply not powerful enough, and this caused a lot of weird behavior. After hours of fiddling with the hardware, I ended up using a USB power bank to power the Arduino, and using the motor controller output to power the rest of the vehicle. Since a lot of the power was used in outputting signals from the Arduino, this solution worked very nicely.GPS nightmareI used two Adafruit Ultimate GPS Breakout Boards for this project, and would recommend them to anyone else in a heartbeat. However, unfortunately, I found that my Arduino 101 hardware serial did not work with them, and I ended up having to write my own GPS library including an NMEA sentence parser in order to work with them. I was able to get them to work using SoftwareSerial, and I plan to upload my library and solution to GitHub in order to help out anyone else who may come across the same dilemma.AccomplishmentsCreating a Reliable Obstacle Avoidance SystemI started out just giving the rover the ability to dodge small obstacles in its path. Now, with my most updated code, the rover almost looks alive when weaving through an obstacle course.Demonstrating Vehicle to Vehicle CommunicationThe Arduino 101 powered rover successfully stops on analysis of data transmitted from another moving vehicle near it.I also found out some useful things about vehicle to vehicle communication. The intersection demonstration uses the track angles of the rovers to determine if they will collide. In a real world application, a more advanced system would need to be deployed, as explained below, since a system only using the two track angles has the potential to calculate false positives. Through tests of more advanced systems that used more measurements from the GPS and more sophisticated ways to calculate a potential collision, I learned a lot about this technology. 1) Bluetooth does not have the range to support this communication in full. Bluetooth also takes time to connect, and does not reliably stay connected between two moving vehicles even if they are in range.2) A more accurate method of location must be used in order to implement full collision detection in an intersection using more than just track angles. Either a much more accurate GPS is required, or more likely, an assisted GPS system possibly using cell towers.Why this is so:If the Raspberry Pi supported a larger bluetooth range, the vehicles could have been started at a greater distance from each other, and more information could have been collected up to the intersection, allowing for a more accurate collision avoidance calculation. I attempted demonstrations where the vehicles start out of bluetooth range, building up a history of data as they move, and connect once in range, but the bluetooth technology on board the Raspberry Pi had lots of trouble connecting the two vehicles while they were moving. I wrote an Arduino sketch that read both vehicles' latitude, longitude, speed, and track angle, and used their position history to plot their paths as lines on an Equirectangular Grid Model of the Earth. The Arduino then found the intersect of the lines, and used the two vehicles' velocities to determine if they would be at the intersection at the same time. This demonstration worked using test values, but the aforementioned limitations of the GPS and Bluetooth hardware led to inaccurate and untimely readings. In order to be utilized in a real intersection between two cars, these calculations would need to be supplied with data from an assisted GPS and a larger Bluetooth range.What I learnedArduino!I had never touched an Arduino before this project, and I have come to love how easy it is to automate things with one.C++Before this project I had worked with Java, Swift, Python, PHP, and SQL, but my past projects did not require any programs written in C++. Since the Arduino compiles C++, I learned a lot about the language, especially when writing the GPS library and NMEA sentence parser.NMEA DataIt was very cool to learn what most GPSs output, and how to interact with the GPS in order to get the information desired.Bluetooth Low EnergyI had always had a small fascination with bluetooth, and writing code that worked with it was very cool to me. I enjoyed how easy the Arduino 101 BLE API was to use.PWM (Pulse Width Modulation)I had used PWM in the past to control servo motors, but this project took me into the details of it. I learned the different between PWM frequency, duty cycle, and pulse length.What's next for Self Driving Arduino RoverStarting my first year of college in about a month, there will be many more resources for me to use in developing my projects. This will open up many possibilities to where this could go. I plan to continue developing the rover, adding different sensors to it and giving it new and improved capabilities. In college and beyond, I will use my knowledge of GPS, BLE, micro controllers, and more gained during this project to aid future efforts in making this planet a safer, more efficient, and more exciting place to live.Testing InstructionsContinuous Collision Avoidance ModePlace the rover in a fairly open area (such as a playground, backyard, gymnasium, etc), with at least 2 meters of space in between the front of the vehicle and the nearest obstacleSupply power to the Arduino by plugging in the cable attached to the power supply, and turn on the Electronic Speed ControlWait 5-15 secondsOpen iPad Rover Controller ApplicationWait for on/off switch to appear in center of iPad screenTurn switch on to start the back motor of the rover and enable collision avoidanceThe rover should start moving forwards and turning its front wheels left or right when it approaches an objectVehicle to Vehicle Communication Intersection DemonstrationPlace the driverless Arduino rover and the RC raspberry pi rover about 9 meters apart (try closer if the vehicles are failing to connect, the raspberry pi 3 has a relatively small bluetooth range)Orient the two vehicles such that they are pointing towards a common point, from directions around 90 degrees relative to each otherPlug the raspberry pi and the Arduino, turning the two devices on, and turn on the Electronic Speed Control on the Arduino roverWait until both GPSs indicate a fix (when the blinking red light on the GPS starts blinking much less frequently)SSH into the raspberry pi and start the intersection demo python programOnce the Arduino rover starts moving, using the Remote Control, drive the raspberry pi rover into the path of the Arduino rover, so that they are on a collision path and moving in directions around 90 degrees relative to each otherDo not stop raspberry pi rover; drive it straight through the intersectionThe Arduino rover should now stop and let the raspberry pi rover passNote: for both the sketches, the pulse length that determines the speed of the Arduino rover is defined by a C++ macro. The rover must be set at a lower pulse width (higher speed) to be able to move through the grass, but must be set to higher pulse width (lower speed) in order to achieve the same velocity on pavement (the rover goes way too fast on pavement if set to the grass speed). A pulse width of between 1050 and 1080 work well in the grass, and 1120 works well on level pavement. Pulse widths in between will achieve different speeds as well. A pulse width of 1140 will stop the rover completely.Built Witharduinoc++pythonraspberry-piswiftTry it outgithub.com      Submitted to    Intel Hacks 2017    Created by  This was a one man team!Chris BlustComputer Engineering student at Rochester Institute of Technology, class of 2022.  This application is arranged by people from different background so that we can find many creative and fun ideas to help other people being motivated to be PUNCTUALOther Challenges we run into is other free social media applications.    Setting up the Live Video StreamAside from coming up with the idea for Guru (which took over 3 hours between all of us), we faced two main technical challenges. They consisted of implementing a video chat service in our app along with having a live whiteboard to draw on during a video call. At first, the idea of a video chat system seemed daunting and we researched possibilities of transcoding and delivering our own content across infrastructure providers such as AWS, but in the end Twilio was found to be a less difficult way to build out this feature while allowing time to focus on the app and interface. We coded a demo app before integrating video with the main Guru platform and divided up tasks such as researching various APIs, efficient server management with video, and designing/developing user interfaces for the product. Integrating a Collaborative WhiteboardWe thought this would be the most difficult feature, but as we soon found out an even harder task was to build the dynamic drawing system. This required setting up a continuous websocket connection to avoid unnecessarily polling a server for new points to download and wasting precious resources. Parses newly open sourced Live Query project was slightly complicated and had minimal documentation but after many hours we figured out how to receive subscription alerts for newly updated points and were able to complete this feature. There were many other smaller challenges such as how to properly assign and delegate tasks among our group, as well as when to sleep (if at all!), but we are happy to have a finished product and to have overcome these challenges.  Creating service even I had never used NoSQL, node.js before.    Connecting to the Microsoft Project Oxford proved more difficult than expected on our Mac laptops than the typical PC. We couldn't integrate real 360 footage due to lack of Unity support.   The most challenging was choosing the proper tube for sensors. But we came up with the best solution - transparent silicone tube. It occurred to be the best for photo-resistors. Also it was a bit challenging to learn our system to capture motion accurately.          Orchestrating each disease with each vitamin essentially researching. All of my data comes from natural food books that I found at the library and transcribed to the site. I ran into problems also testing a learning system that could potentially aggregate all of your disease results and generate a vitamin catalogue from that information to test this concept I wrote a small program that uses ML concepts but ultimately was unable to find datasets that could actually help generate actionable insights.      The basic challenge was accuracy. Sometimes even Alexa fails to understand the exact user utterance depending on ambient noise , accent and network issues. My skill had to be reliable and precise so I had to add a secondary layer of Natural Language Processing on top of Alexa's existing Intent builders and NLP engine. I did this with over 1600 slot values and handling a bulk of individual string variations in my code.The second challenge was the probability counter. My skill, Medical Assistant only gives the diagnosis when that probability counter reaches over 97% for any medical condition. This was done by careful mapping of medical conditions with their symptoms. It took me almost 2 months just to do that. Eventually I cracked the code and successfully made the entire package up and running.    Make a minimum viable product as a proof of concept, and the video editing to present the project.  I have used ASK-CLI and Node.js to develop skills before, but nothing this complex. I had to take time to read the Alexa documentations, and join the Twitch streams to learn how to use APL.I had to create and record a large number of audio files to cover all of the possible loop variations that the end user could choose. I have studied music technology about ten years ago, so I felt a bit rusty creating the loops, but It was a great opportunity to catch up with some of my old college buddies to run my ideas past them and to get their feedback.There is also a known issue that I have encountered where card text overrides APL on the echo spot. I used to excellent Slack community to confirm this was a bug, and not me going mad!  How to structure the data efficiently and share it between a nice graph-based front-end and backend. OAuth tokens expire quickly. Ideally UnicornLabs wouldn't have to handle OAuth to enable mobile devs to actually use the native SDKs provided and just pass a token to our service.   We implemented lots of papers to try to implement STOA GANs which aimed to create high-solution images but most of them failed. Because training GANs is also hard, we normally spent a whole day and get nothing. It was a tough process, For examples, there are some problems we ran into:Unbalance between the generator and discriminator that cause overfittingUnable to converge  How to make hosts speak, how to recognize a specific image smoothly, and how to draw arrows to guide in 3D world and for a long distance!    The initial idea for the app involved using nfc or bluetooth to gage the range for the popup. Without the experience that would typically allow us to develop this kind of feature, we had to get creative with how to track location in a way that was accurate enough and allowed us to set parameters- this involved math... a lot of math... by pulling the gps longitude and latitude coordinates provided by every google chrome browser, we had to use trigonometry, conversion formulas, and advanced algebra to calculate the distance of any two coordinate points, and make it so that only people within 100 meters of each other would appear on each user's screen. It was challenging finding a new and creative way to get beyond this issue, however making it work felt really good when it was done. We also ran into some other minor technical issues, with things such as asynchronous functions/operations, typical bugs, and of course our favorite: syntax errors.   Working with different APIs and aggregating all the data together dealing with asynchronicity. We had some troubles integrating scripts with a Google Cloud Ubuntu VM, and had to do some hacky tricks to workaround limitations in a terminal-only environment.   Learning how to use docker was relatively challenging, and while the Openfaces software does the majority of the facial recognition work, effectively implementing its python interface was very time-consuming. Furthermore, since we are attempting to find your Doppelganger, we needed a large image database. Associated with this were long download times and image processing.The Endgame    The sensor not being installed properly. Mysql not being able to pick up the data it was being sent.     I'll be needing help from Full Stack Developers to make the back-end software work out :) Looking for devs to help out and make this IoT Device a reality :)  Authenticate against AD Azure    An way to find a perfect query to came with videos that makes senseWhat I learnedI really learned how to create a chrome extension. I've never done it before :)What's next for BroadbandTV: Yelp Video Augmenter The Yelp's RecipesI've a whole roadmap in the GitHub repository: https://github.com/andregumieri/vanhackathon-broadbandtv/milestones/RoadmapIt's open sourceThe source code is available here: https://github.com/andregumieri/vanhackathon-broadbandtvBuilt WithbrowserifychromegulpjavascriptsassTry it outchrome.google.comgithub.com      Submitted to    VanHackathonWinner                VanHack Premium - 1 year membership                  Created by  Andr Gumieri  While low level concepts are being captured and represented , I see that I may need a hierarchy of layers to capture higher level patterns. This is going to part of the next phase of this project    We had never interfaced MongoDB with .NET, so correctly adding all of our data was quite time consuming. We also had problems with our algorithm that searches along the route to find the flood events.      Blerg. Munging all this JS into one useable app was a bit of a pain. I ran into some trouble getting the app to run smoothly, which is probably because I basically copy & pasted two tutorials together. Originally, I had planned to use both color & head tracking. Colors to rotate the object, and head tracking to move the overall scene. I couldn't figure out why, but I think my color & head trackers were overwriting each other. \_()_/Also, I was planning on adding a "upload STL model" button, but I ran out of time.And of course, there's still tons of unused / outdated code left in the comments + a heap of extra JS from the examples folder that I haven't cleared out yet.    As this is our second plugin that we decided to submit for the hackaton we had really small time frame to finish the implementation.What's next for Dependency ViewerWe plan to expand the plugin from viewer to analyzer. Our goal is to help the user find and prevent potential problems fast and easy.Built Withatlassian-connect      Submitted to    Atlassian Codegeist: Add-on Hackathon    Created by  Took part in all steps, from the requirement  to the publishing on the Atlassian marketplaceGeorge StoyanovMaking sure the add-on is completed before posted and trying to balance between requirements and time limit.Boyan AngelovGalin Mladenov              The multimodality aspect was tricky, as screens are not encouraged to be used before bedtime. It was really tempting to get carried away with visuals, but the main point of this skill is to have the kids focused on their tasks and not get distracted by a screen. Therefore I decided that the skill should provide a full experience on voice-only devices, using sound effects, and the screens should serve as supportive visual cues. I chose a very calm visual language where each task in the routine is represented by an image that reminds the kids what they need to do. I also added a short text description for each task, in case parents missed the audio instructions and want to help their kids remember what they are supposed to do.    Time constraint, Busy Schedule; working and hacking, Power Supply.  figuring out how to detect the outlier patterns in the data    Most of our difficulties arose from integrating the Bot as an orchestrator of the application's flow. Combining Direct Line with the Bot's connection to Microsoft Teams became tricky while testing and developing. Pinning down bugs in the bot code was extra difficult because of the need for a live connection to Microsoft Teams instead of being able to test on a purely local environment.   Design is not my strong suit so I tried to implement Atlassian AUI for the most part of it but I ran into some issues that I fixed with my own styles. I definitely needs to spend more time on this.  Rerendering sidebar without refreshing the pageSuper messy code  The indico api expects just faces, so I had to implement live facial localization into the algorithm. Doing this while also fighting to keep the webcam view as smooth as possible was quite challenging  NC.js has many parts that need to work together. Creating it requires learning the STEPNC library, Javascript, and understanding the machining process. If one part of the application breaks it affects every other component, making it hard to pinpoint the exact problem. This behavior requires everyone on the team to have an understanding the entire program in order to effectively add features or fix bugs. Adding an independent view of the geometry seems like an easy task, but required the way geometry is loaded to be refactored entirely. Another issue is that the underlying library is poorly documented and doesnt always behave as intended. This means that when a problem arises it is difficult to tell if its a high-level issue, a low-level issue, or undocumented intended behavior.Accomplishments that we are proud ofSeeing the activity of a machining tool in real time allows for levels of oversight not otherwisepossible. NC.js enables supervision of an operation and confidence that tolerances are beingmet, without needing to be physically present for the machining of a part. Various tools can beused in the simulation in order to see if there will be any problems with a different size tool. Atool of the wrong size can be caught before machining begins by using the simulationenvironment to change what tool is being used on the machine.Because NC.js is designed to be web based it is possible for a part to be monitored anywherein the world. The software could be used in a commercial setting to prove the effectiveness ofnewly developed tools or optimized toolpaths.The largest possible benefit is in the potential automation of machining. Automaticallyensuring tolerances are being met could eliminate the need for human oversight.   Shown above in pink is some of the tolerance highlighting that is being done by NC.js as the Okuma is traversing each GCode operation. Tolerance highlighting will allow for the user tosee which of the current tolerances shown on the sidebar applies to the current workingstep.The end goal is to allow the user to change these tolerances and then save those changes toa new file. These tolerances also allow for error checking throughout the application to makesure that all operations complete correctly.Another benefit is the ability to view the geometry ,at various steps in the workplan, in aseparate pane. This will allow for STEP data to have 3D visualization that is before or after thecurrent working step being machined.The model shown above has annotations displayed throughout the model to show tolerances throughout the piece. The color data is also applied to the piece in order to give the user a better distinction between different parts of the scene. A workplan view is displayed on the sidebar to show the underlying workplan and all of the relevant workingsteps for each of the operations. The nested elements can be expanded to see the hierarchy of the workplan which is useful to machinists.The impeller model shown above has the bounding box of the tool and model highlighted. This box is used for many of the underlying rendering calculations. The impeller model also has some more intricate tolerances highlighted on the blades of the impeller which shows the potential of tolerance association with faces on a 3D model. The tools pane on the sidebar shows which tools are being used in the workplan.  Any tool that is defined within the workplan but isn't being used will be lowlighted in order to highlight important aspects to the user.The above shows the list of workingsteps, organized by setup. The simulation will pause in between each setup in order to simulate the need to change the orientation or tool of the machine. The toolpath is displayed and eventually a trace line to show any deviation will also be displayed.What we learnedDuring development we learned a good deal about web design with React components, as well as how to transition our simulation from using a preset path to the tool locations given by MTConnect. By saving state information at higher levels we are able to propogate changes to all components in a timely fashion. This enables all our UI elements to run in sync without losing information between transitions. Understanding how to read the MTConnect data took some time but proved to be a worthwhile time investment. The richness of the data allowed us to easily display all the relevant information. In addition we also were able to get important experience working as a team on a large code base, especially in regards to using Git as a source control tool. What's next for NC.jsFuture features will focus on statistical analysis and automatic detection of machining faults.Features that are currently being worked on include:Tracing the path of a toolProviding automatic calculations to determine possible deviations, detecting faultsbefore the entire part is machined incorrectlyComparing material removal to tolerances associated with each part faceThe ability for NC.js to make recommendations for feed rates and spindle speedsbased on optimization algorithmsVisually previewing tolerances, workpieces, and tools while the simulation continues inthe backgroundFurther goals are to allow for tolerances to be added or removed from a model using thesimulation. The end result can then be saved for future use with different tolerances than theoriginal model. We also want to generate recommended changes to the workplan based onthe differences between the simulation and the MTConnect data. These recommendedchanges would further optimize a model so that all tool wear and other costly activities canbe limited.Built Withcadjscss3html5mtconnectnode.jsreactsteptoolsapisteptoolslibrary      Submitted to    MTConnect AppsWinner                Second Place                  Created by  I have worked on all of the panes that appear on the side of the interface as well as all aspects of the backend API and 3D rendering view.Nicholas FayI have worked on various aspects of NC.js, including client-side code and interface design as well as backend work. In particular, displaying tolerance information and some of the CAD viewer controls were my work.Ian ChamberlainI did most of the frontend styling, including creating many of the React components for the header and sidebar.William Rigby-HallI worked on a lot of the back end silliness that controlled the simulation. I also spent time making the UI switch dynamically between "Mobile" and "Desktop" View.Patrick HesselbachI did mostly back end work and client side implementation.Stephen BealeI worked on the switch between the mobile view and desktop view, moving the state information to a more maintainable place by creating the ResponsiveView component and the mobile sidebar view.Robert CarneiroI created the solution for emitting, receiving, parsing, and interpreting the MTConnect information from the machine tool, as well as the code to drive the 3D simulation from the data received.Kathryn Lovell  The Polymer documentation isn't great. It took us a while to figure out the required dependencies, and how we should include them in relation to the polyfill. Installing the dependencies was also a bit of a pain, it would be cool if they were hosted on their own CDN. We had some dependencies installed via bower, some installed via NPM, etc.  Should we build the feed ourselves from scratch, or use a service?That was the big question. We decided we wanted to get it up and running as fast as possible, and learn from there. The decision was made to go with Stream. [spoiler]We never regretted it![/spoiler]We short-circuited our usual process, and built the feed as fast as possible, barely writing any tests, using a different Trello board. Overall, it worked great, and we had something people could actually try in about 2 weeks.RabbitMQThe first implementation of the feed was using our existing infrastructure to process activities in the background, namely Sidekiq. But Stefano, a developer at Devpost thought that it would be the perfect time to introduce RabbitMQ and move towards a more evented architecture. So he did.He'll talk about that in more details in an upcoming blog post, but the main challenge for the team was to still get the feed moving, adding new features, while Stefano was adding RabbitMQ to our stack, and integrating the feed with it.  Figure out what APIs are available on Cloud to make Canned Responses concept possible to move to Cloud.Webpack HMR (hot module replacement) is hard or impossible to configure in environment proxied with ngrok together with Vuejs.It's not possible to truly integrate with comment field as we do it on Server which lead to second comment box that doesn't implement many features from original comment box from JIRA  Working with new technology always presents challenges and we can't think of anything that wasnt associated to some challenge. Below are the major challenges we encountered:Understanding the BLE architecture and how to use it from publish-subscribe APIs to UUIDs for the services and characteristics.Learning how to manage the BLE states and hex data conversion within the Unity C# scripts.Figuring out how to convert the loudness sensor electrical signal output to accurate dB readings across a range of noise frequencies to display on the mobile app including use of signal to dB conversion, equations for mapping and smoothing and a SLM for refined calibration.Realizing our expected use for the Grove Vibration Sensor was not in align with the intended use for the sensor resulting in a re-directing of our focus toward the more important noise monitoring and tracking and removal of the scope associated with the vibration and motion sensing. We learned that our original expectation for the vibration sensor based on web findings, for instance, to detect and quantify the low frequency rumble of a motor or vibration imparted to the NEATVIBEwear device was just not correct.  We built an entire functional Android application on Android Studio, but it would only work for android 5.0 because of Bluetooth permission issues associated with 6.0+, so we had to scrap it and learn how to use Evothings in the span of two daysWe hit the API stream limit multiple times (at least 5) on plot.lyTwelve-hour time zone difference caused difficulties in testing and collaborating - two of us were in a different country for an entire month.Lots of difficulties getting Bluetooth to communicate in real time; we had no experience with it and had to learn a lot from the limited documentation.  The project initially had many hardware/physical problems which caused errors while trying to print braille. The solenoids were required to be in a specific place in order for it to pierce paper. If the angle was incorrect, the pins would break off or the paper stuck to them. We also found that the paper would jam if there were no paper guards to hold the paper down. Accomplishments that we are proud ofWe are proud of being able to integrate hardware and software into our project. Despite being unfamiliar with any of the technologies, we were able to learn quickly and create a fun project that will make a difference in the world. What we learnedNone of us had any knowledge of python, raspberry pi, or how solenoids functioned. Now that we have done this project, we are much more comfortable in working with these things.What's next for Braille PrinterWe were only able to get one servo motor which meant we could only move paper in one direction. We would like to use another servo in the future to be able to print across a whole page.Built Withbloodhardwarepythonraspberry-pisweattearsTry it outgithub.com      Submitted to    Hack Western 3Winner                1st Place                  Created by  Andy KeechIvan YungComputer Engineering student at Western UniversityMichael BurgessDaniela Arcese    Everything  Sourcing data from the devices took days of research. Thankfully, we were able to reuse open source work already in place to feed established tracker sites like FlightAware and ADSBExchange.Integrating dependencies between Spark Streaming and Kafka for the first time took several days to get right. Turned out much easier for Kinesis and EMR Spark.DynamoDB took some getting used to, given its restrictions coming from Cassandra.What's next for 1200.aeroContinue to grow user adoption and airport coverage.Better UI to depict altitude profile.Slack integration for use in the field by flight schoolsBuilt Withamazon-dynamodbangular.jsdockerecsemrkinesisnode.jsparticlescalaTry it out1200.aerowww.calido.co      Submitted to    AWS IoT App ChallengeWinner                Second Place                  Created by  I worked in the development of an NodeJS API to retrieve the data from the Cassandra Database and send it to the web UI. sdelosrios95Developed the integration between Raspberry Pi devices and Kinesis/Spark/DynamoDB back end, as well as the event detection logic (in Scala) based on own flight experience and that of fellow pilots.Ivan Vasquezhttps://www.linkedin.com/in/ivan-vasquez-0230766/I worked in the platform side building the network infrastructure, security, scalability strategy, container design , and automation. We migrated the frontend webUI user interface and  non SQL database access API from an Openstack solution running in premisses, to a EC2 Container Service. We used cloudformation to orchestrate most of the AWS operations including VPC, security, auto scaling groups, load balancer, ECS cluster, and more.      Oscar BermeoI worked in the development of the UI and backend APIs. The UI was developed in Angular 1.6, integrated with Google maps. The backend was developed in Node.js and DynamoDB as database engine. David Eduardo Morales  Integrating various technologies.To provide a completely functional loosely coupled android app in a day is not an ordinary challenge.   Discovering the geo localization Api using Google Map API.      Data filing in the directories is at times difficult to access via code based on the path is was stored in. Figuring out what some values in differing data sources meant or referred to led to some time consuming momentsSuggestionsREADME.txt files or code snippets of APIs consuming the data would prove really useful in understanding how the data is intended to be understood.     Fabric UI    Many. The valves for water we purchased weren't working, the NodeMCU couldn't connect to the CMU-Secure wifi, we ran out of epoxy glue, and much more.      I have encountered many challenges. First I have to optimize and try many and many times to work LM35 and Grove Temperature sensors because their values are different so the example codes get wrong data. When I switch BaseShield working voltage to 5, problems start. Although these sensors can work with 5V but I solved with trying and anlaysing. I also had a problem with connecting water sensor to Arduino 101 with LCD because LCD sticks when sensor is connected. So I used another Arduino. At last I think this is a problem for water sensor. I bought these sensor for detect rain but these water sensor is for water level not for the rain. I will go to my village and I can't get the Rain Drop sensor that's why I had to use.       Because of the limited requests to the Alchemy API, we had to test our app with repos with a low number of commits.   Most of the challenges encountered with the project involved communicating between the computer and Arduino.  We needed to communicate over serial to the Arduino through the C++ application, and we managed to find a third party library that allowed us to send the serial data we needed.  We then had to format the data so we could 1) update the motors quickly and 2) make sure we had accurate data on the receiving end to ensure the Arduino interprets the computers commands correctly. In terms of hardware, we had no idea if the motors would even move the cart because in order to get cheap motors, you have to settle for no specifications about what youre buying.  Thankfully that worked, and the motor controllers held up.  The Kinect has a proprietary connector that splits into data for to the computer over USB, and power to a 12V wall adapter.  We wound up taking our base 24V battery power, stepping it down to 12V through a buck converter, feeding that into a 12V car inverter that inverts it up to 110V AC, at which point the wall adapter rectifies and steps it down to 12V DC.The Kinect API also took some time to figure out, but eventually we were able to get data from it very consistently.  We originally implemented bang-bang control for proof of concept and initial testing, and then upgraded to proportional control to make tracking better.  Being able to integrate the Github API in our code and representing the data visually.What's next for lovedotjsAdding more data from Github like frameworks and starred repositories, creating accounts that are saved to databases and recommending other users at the same hackathon, using Devpost's hackathon data for future hackathons, matching frontend users to backend users, and integrating other forms of social media and slack to get more data about users and making access easier. Built Withazurecsshtmljavascriptnode.jsTry it outbit.do      Submitted to    TreeHacks 2016Winner                Dropbox - Most Cupcakey                   Created by  Wrote back end node.js server. The server analyzes the github user's repositories and creates a comprehensive language stack for later comparison.Jessica, Conner, and Anne did a fantastic job with the front end UI / UX. Great hackathon team overall. S/O to Anne for the awesome octocat logo!Edward HuFull Stack Web Developer. Jessica Jiang21. I like to make stuff.Anne Zhou  Mind-lowing algorithms about how we could store all the movements done in the present. Also the challenges we made were pretty difficult lol (level 4).    The robot in the P tried to rise up against me and tore the paper between the S and T. I put him back in his robot place so we're all safe. You're welcome.What's next for Devpost Sticker WallHopefully, it will hang in our office and remind us of all the great people we've worked with and the amazing stuff our users have built. If your sticker is represented in here, you are obligated to vote for me.Built Withmore-stickersposterboardscissorsstickerstape      Submitted to    Devpost Hackathon 2015    Created by  Stefanie MaccaroneHello!Cassie Breen        The major challenge for us is to rework on UI/UX part in short time after accepting feedback from our alpha users. Secondly, working around with Instagram APIs was hard initially, but we did it. :) Accomplishments that we are proud ofWe are extremely proud of hitting the sweet spot in the influencers' problem and building a platform to empower more than 10M Instagram brands and influencers with Kin based loyalty program in one click.Short achievements: Completing OkPerk Android app in a very short time, acquiring okperk.com domain name and 80+ Instagram influencers in waiting list. :)What we learnedWe learnt a lot about community engagement, Kin SDK, and viral growth effects of rewards or loyalty program. We never thought implementing cryptocurrency-based rewards program in our platform would be so simple. :) Thanks, Kin.org  What's next for OkPerkOkPerk.com comes with huge potential for business and viral organic growth. The best part is, once a brand or influencer activate their community with OkPerk, they would be sticking on to us for a long time. *Going live in beta with the 80+ Instagram influencers and brands in our waiting list who's average audience size is 5000 Instagram users and has the potential to bring in 100k fan to their Kin powered community. *Implement auto-reward feature and include the freemium model.Scale-up :) Here demo video link: https://youtu.be/eo3UqKLLeWgBuilt Withandroidandroid-studioapigoogle-cloudinstagrampythonTry it outplay.google.comokperk.com      Submitted to    Kin Crypto Challenge    Created by  Yugendhar Devale    Specially invented MerkleTree of addresses with truncated 160-bits hashes to protect from front-running attack and reduce QR-code size. See presentation slides.  Unity, unity, unity! Amazon Voice Services. 60 second limit      My biggest challenge with this project was time. I worked alone on this project from start to finish, so naturally I was stretched for time trying to accomplish everything I wanted to. I spent my fair share of time simply debugging strange behavior.It was also a challenge to hand certain data from python to javascript for the lyrics to be displayed in real time.  The challenges I faced in making this game is that there are some features in this game which are very difficult to add and there are no tutorial present on internet.I am a beginner in game development. I am pretty sure that this game will eventually come to real world in the near future. I really proud of what we achieve so far!            Understanding the user's messages and providing robust dialogue capabilities.  The leap motion requires multi-threading for gestures (unless I'm dumb and overlooked something). Unity doesn't like multi-threading.  We ran into problems with early versions of the layout and we always got together in Skype to take matters into hand and find solutions. We also ran into problems with time management, so much so, that we, unfortunately, had to choose to leave some of the original questions out of the app to better convey our ideas.  Because of the scalability of Kindex, we needed to maintain focus on our primary goal: to index family records and memories. Refining our interface for users of all ages, while also accommodating various digital media types (photos, letters, journals, video, etc.), has also been a complex issue  The main challenge I faced was developing this app on my own in a language I had never used before. It was also very challenging to extract the relevant details from deals and events referenced in emails, given the wide array of HTML structures out there.   Figuring out how AWS services interact with each other, how lambda works, serverless architectureResponding to slack properly and timely  In our initial architecture, audio was captured into chunks and sent immediately over the websocket. We were able to succeed in sending high quality audio, but loud pops persisted where chunks ended. We ended up spending time to buffer and connect the chunks before sending them over the socket, which helped immensely with audio quality. Additionally, after sending the raw data, playing it as a WAV was seemingly impossible. We had to research the WAV standard and, on the fly, add headers to the WAV files before playing.  Making a minimal value product instead of trying to build all the features simultaneously was hard.  It was not straight forward to get the information our of Akinator. We started with beautifulsoup to get the information extracted from the website but then it was less of HTML and more of Javascript and hence decided to use Selenium Webdriver to extract the Information.Since we were using our own Flask Server having the option of repeat on Amazon Lambda was a bit complicated since the session information is maintained on the server.The pages don't load at the same rate in Akinator and hence we had to choose a optimum sleep/delay time to provide good response times as well as to avoid unexpected exception scenario.We tried using multi-processing in Python to control the Sphero and have the Twenty Questions application working at the same time but it did put huge load on the RaspberryPi and lead it to hung state.Since each RaspberryPI has just one Bluetooth, we were not able to control multiple Spheros at the same time and hence we could not do Swarm Robotics.  We had a hard time setting-up our NPM-Typescript-Webpack stack and ultimately decided to drop it for vanilla Javascript. It also proved to be very hard to generate a pdf on the client-side using Javascript, but we managed to do it.      Using relatively limited processor/memory resources and having to do a lot of multi-threading to make the app look instantly fast while maintaining a large number of records. Timing out the (1)letter recognition and (2)starting the search needed fine-tuning until best results were reached.    Integrating all the different systems together was a challenge. Also, getting an admin account in to post calendar invites.  The challenges we have had was in shortPerformanceAuthentication and Consent modelPurchasing process in the Store    As usual the buggy API calls XD    The 10 hours was not sufficient time to include all the feature we originally planned to incorporate. If had more than 12 hours, then we could make our project a lot more sophisticated. Because we used embedded while loops, debugging became also complicated.  I was kind of hungry while making this.  The different intonations in some languages makes detecting speech accurately a challenge. Words within the same language with similar pronunciations make life difficult. Also, limited experience meant we decided not to implement this as a mobile app instead.What we learnedIt was interesting to learn about the current standards in voice processing tools. We were really fascinated to discover that there were open source libraries that offered text-to-speech abilities in almost any language or dialect that you'd care to name. This app also gave the both of us an eye-opener into web development with Flask. What's next for geoLingualStay tuned!Built WithflaskjavascriptjqueryjsonphppythonTry it outwww.geolingual.com      Submitted to    SteelHacks 2016Winner                Best Domain Name                  Created by  Amanda EngI inhale and exhale codeBrian HI live and breathe code  The most difficult part of FE was using Google Material Design in a mobile device. Backend faced issues trying to integrate with Slack since there is no feedback or debug options.    1) Extracting information about symptoms from user query 2) Maintaining session for chat NodeJS being stateless      Creating Score Board with Customized Opponent Names and Scores. [Using Python Script]Designing Themes and Questions.Content building.  Building our own live streaming system from scratch  The biggest challenge was to understand the Augmented Reality domain and implement it on an Android application. Designing and developing a UI for the application was another challenge given the absence of a UI/UX Developer on our team.Integrating google maps to support location sharing was another challenge that we overcame during the course of development of the application.  Playing the video at the same time was the biggest problem for us (well but that's what all the fuss is about). We solved it by using NTP and adding device states and sufficient gaps for network and bandwidth delays.    We are still lacking the skill required to build an app and the knowledge about Big Data. We plan to equip our team with the technical people though. .   Spent 3 hours debugging API result, Postman was set to Text, not JSON.ASYNC is Terrible!  The challenge was in the stage of the integration. The game we created is hosted a server and may take few moment to launch.  We ran into an issue with multi-screen: we recorded the incorrect screen sometimes in a multi-screen or multi-GPU environment.We ran into an issue with getting a beautiful window frame on each video, which we solved by compositing a pseudo HTML window via Canvas.We ran into an issue with authenticating and uploading to a serverless provider, where the app would fail if not authenticated.  we have two challenges.  (1) we are new to meteor and (2) it would take time to setup wifi with Raspberry Pi, so we used LAN for this hack.  There weren't many challenges with the core design but I did run into issues when trying to improve it.  First the library I used for the Curie sleep and interrupts is not "official" yet and has some issues.  I wanted to use PM.sleep(x) durations for my alert level timers to conserve more power but they were causing weird behavior.  Also, to eliminate the need for constantly checking the state of the switch in the state machine I tried to use normal interrupts but was having issues with detaching and reattaching the Curie sleep interrupts.I also experimented with adding the CurieBLE library to send a message to the user's phone when the gate was opened but the connection would always get lost when I put the Curie to sleep even though the Curie datasheet says BLE should be able to operate when the SoC is sleeping.  The firmware for Curie seems to still be in development in this area so I dropped this idea as I wanted this to be a battery powered device and needed the sleep state to work.  How to calculate the air pressure ? How to store the data on amazon cloud and retrieve it  ?Providing accurate values to the user.  It was our first time working with the Google Maps Api but it was awesome and We learned a lot.  Needing to create an accounts-trello package (none existed in atmosphere that worked for us).        Porting an API into a VR environment        Handling 2 apps simultaneously in a hackathon scenario is a HUGE pain which we realized after deciding to go forward with our project. Resource management and debugging is very hectic in this case.Apart from this handling of the online and offline scenarios and making sure data is persistent throughout sessions was a big challenge. This was the first time we had to think about scenarios where internet is BARELY available.  The first problem we faced was not being able to crop the Adversarial Patch, due to auto-croppers misclassifying the patch. We resorted to outsourcing to a human. (Thanks Li Ying!)When implementing the papers, Ming Liang encountered his greatest frenemies Linear Algebra and Keras. They presented the greatest challenge for him in this hackathon, especially the part using projection matrices to project the change vector to a hypersphere s.t. the radius of that hypersphere is less then a pre-defined value gamma. He died there.After studying 1 paper the night before and getting a crash course from Ming Liang, Jet (Jun Jie) faced difficulty trying to balance the requirements between adversarial machine learning code and web development fundamentals (from Bing Quan). He was stuck with linking web requests with ML outputs, and survived only until 6am. But he learnt a lot, at least :)  Getting images to store on parse and then downloading them.Deleting images off of parse after they have been viewed.User Registration and finding friends.    Getting the accurate distances of Estimote Beacons to our phones, integrating text to speech into our project, and setting up the environment for Estimotes.   Research and get 'up and running' with random APIs were not trivial tasks. Free or trial weather APIs are very limited. Also, the holiday API stopped working for a weekend, and we almost had to change providers. And finally, we had to build the front-end in record time as well..  As we wanted to maximize aesthetics, we had some high resolution assets; this posed a problem when loaded using some devices. As we were on a time constraint, we had a backlog of features in mind that we later added on top of a working prototype. This resulted in some tightly coupled code, as we didn't take the time to make full design docs and specs.   Restricted access to travel APIs (with more functionality) No consistent API that offered us exactly what we needed so we had to rely on a host of different travel related         APIs, such as Sabre, Amadeus, Lufthansa etc. There was no Starwoods API either so we had to write our own         scraper which we turned into a microservice and deployed on OpenShift Unfamiliarity with OpenShift so there was a learning curve but once we got the hang of it, it was really great to         use    Finding suitable DCOM files on the internet was particularly difficult due to privacy concerns in healthcare. Many of the DCOM files were not good.Accomplishments that were proud ofWe are particularly proud of pulling together a broad amount of research to prove the presence of a problem for radiologists. Its easy to say, this is a problem I will solve, but it is difficult to quantify and prove that it actually exists. We have very clearly shown use cases for the product. In addition, being able to take DCOM files, which are 2D, and turn them into 3D is far harder than it sounds.What we learnedBy doing a deep dive into the state of the radiology service line and exploring it to determine where and how this product fits in, we learned a great deal about the different parts of the industry.Whats next for ScanShareWe want to further expand the program with knowledge trees to help doctors. It should have natural language processing to derive and give diagnostic recommendations and knowledge based on the notes of the doctor. In addition, it should also be able to use algorithms to predict the diagnostic group in which the scanned fracture or problem may be in. This should be done as a comparison between a healthy model and the model being analyzed. We would also like to expand on the collaborative aspects of the platform to include systems where experts may give assistance to radiologists.Built Withgoogle-cloudherokumongodbtorchuipathTry it outwww.scanshare.megithub.com      Submitted to    MedHacks 2019Winner                Best UiPath Automation Hack                  Created by  I developed the ScanShare mobile app by employing a smooth UI/UX experience for radiologists/surgeons in a mixed reality setting.Sai AguruI created the back end pipeline that takes DICOM files from users, converts the files to STL models and pushes changes to Dropbox.Alexander ProschekI was able to help with the formation of the idea behind the project. I researched the background as well as different medical scans and how to identify parts of the body in these scans.Riya MehtaDhathri Bobba  This was the first time I've used react to build a full web app, so everything to do with that required some learning.  Having to integrate new and numerous technologies into a single product in such a limited time.  Timeouts, tricky webpages, and similar things. If you are familiar with web scraping, then you can know that this is a continuously changing and challenging topic, where there are new challenges every day.  On the mobile end, we wanted to make the application target Android 10. This meant carefully navigating various privacy demands that were both scarcely documented, and completely new to our team's developers. Meanwhile on the back end, it was tough performing feature detection and filtering out noise with OpenCV.     While brainstorming logic for isolating the successful transaction log from unsuccessful transaction log.  The BLE libraries do not do a good job of cleaning up after themselves and are not well designed for a system which needs to switch between both sending and receiving BLE packets. In general hardware adds an additional level of complexity and things "just not working", and once you move to wireless data, that is hugely exaggerated.The amount of BLE chatter is unbelievable, and when you don't even know if your device is working, trying to pick it out of a collection of 190 BLE scanned devices becomes tedious. :)  The main challenge for the project was to do all the best for the application, to work fast, and efficient. Debugging was also a challenging part of the application because we experienced different problems which we never encountered before.  Creating a (mostly) voice-only product with external triggersAPI integration, especially when attempting to query large datasets from Lambda  One challenge we faced was handling multi-server apps. Of course, the app becomes highly network-bound, and teamwork becomes more important.Tuning the image processing was also a challenge that was resolved by filtering the images to pop out the text.    At the start, only two of our five members knew how to develop applications for web browsers, so they needed to guide the other members in the technical portion of the project. We also ran into issues integrating the two open-source bases with our existing knowledge to get the game to function. Game design presented a challenge - how can we make it fun to record samples of the player? How can we adapt pre-existing projects to meet our needs? We ended up simplifying the design to open the game up to as many different languages as possible.There were also the many inevitable issues dealing with resource loading, and microphone access and data collection.    A major challenge was to incorporate AJAX in posting complaints. Integrating the Dropzone plugin was another task difficult to implement for us.  Learning Angular, Git, Grunt, getting good at Bootstrap and CSS, learning Node.js and Express, learning about sessions, promises, Spotify API, YouTube API, and learning about design and UX, contacting those API to overcome difficulties, fixing countless user errors and try to make the app API codes secure. Making sure the user won't break the app by changing playlists mid conversion, etc. A lot of work!    While we are very hopeful of the aim of our application to help people, there is one challenge that we need to hurdle which is the cooperation of the airlines. We need the airline to open up this possibility of changing the existing booking and opening up an api for this application. At first it may look as an additional effort to the airline but we believe that this can help the airline's image to the general consumers in the long run. Airlines that participate in this initiative can provide their loyal patrons more satisfaction knowing that as a buyer of the ticket, even though schedules change, they will not totally lose their hard-earned money by providing an option to swop their tickets. The airline also can maximize their resources by flying with almost no vacant seat which is more environment-friendly.What's nextWe hope this solution will gain traction and change the way the travellers book their flights. We hope airline companies will be aligned with this proposal. We also hope that other major players in the industry, such as Amadeus, who have been providing the best industry solutions to power the travel industry to partner with us. After all, we believe that in order for the travel industry to prosper, we need to give the best experience to the travellers and value them.Built Withbulmacryptoethereumethers.jsexpress.jsfirebasemocha.jsnode.jsreactsmart-contractsoliditytruffleweb3.jsTry it outgithub.comgithub.comgithub.com      Submitted to    Hack the Journey Online EditionTorus 4 Everyone    Created by  Karl CamotaAbiel Villarosa  Accurate detection of emotions via text is a work in progress, but far more accessible to developers than it was even a few years ago.   Among the challenges we ran into were the signal processing aspect. The Muse Headband actually has a lot more variables, including accelerometer data. Isolating what we needed specifically was an initial challenge. Another major challenge was interpreting the EEG data. However, we were able to find papers and MATLAB toolkits that allowed us to perform these complex calculations such as FFT and PSD easily.We were fortunate enough to be allowed to use the Muse Headband and Amazon Echo from MLH.     One of the biggest challenges we faced was integrating with Google Calendar. Accessing an external API is an asynchronous procedure, and we had to make a clever series of callbacks from the chatbot to fetch and update events in the calendar.  We expected the cards to used RSA2048 signatures as documented. Instead we found that they use 384 bit elliptic curves. RSA is easy to implement thanks to EIP198. Elliptic curves are a lot more complex. It is especially hard since 384-bit math does not fit in the EVMs 256-bit words.Gas cost is an issue. When we first got it working, a signature verification took half a billion gas. After some precomputations and optimizations we got it down to 20 million. We have two more tricks (Jacobi coordinates and base 4 precomputes) that we did not get to implement. With these included gas cost will be below the block gas limit, making the wallet viable on main net. There is likely room for further tuning.    Connecting the separate languages was challenging to achieve and vital to our project's success. This connection was especially important when joining the chrome extension(javascript, HTML, CSS) and our neural network(python). Additionally, since we are relatively new to the field of Artificial Intelligence, we chose a network architecture (bag of words and CNN), which it turns out is not optimal. If we were to do it over again, we would try to implement an LSTM which would provide some improvement in classification.    Using Google Cloud Platform has been the only real challenge we ran into. Being all first-timers with the platform, we spent a lot of time dealing  with parsing through the documentation to find the proper way to establish a connection with our server.       There were problems when integrating Framework  into the other projects (by copying Framework folder into another projects root directory and deleting the project.json from the Framework root directory).Framework got broken because of project directory into which framework was integrated to was now considered as a root of the project. Because of this relative paths were not working no more.Easy solution  was to force users to have project.json in Framework directory also. But that was not a satisfing solution.This led me to write code that makes Framework selfconfigure itself on startup. Now Framework can be put into any folder or sub-folder of a parent project and it will still run, even without project.json in its directory.What's next for UiPath-Testing_FrameworkWhat's next in the near future is adding, and refining Assert methods code to include even more situations.Idea for the far future is to develop custom activity which would, when Framework is run, popup a GUI from where a developer can filter which tests to run.Built Withc#uipathTry it outbitbucket.org      Submitted to    Power Up AutomationWinner                Overall - Grand Prize              Winner                Best Automation Framework                   Created by  Stefan Krsmanovi  UiPath is a new technology for all of us, so we spent some time trying understand its components. Also, it was really challenging for all of us.  In making our website, we had issues with connecting our domain name to a server. We had a few problems when connecting our app to our website to enable live location updates. An additional issue that our team had was getting used to how to use GitHub. While it is somewhat easy to use, we had a few issues in updating our website including a number of errors that caused our website to bug out.  Some of the challenges we faced were : configuring, calibrating, and capturing the raw sensor data from the Myo Band.  Combining multiple simultaneous sensor inputs to precisely control the robot. 3D printing errors.  Creating a user interface that would be simple and intuitive for the users.    Lots of the user interfacing was much more challenging than expected, as making sure everything worked on all screen sizes required a lot of hacking.        It turns out plant care is one of the few remaining things that don't have an API in 2016. We've had to manually gather, curate, and organize a bunch of data from disparate sources to populate GROOT with valuable information.A team of 4 developers doesn't make for easy design, UX, or marketing. We definitely dabbled outside our comfort zone!  I've done a lot of hacky front-end projects like this before, so everything came together pretty quickly. I had already solved most of the tricky bits in other projects, enabling me to reuse a lot of code. I'm proud that I was able to get it all working in a few hours. Makes me want to work on it more.What I learnedWow, it is so much easier to prototype a project when you're not worried about how it looks. Focus on the function!What's next for Job BotCSS/styles. Implementing file uploads + a Ziggeo video recorder + app management. I'm really bullish on video applications. I'd also like to build an email processor (sendgrid & mailgun have great APIs for this) that lets hiring managers pass/save applications via email. (e.g. "reply save to save for later")Let me know what you think and I'll use it to improve these prototypes + pass it on to Product!Built Withfirebasehtmljavascriptjquerylocalforagemailchimpmustacheziggeo    Created by  what comes before alpha release?Neal ShyamI write Devpost's weekly newsletter. I also write code.  Figuring out how to structure the data for both doctors and end users to track their status.  As our solutions integrates software with hardware, one of the main challenges was to come up with a solid vision of our technology. In order to build a reasonable model, we had to decide specifically the functionality of each component of our prototype, and figure out, how to make them feasible. In particular, we had to decide how to relate the mathematical model for predicting the possibility of having a particular piece of ghost gear in some area (heat map) to the software solution (platform) for reporting found items by volunteers and divers. In addition to that, we had to work out the value proposition of our solution for the users, the fisheries, and the government. Overall, the ideation process was the major challenge for our team.  It was challenging to create a platform from traditional concepts and incorporating it into a package solution. AccomplishmentsWe completed the platform in record time and worked well as a team. What we learnedOur lessons include taking more time to shape our ideas, e.g., ideation prior to starting coding.What's next for esusuWe plan to launch our platform this year! Our team plan on continuiing this project after this hackathon.Built Withangular.jsdomain.comexpress.jsmongodbnode.jsoffice-365radixTry it outff4893fd.ngrok.io      Submitted to    TechCrunch Disrupt NY 2016    Created by  Kaweenia Cootemahdi hamdiAmos AddyDI LUONGNikki PuckettIain CostonDamon ChenJohn Katt  Taking old laptop parts and adapting them into a small, simple, and easy to use device.Creating the form of the device to be fit inside the mouth.Creating the interface to control the mouse with the device.Accomplishments that we are proud ofAfter many hours of hard work we were able to connect the device to the computer and use it as a mouse. We are able to create a form that, when connected to a mouth guard, it is able to stay fixed inside the mouth and be used to control the computer mouse. What I learnedThe importance of good documentation in third party APIs that are meant to be used by others.How iteration is key to the success of a product. The importance of good team members to guarantee a great outcome. What's next for PaletteMaking the device wireless. Improve the shape, form and texture of the device. Built Witharduinoc++Try it outgithub.com      Submitted to     NYU ABILITY Technology Hackathon    Created by  DeveloperOliver HoffmanHardware/Electronics HackingShawn BramsonConnect with me on LinkedIn!Daniel LevineI enjoy creating things that genuinely help people.      Keep a 60 fps framerate is always hard developing cardboard games, and develop some more challenging puzzles.    Building a great optimization tool required us to dig deep into the apis. Not only we wanted to build a tool that found the crossing paths of many hackers, but also a routeing system that avoids areas that are more dangerous too.Accomplishments that we are proud ofWe were able to also build a chat-bot on Facebook that uses AI to converse with the user.What's next for Tripper?Expand the web service into a full fledged social network for avid travellers and adventurers. Incorporate crime data from more cities. Work on our AI to the facebook chat bot for the service Suggest itineraries for upcoming trips.Built Withjadejavascriptnode.js      Submitted to    MHacks 8Winner                Esri - Best use of Esri Technology                  Created by  Frontend, worked on route optimization, UI+Logo Design, Server Management Shubham NaikDevContributed to the server side of the web app and handled API callsAnant GoelContributed to UI/UX.Shelley PhamProject's backend + the Facebook AIMinh HoangGoogle SWE Intern, 2017      Time management for Hackathon (, this is our first time to join.)  Sloppy APIs  The challenge is the fact that the body of the mail has a somewhat strange html markup. We needed some plumbling to fix this.    The Lexalytics Semantria API proved to be a challenge to set up. The out-of-the box javascript files came with some errors, and after spending a few hours with mentors troubleshooting, the team finally managed to get the node.js version to work.       One of primary challenge we ran into is to understand the various reasons of readmission.  With the help of our healthcare provider partners and available sources in public domain, here's the summary of causes for readmission.Patient truly does not understand the seriousness of their illnessDont understand their discharge instructionsDont want to admit they have  trouble taking care of themselves at homeAre not able to get their medicationDont understand the importance of taking their medication/s as directedDont understand which medications to take and whenNot able to set a follow-up appointment to see their PCPConfused with how they are feeling during recovery therefore they stop following their treatment planDont feel well when taking their medication/s so they stopFamily caregivers are not knowledgeable enough to provide proper careKeeping it simple and meaningful : Its an ongoing challenge of keeping solution simple and avoiding overcrowded information and features. We have worked on  several cutting edge technologies including deep learning, beacons, augmented reality, smart watches, LBS, GIS etc. from technology side. Coming from technology background, we were always excited about possibilities of bringing a positive health outcome using technology. Challenge that we faced is to really say no to technologies which are not yet ready and identify meaningful use case for ready to use technologies.  Working with RGB and hue was very difficult. It is difficult for a computer to differentiate between colors such as Red, Orange, Brown, and Gold. As such, we tried multiple methods to identify the location of the colored bands on each resistor - such as Floodfill, normalizations, and transformations. Ultimately, we were able to find a way to identify the colors and its location using a graphing technique, and local maxima.  Missed a lot of sleep.    Lack of Wi-Fi.  Deciding on an idea and lack of sleep.        Learning to use Indico to perform image processing as well as dealing with session cookies for user login storage.Using the accelerometer to also reward walking in order to pick up litter to prevent users driving which negates their environmental efforts.  Chanlanges I  would like to mention hereSpecially enterprise organization are not familiar with the difference between Office Add-ins and COM/VSTO add-ins. Office Add-ins have a huge advantage in compare to COM/VSTO Add-ins.Not many are familiar with the Add-in security model and the requirement of an Office 365 Global administrator having to give consent. They simply haven't been told that this is a key element in providing them the security they want.The Planner API is not updated fast enough in regards of what we want to develop :-)  This is my first APL skill, so I had to learn how to properly use it. I wanted the skill to be authentic, so I researched the local dialect and consulted with an Australian friend.      Over the course of the implementation of our verifiable data chain we ran into the following challenges: Providing DID identity to individual data sets and algorithms No reference implementations for verifiable data chains No common semantic standards for automotive digital twin Lack of validated source code for scalable issuing and anchoring of verifiable claims (>100.000 Tx/s) Getting access to sensible OEM driving event data and real algorithms Lack of E2E driving event data models and architectures (location, transport, events, processing)We solved (1.) and (2.) with our data chain implementation. We researched W3C vehicle signalling standards and semantic web as well industry 4.0 standards to address (3.).We are testing the sidetree protocol on IPFS that might lead to a solution for (4.). We are planning to benchmark the protocol in the next weeks.We are trying to address (5.) with BMW.  Data model implementation patterns need to be created in order to enable effective implementation of cyber physical systems (CPS). This is a general task to be done regardless of the use of DLT technology. To address (6.) we are working with the German Centre of Artificial Intelligence (DFKI) on a CPS integration methodology, a data modelling methodology and reusable data model implementation patterns for verifiable data chains.Accomplishments that we are proud ofWe accomplished the development of verifiable data chains including the following technology components:  DID manager  Secure key management  HD identity wallet  Verifiable claims  Deployment on severless cloud functions on AWS  Integration of tensor flow algorithms for image processingWe are now planning to integrate historic agile driving event data from an OEM and a black box algorithm with our existing data chain solution on Amazon Web Services.What we learnedOur primary leanings are around the following areas of interest:  Abstraction of the DID method to any object  Identity solutions for machine learning agents  Scalability of off-chain data structures  Integration of serverless cloud infrastructure with key management and identity wallets  Verifiability of data chains  Applicability of data chains for above mentioned data-driven business solutionsWhat's next: our broader objectiveOur broader objective is to establish a scalable digital twin protocol and a technology layer for autonomous things. We call these digital twins autonomous digital twins that are verifiable, semantic and privacy-preserving.We are integrating further W3C and Industry 4.0 standards to establish a semantic digital twin network. Our work integrates further innovation in cryptography and privacy-preserving solutions to achieve GDPR compliance for both, the human and non-human entities.We are looking forward to field test our solution in a complete mobility ecosystem.Explainer VideosIntroduction to Verifiable Data Chains - Video Part IIWe created a video to explain the concept of Verifiable Data Chains for driving event data processing: https://vimeo.com/310934948Explanation of our Verifiable Data Chain  Demonstrator for MGC - Video Part IIIThis is an explanation about our verifiable data chain demonstrator:https://vimeo.com/310937978how to use ithow it workswhat you see in our demo implementationSafer & better mobility through Spherity's verifiable Driving Event Data Chains.References[1] A DID for Everything - Rebooting Web of Trust Working Draft[2] W3C DID Specification[3] W3C Verifiable Claims[4] Decentral Identity Foundation[5] Dangerous Driving Event Analysis System by a Cascaded Fuzzy Reasoning Petri NetBuilt Withaiamazon-web-servicesblockchaindidsdigital-twinningethereumjavamachine-learningpythonsecure-key-managementverifiable-claimsTry it outinterlinked-client-app.s3-website.eu-central-1.amazonaws.com      Submitted to    Mobi Grand Challenge Hackathon    Created by  CEO/CTO of Spherity GmbH. I did the tech research about verifiable data chains, secure key management and digital twinning for mobility applications. I worked with our dev team to test different implementation options and to develop real-world prototypes.I am a physicist by training with a Ph.D. from the University of Aachen. I served served as a Council Member of Global Future Network for the World Economic Forum and worked on their future of transport research.Prior to founding Spherity GmbH, I worked for innogy SE, German Aerospace Center (Deutsches Zentrum fr Luft- und Raumfahrt, DLR) and Accenture GmbH.Carsten StoeckerCEO/CTO of Spherity GmbHDr. Michael RtherRicky ThiermannOleksandr Yenkalov    The main challenge for us was navigating through the Facebook API and integrate that into our swift code within the time that we worked on the application. Connecting to the swift 4.0 url challenges.    We had almost no experience with openCV, machine learning, and the dragon board, we needed to learn everything to make this project happen. We struggled against the proccessing power of the Dragon Board to get the openCV library installed, along with the solution causing hours of struggle making a executable for linux debian.    Understanding the complexities of DNA sequences was very challenging for us since we do not all have biological backgrounds and in order to provide useful output, we needed to ensure we had a grasp of the science. Moreover, working with big data sets and many files was not easy. What's next for GenomeAi:We would like to create a database of many genes in order to incorporate AI and machine learning to predict which mutations in oncogenic sequences are the most important. Moreover, increase the functioning of the GUI and increasing the number of statistics on the data in order to help doctors and researchers as much as we can. Built WitheclipsegithubguijavaloveTry it outgithub.com    Created by  I worked on analyzing the FASTA sequences and finding the oncogenic mutations and determining which ones were the most crucial using Java as well as some UI development. I learn't a lot about bioinformatics and mining huge data sets.aaronsossinCreated entire GUI and ensured that all code provided by teams members functioned togetherajaypatel24Most of the front end and integrating the neural networks into the project to stream line the analysisSamuelRahmanWorked on the backend. Most involved in integrating the data sets and the systems analysis of the programs. Used our program to help detect risks in high risk population. Critical in our presentation and organization of our project. SimonRahman  The biggest challenge was synchronizing between the Unity client app and the web server, especially with the near real-time data streaming. Although Amazon does have an AWS Unity SDK, it only supports a small subset of all of their services, so I had to use a web server to communicate to AWS on the app's behalf.      A shitload of data to input (at the moment of this post there were 800 lines in the excel document linking the 300 photos, and defining interractible areas and objects)  Working with Facebook. Aparently, we need validation from them to make the bot public and the whole process might take several weeks.  The main challenge was trying to build the entire pipeline with glasses at first, we pivoted to using a phone. Another challenge was also in objects/people recognition.  Trying to understand the intents in Alexa through AWS Lambda was difficult for the team. Then, uploading the Python scripts made by the team to AWS successfully took multiple attempts. The biggest challenge for the team was to reproduce the Spotify playlist onto Alexa as Amazon Echo doesn't allow developers to address third party apps through custom skills.  Since we couldn't run our bot without a public endpoint we realized that we couldn't use the Wayfair codebase. We almost pivoted and attempted to make a bot that would just live on Wayfair.com but we all wanted to work with something new so we kept with using messenger.  There were no actually big challenges since using the UiPath products is very simple and intuitive. The documentation offered on the internet is detailed and sufficient. The only challenge we had was with making our custom activity more appealing by having an icon and displaying it under the right category in the UiPath Studio. We had to start learning Windows Workflow Foundation.  Specifying the system in such a way that it has a "reasonable" and enforceable privacy policy (who can access what documents, and what operations they can perform on these) presented an interesting challenge and is still evolving.Email parsing can be a pain, especially with the large fragmentation of the email app market, but I think it is a challenge worth undertaking since I believe email will be around for some time yet.      One of the hardest challenge I had to deal with was to achieve a smart ink detection on the manuscripts. I'm using a self-made algorithm that uses grey-level gradient analysis and reconstruction scheme. And it does work just fine, whatever the type of manuscript and their quality. The algorithm allows the user to get rid of any bleed-through on any manuscript. Another one I'm proud of was to connect my application to the internet: automatic updates, active embedded technical support... it was worth the work.    Integrating different APIs  importing our custom sounds that we recorded caused many troubles. Additionally the score you are given at your completion would not stop increasing ruining the point of a score.  We had our fair share of challenges on this project:Deciding on our idea, and what features we should support took over five hours of our most productive time. We initially tried to implement way too many features, and didn't really get anywhere for a long time. We finally decided on a minimum viable product of having locations, and ways to move between them. This was fairly simple to code and we moved on from there.Originally, we used JSON as the base of AdvLang but we quickly realised JSON syntax was not really great for what we were trying to achieve. After trying several different approaches, parser generators, and our own recursive descent parsers, we finally settled on our current approach, which we think maximises simplicity and readability.Getting curses to do the "right" thing was a pretty long process. We originally had the user type in their choices, like a proper adventure game, but decided that offering a menu of choices would improve the experience. Little did we realise how much work there'd be to get a menu working inside a terminal!  There were plenty of challenges, one of it being, hosting the python script on the cloud.  No one in our team had game development experience, but we installed Unity and learned SteamVR SDK from online resources to bring flappy bird to life!     Weatherdata.. is super expensive or inaccurate.. UNTIL i found the solution.. darksky.net (super awesome weather API! at a fair price)  The main challenge was to make sure the service is able to handle large traffic. With a set of caching practices and Redis SmartLogger is able to scale up easily.Business SideSmartLogger will be a paid service. There will be multiple membership plans with different limits of Traffic and number of Channels. Similar to other log collecting SaaS platforms.Technical DetailsDeveloper(s) working on company's app, can define the bug categories, let's call them channels. They stream the logs on the channels. Each of the log entry must contain the MS (MonetaryScore) attribute which defines how much damage company takes when this bug occurs (This is of course arbitrary value). SmartLogger keeps track of bugs and their MS's to calculate the total damage of bug received on a particular Channel.SmartLogger uses the data generated above to show insightful information about the processes that cost company the most amount of money.The beauty of our solution is that it's very flexible - It can estimate the amount of damage taken starting from simple error 404 pages to much more complex UX issues. Built WithcurllaravelmongodbphpTry it outgithub.com      Submitted to    Delivery of Things World Hackathon    Created by  Nick ChikovaniGiorgi Gedenidze  As usual with startups, money has been our biggest hurdle. We had the passion, we had the ability to create something useful, but we had little money. We could solve it by doing some other projects for other people, and while that has been fun too, it has provided us the money we needed to build this system.    Creating an easy to navigate UI that allows hiring managers to easily share new applicants submissions while displaying company feedback in a helpful way    Machine learning and sleep. One issue led to the issue of the other.   It has been a huge challenge to struggle with the control of the interferences and to be able to finish the whole project in time during the Hackathon, we were working with a technology we were all unfamiliar with (as we all have CS background, and are not familiar with waves and interferences).It was also a challenge to be able to verify its behaviour, as it's based on a super delicate equilibrium and the created spots are invisible and difficult to find.  Coming up with a viable hack is hardAfter discovering that the Oculus Rift doesn't really support Unixes (sad-reacts only), we had to pivot. We pivoted four times before we came up with imgrep. After that the code just flowed.Technical stuffOCR is kind of slow, so we made imgrep fast by caching the results of Tesseract OCR's processing in a small sqlite database.      Some of the links returned by the tool that read the email are not useful like un-subscription links for example. The challenge was to identify these links programmatically and exclude them from the users list. We also provided the users with ways to report these links to exclude them in the future.Setting up the mailbox was really difficult    We ran out of wire and had to improvise a couple times.We burned some stuff (only 2 fires this time!)We shocked some stuff (but nothing actually broke!)The BLE module purchased ended up actually being a cheap knockoff with very little functionality.  We had to rethink our method of attack to get the project to work.  We had to decompile someone elses Android application to figure out how to write our own to interface with the BLE device correctly.    OpenCV is hard, especially with Python 3. It was also difficult to mount the servos on an assembly of some kind. We ended up using a beer bottle!  Time constraint for deep learning is beyond our expectation. Usually, it takes 2 weeks to train a fully functioning LCD, but we do not have the luxury of time to make a complex and sophisticated LCD from scratch within 2 days. It's depressing to realize that the sophisticated version that coders have spent the entire first-day writing might not be able to implement completely. However, we made some necessary adjustments and manage to present the first version within 48 hours.       Rethink the online shopping experience, to build something simple and friendly, that inside a messenger phone chat.Some requirements that are implicit on the platform are hard to find on the documentations (like the length of the words on cards).Some useful features related to the platform that currently are not supported by Lex could be useful for these kind of bots, like the sharing button or open a link on the browser button, what made us directly post into Facebook using their API when we need to use them.The lack of a way to notify the user when the session has expired makes the conversations somewhat weird, could be fixed by implementing some infrastructure for caching the state by us but it would be nice if instead we could say "Bye" to the user.The lack of a way to put a custom userId on the lex testing panel,  that would have allowed us to do more accurate and quick debugging, instead, we had to deploy a version with a Facebook id harcoded to being able to test it.## What's next for Personal ShopperWe want to make our Personal Shopper a smarter assistant:To recommend clothes according the products that users loved.To suggest some items that match the clothes users bought.To send alerts when an item on the user list is on Sale, or when some new product that user might love is on sale.We want Personal Shopper not only think to of our style, but also in our pocket. That means, to recommend clothes according to a price range.We want the conversations to be more natural. Start them with versatile questions that might include filters on them.DependenciesThis project depends on this other two repositories:nameURL@lateralview/aws-lateral-chatbot-commonhttps://github.com/LateralView/aws-lateral-chatbot-common/@lateralview/aws-lateral-chatbot-adminhttps://github.com/LateralView/aws-lateral-chatbot-admin/Built Withamazon-ec2lambdalexmernmongodbreactTry it outgithub.comgithub.comgithub.com      Submitted to    AWS Chatbot Challenge    Created by  As Product Management of the team, I helped designers and developers work together to define goals and maintain a vision.nfigliozziNahuel VeronDiego GarcaElisa Goyeneche  Our team ran into a number of challenges during development. Being a newly available technology, references were sparse so we were not able to find the exact behavior of some of Amazon Lex slot types, which led to some guesses on how user input would be interpreted. Creating a Lex intent loop was also a bit harder than expected, and we wound up using a continuous ConfirmIntent loop with multiple optional Slot Types as a bit of a hack.  As a result, we had to perform additional user input validation to ensure Lex wasn't registering a "negative" sounding artist name as a "status denied" confirmation, as was the case for the DJ name "What So Not" for example. Creating a proper timer also proved to be an issue, as we wanted to give the user an option to extend the timer while being mindful of Lex timeouts.  We eventually decided on storing the time with the rest of the "state" information, which allowed us to restore state without worry of intent expiration.Prioritizing concerts was also a hurdle, as we had to factor in the nature of the search result (whether it was by related artist, by genre, etc.), whether or not there was any artist overlap with previously displayed events, and whether or not the event received a certain percentage of votes in a previous round of voting.If you plan on terminating a voting process after the last vote is placed, leave a bit of a buffer for misclicks and changed opinions. People love to change their mind after a vote has been placed.  This is actually my 2nd powerup to develop for Trello, the first one was not a really great idea for a powerup mainly because it's main purpose is already existing as a built-in function in Trello ( lol ). Well, Trello's api was really easy to learn and adapt into, But one challenge that I had when building this(and the other) powerup was how Trello's authenthication works. Eventually I got passed it, then I had to review my javascript to catch up with the future and promises (pun intended). The biggest challenge was that I started developing this specific powerup in a short amount of time.     Accomplishments that we're proud - we finished something (kind of)What we learned -programming is toughWhat's next for A Dance With DeathPart 2 coming soon!Built WithjavaTry it outgithub.com      Submitted to    HackRPI2017    Created by  I worked on one of the methods t25tesflc19villajkurian20bk30davi      The greatest challenge I faced was getting the device actuators to trigger based on the feedback from the cloud. With a lot of support from several websites, I was able to make it work.    We are not experienced in Unity that much so we have to deal with the Scripting API they provided    Figuring out a specific idea whose scope was realizable for the duration of the hackathon.Understanding how to use the API libraries for our project.Dealing with a lot of bugs/crashing in the mobile application    I'm a beginer on Meteor, I've only published one app with this framework. Also, I'm not a web programmer in my full time job, I work as a game programmer on PC and console games. This lead me to time loss such as  re-reading some basic meteor stuff such as the rounting part using iron:router. Simply having a correct Html/Css layout took some precious time.Also since my artistic talent leaves much to be desired, I could not create my own textures. I had to find free textures online but ran into packages to help me such as bootstrap & FontAwesome. While designing the second page where we see graph, I lost time trying to figure out how to show the data and what I actually wanted as a final result.The usual issue, I wanted to do more than I could so I ended up with a partial application and no sleep in that 24 hour.At some point I had issue between the RPI communicating with the meteor server. I didn't want to spent time on that part in the hackathon but I had to in order to have working feeds.  Who cares about headers anyway?  CSS & positioning & floats & setting widths are horrible. My least favorite part of front-end projects. And, perhaps because it was 4am, I couldn't get the text to auto-truncate & apply ellipses with CSS. I feel like that's possible.  The main challenge on the coding front was setting up the sin wave, and making it adjustable; but more-so in a good bullet-proof system that decreased the chances of bugs and gave the user a beautiful experience. But, also I think what was harder was creating a perfect alignment system so that the spikes always aligned with the max amplitude of the wave regardless of the global step or the fps changes. But on the UI and UX front, though I got the initial idea and concept nailed out quickly actually making that happen proved quite the challenge: creating the icons, assets and most importantly colour schemes :D - it got quite tedious. The tutorial and ironing out the learning curve of the game to make it enjoyable was also very difficult. The tutorial had to be more "walk-in" and interactive to truly show the concept, but the passing of the game had to be right when they began. This however more challenging when people who playing the game for a while said the start became boring since it was too easy. The dilemma I still don't know if I have solved!    It was hard to find a good emotional dictionary and then a perfectly match between emotions and movies/songs. Users may also listen to weird songs or songs with no lyrics.                Product: Providing tangible value without reinventing the wheel. Design: Creating a minimal, user-friendly yet eye-catching design.Engineering: Making texts/calls through Twilio, by-passing the iOS lock screen to enable direct actions when device is knocked on, creating an API for our features.What we learned how to:Connect complex global issues with technology and design.Work in a team, split up tasks, and communicate with people we just met.Use existing iOS knowledge creatively to provide easy usage.What's next for ReaXnProvide more safety features, and more quick-access meansCreate an API for iOS and Android for any app to extend our quick-access features.Try it outgithub.com      Submitted to    CHIMEHACK 2Winner                Chegg: Young Innovator (University Students)                  Created by  I worked on UX/UI/Branding design for ReaXn app and deck design for our presentation.Luna Chen2015 KPCB Design FellowI worked on the iOS development, specifically focusing on the gesture detection and location tracking.aspinI worked on the iOS app, especially implementing the front-end and connecting with the Twilio API.Benjamin ReynoldsCreative coder and designer | Software, Game, and App Developer @ MITDhruv PurushottamSonny Li  ContextIO OAuth issues. I couldn't get 3-legged OAuth 2 to work securely, so pivoted to a "Post Secret" style omni-email address, where people forward emails to a public address that feeds into the ContextIO API.    The major technical challenge was to build WRCs and have an economic value to it on the platform. Having a floating point value of WRCs based on the supply and demand. With a minimum floor price.  We ran into some complexity when dealing with object detection, Initially we wanted to make use of the ResNet-50 model for PyTorch and planned use that for inference. We found we had issues getting a model up and running and exposed over an API due to time constraints. This meant that we opted too instead make use of the Google Cloud Vision AI API.Messenger Bot is providing an image_url of the photo attachment. In order to send this attachment to Google Cloud Vision, we first tried to send the url to Google Cloud Vision.It couldn't extract the labels because Facebook is using Image CDN. URL of images are constantly changing.A solution is to send the image data as base64.  CORS blocking between the client and the server, extracting useful data from APIs, successful integration of lifecycle methods, passing data from one page to another using the router component.  Setting up the arduino was a challenge, because we did not have a WiFi module. Without the WiFi module, we can't get live data from the arduino into database.   OCR recognition of documents - needs backend ocr to optimise image and translate full pages accurately. Currently only works by highlighting text    I had some issues in finding suitable SSL certificates that I could use for the demo site (www.onegraph.xyz) without spending too much, but was able to use Let's Encrypt and an open sourced Azure WebJobs extension to provide certificates for OneGraph.While building the app, I additionally ran in to some OData errors with a request I was trying to make with the Graph client, and ultimately created an alternative approach for the same functionality. What I learnedAs a student, I had never made an enterprise-facing application before, and really enjoyed learning about Azure Active Directory and the Microsoft Graph to make OneGraph as an application for people in organizations to use for responding to security threats.  In addition, through building numerous view models, controller actions, and views with Razor syntax, this project was my deepest dive in to ASP.NET development thus far! What's next for OneGraphI would love to extend the current OneGraph application to enable users to add machine learning automation for processing and responding to alerts from the Security API.Built Withasp.netazurec#csshtmljavascriptmicrosoft-graphTry it outwww.onegraph.xyzgithub.com      Submitted to    Microsoft Graph Security HackathonWinner                Popular Choice                  Created by  Abhishek JoshiStudent at the University of Washington - Seattle.  We went many many failed designs, failed features, and broken discord API libraries.  On the second hackathon day we discovered that there are no sound drivers yet implemented in the SensorTag. Consequently, MoodAmigo is temporarily relying on a smartphone application for voice input.  Building our own ML model was challenging because we had to collect our own images and invest time training the model. Also, the obscurity of publicly available research papers increased our time spent research significantly.    Time, money, talent and vision are constant challenges. We actually gave up on trying to apply for this hackathon since we started building the prototype only 2 weeks ago. The stars aligned and the deadline was extended. The world needs a proper end-of-life custodial product as soon as possible!!!  The plugin part of Space Engineers isn't very well documented so I had to do a lot of research on how to work with it.As building an extension was completely new to me I needed to do a lot of research into it.  Not enough data! Our similarity clustering algorithm as well as many failed experiments would have been more successful had we had a year of data.Too much data! The demands on the browser are almost higher than we can manage without some serious work on our visualization.  The large aluminum can apparatus we created acted like a large antenna that was very sensitive to surrounding signals. We were able to solve this issue by decreasing the impedance of the circuit, thus reducing the sensitivity.  IRC is really awkward about... Pretty much everything. Also sending SMS is expensive, and 5 free credit barely cuts it!!    Overall, the entire project was challenging as it was the first time for many of us working with Node, Express, and Arbor, as well as the first time we've ever deployed a web app to a server. Some specific difficulties we ran into: collaboration: the real-time inputs of ideas and updating the interface for all users was challenging to execute, but we learned how to use Socket.io to help make this easier.new languages: JS was fairly new for all of us programming and using Express caused us to have to duplicate and shift around many files, breaking some of our pre-existing links.servers: we tried deploying the app to several cloud systems, like Heroku and Meteor but they had dependencies that we didn't build into our code. In the end, Jeff at Linode was very helpful at 5AM and we were able to set up a Linux server.    Getting the Muse headband to work was the biggest challenge we ran into. The LibMuse SDK had no support for Swift, so we had to create bridging headers to integrate our Objective-C Muse code with the rest of the app, written in Swift, which ended up being far more difficult than we initially thought. Moreover, pairing the Muse headband to our laptops through Bluetooth was extremely challenging, which also negatively affected our ability to obtain the datasets for the Azure machine learning model.            We have created 2 different web designs tackling the user experience of our website. We had a bit hard time finding the data about all the routes and stops. We manually searched and collected the location information about each and every stop.    Since it is on PC/tables, it needs to support both touch and mouse well, I have to do a lot of low level interaction detection.   The majority of our challenges centered around the Amazon Alexa Skills Kit.  Their documentation is lacking to say the least.  Getting the device to recognize certain speech patterns and implement a specific type of implicit key authentication was a fun challenge to tackle.We also had our first digital ocean droplet hacked :/   Poor lightning led to noisy data from the camera sensor.Also, chess is hard.    -3D printed joints fell apart somewhat frequently.  Super glue and two-part epoxy was used to reinforce the parts.-The palm of the hand was designed for a left hand, while every other part was designed for a right hand.  The plastic part had to be carved and modified extensively in order to be compatible.  Various hardware also had to be secured to the hand with duct tape.-8g micro servo motors were barely powerful enough to move the joints.-The DVD drive stepper motor was not able to handle the weight of the hand reliably.-9V batteries were used to power the Arduino, but when all five servos were powered at once, the Arduino would restart.  A separate USB power supply was used to fix this issue.-The acrylic plate was originally designed to accommodate four servo motors with the intention of linking two fingers to one servo motor.  However, it was soon realized that micro servo motors are not powerful enough to handle more than one finger each.  There was little room for a fifth servo motor, so it was mounted hanging off the edge of the plate.-One servo motor's gear stripped and the motor stopped functioning.  Luckily another broken servo motor was available.  The two broken motors were disassembled and reassembled into a functioning motor.  There were challenges throughout the project, but learning (and sometimes biting off more than you can chew) is all part of the hackathon experience. On the backend, one of the biggest challenges was handling so many asynchronous requests from multiple sources in our API. The front end team also experienced a few hiccups, mainly managing state with React-Redux. And of course time constraints were also a big challenge (big thanks to the organizers for the caffeine supply).  Nothing too technically challenging, thinking about the process was the fun part.      We spent that majority of this hackathon attempting to learn how to use PhoneGap.  PhoneGap allows developers to write apps in HTML and JavaScript, then it compiles into Android, iOS, and other native applications.  We thought that PhoneGap would be the perfect solution for creating an app that used HTTP APIs like a web app, while at the same time tracking user location in the background like a native app.  Unfortunately, while PhoneGap is a web app internally, it is not actually designed for use with HTTP APIs like Venmo.  We found several hacky implementations of HTTP APIs online, but none of them would work on our machines.  Therefore after approximately 24 of the 36 hackathon hours we switched to making a Ruby on Rails app so the Venmo API could be used properly.What's next for Cash OnlyWe see Cash Only becoming a platform like Lyft or AirBNB, where individuals can disrupt corporate monopolies and everyone gains convenience.  Future versions of Cash Only will include a gamified badge and rating system so users are held accountable for making exchanges.Built Withparseruby-on-railstwiliovenmoTry it outgithub.com      Submitted to    Hack the Planet    Created by  Erin HoffmanBogdan PozdercaStudent at Michigan State UniversityLauren Bretz      Even if the internet is overcrowded at the time of an accident, I'd like to enable notice reservation automatically.  In order to get ready for the situation of hard phone connection, added voice mail function and then recorded voice mails are changed into texts to analyze their sentiment to help receipts judgement.In order to get ready for the situation of hard network connection, decided to store the information not only on the Bluemix but also on the FlashAir. Notice from iBeacon server can be sent without starting the application.            None of us have worked with Pebble before, and most of us have never worked with Android. We hit a couple walls along the way. Familiarizing ourselves with Pebble took awhile, and trying to get the two apps to communicate was a definite challenge.  Really should have learned Git beforehand...              New technologies, pressure.Changing from react to ionic to react, sticking with ionic in the end.  The hardware limit was the biggest challenge in the process of development of this app. We wanted to give the user an exact number for how far is the performance, but the bluetooth integration had a limitation of detecting the distance.   One of the first problems we ran into was how to incentivize people to mine the blockchain data. While a common reward is paying cryptocurrency to the first one who successfully mines the block, we did not have a reliable, approved source to fund this operation (spending tax dollars would undoubtedly be a controversial debate). We decided on providing the opportunity to start their own polls and collect data they are interested in. A harder challenge was actually creating the decentralized blockchain network and the public ledger that would record the collected data. Accomplishments that were proud ofWere proud of the fact that we were able to create a blockchain-based distributed application in such a short time frame, considering none of us have had experience working with cryptocurrency or blockchain related applications, or peer to peer networks before for that matter. Were also immensely proud of our idea, and its potential to make a huge difference in the world through improving the democratic process and helping prevent fraud, corruption, and voter suppression, especially for minorities and oppressed groups.What we learnedIn adapting the blockchain model for cryptocurrency to work for voting, we had to really understand the advantages and drawbacks of online transactions and a decentralized network. While there are indeed risks of outside interference from malicious hackers and other technological threats, the blockchain mitigates the problems of voter suppression and voter fraud. In addition, we learned all the technical details of implementing blockchain with Python: public key cryptography, hashing functions and mining, proof of work, immutability of blocks by dependency of previous blocks hashes, etc. What's next for P2PollIn the future we hope to add more security measures to the blockchain and implement two factor authentication in an effort to live up to our mission and make voting even more secure. To make it viable in the real world, the incentive for mining must also be expanded upon, so only those who accumulate enough credits will be able to run custom polls on the blockchain. Finding a way to make a more robust registration system which verifies an individuals identity will also be a major leap in making P2Poll viable in important referendums and even elections in the future.Built WithbootstrapflaskjavascriptpythonTry it outwww.p2poll.comgithub.com      Submitted to    BostonHacks Fall 2018Winner                Best Overall Hack (1st place)              Winner                Best Domain Name from Domain.com                  Created by  Julia MaYirui RuanBartholomew C.Jeff HuiComputer engineering student with an interest in solving social problems with technologyWilliam Pine  Integrating API for OCR was challenging along with comparing two results from two OCRs to improve accuracy. and using stanford webprotege online service for concept mapping using ontology network.What's next for Eventual.lySeamless sharing with friendsEvent managers can monitor poster exposuresDevelopers can create powerful connections with users via Eventual.ly e.g. link to ticket vendors on Songkick via Eventual.lyBuilt Withdropboxidol-ondemandprotegetesseractTry it outwww.slideshare.net      Submitted to    Hack The VisualWinner                Runner Up Prize                  Created by  Christie LauMSc Human Computer Interaction with Ergonomics at UCL; freelance photographer.Gustavo Soto MinoSandeep Georgehacker, thinker, data visualiserMiguel Aulet  Team members have little to no experience with Javascript, so learning a new language was a challenge, but an enjoyable one.    The challenges we ran into were to plan the service's architecture to be resilient to errors, secure, easy to manage, and highly scalable.  Working with git proved troublesome at times, as we had to rebase multiple times throughout the project, and many commits were duplicated in some pull requests, and we had to modify the history of commits. Another challenge was understanding how Azure pipelines and coverage worked and how to implement it into the project.  One of the major challenges we ran into was dealing with images which had both known and unknown faces in the image. Treating such images required a more intricate method of image processing so as to process the current information and store the information for future use at the same time.  The most interesting part of this project was coming up with a way to define the product "nearest neighbors" in the parameter space of key attributes in such a way that a user could navigate through it. In the end, I generated tables with one row per product and columns for product ID's of products that there were slightly-smaller and slightly-bigger along each attribute, as calculated with a Python script.For the visual display to have the stop-motion film look I was going for, I needed images of products with white backgrounds and from identical angles. In the future, it would be ideal to use 3D models of products to render out silhouette shots from identical angles and distances.  There was no Emotion API existing in the marketplace. We used the Face API to extract Facial Attributes and later extracted Emotions using the already defined Facial Landmarks. We compared the intensity of each emotion to find the most prevalent one. This become our solution to the lack of the Emotion Identification API.It was the first time that either of us worked with Firebase. During the initial few hours, we were very confused about how to work with it. However, we learnt as we coded and successfully operated the back-end of the mobile application.  Creating the augemented reality application was quite challenging and inferring indoor information using the floor plan was even more difficult. Initially, I tried to use Wifi triangulation for indoor navigation, but it turned out to be quite difficult and hence had to go to with mobile device's motion and orientation information to help with the navigation.  We ran into many challenges creating the feature to allow users to create a queue of words to play in sequence while ensuring a proper amount of delay between words.  The greatest challenge for us was to make the navigation efficient and effective. Implementing speed limit over the user was also an interesting concept that took some time to get around.  Unicode characters caused a variety of issues, especially working between operating systems and programming languages. We are happy that we were able to resolve most of these issues.  Creating one central app for different type of commuters requires different customization and UI.  We ran into a lot of technological problems related to authentication and want into a few walls trying to render properly the front end using bootstrap.  We first planned to use MQTT to communicate with the cloud, but certain limitations in certificate handling forced us to revert to WebSockets over TLS.Accomplishments that we are proud ofMicrosoft's HoloLens and AR are an important step into the future of computing and we are excited to be part of this development.What we learnedIt's possible to create great inter-connected applications by combining various different platforms and programming languages, from desktop add-ins to, server components and 3D AR apps.What's next for HoloMeeting-ClientThe initial HoloMeeting-Client currently provides a shared virtual presentation screen with basic previous/next slide controls. It would be useful to have annotation options, shared audio (currently remote cilents have to join by phone for audio) and a more fine grained permission model.Built Withc#dockermicrosoft-hololensnode.jspowerpointunityTry it outwebgate-holomeeting.s3-website.eu-central-1.amazonaws.com      Submitted to    Hack Productivity 3    Created by  I worked on the Powerpoint integration and streaming server backend.Christian GuedemannI worked on the HoloLens Unity3D App and the node.js based meeting server, while my colleague christian was in charge for the PowerPoint Add-On to upload presentations to the meeting server.Richie Schmid    One of the key challenges building Easy Agile Roadmaps was ensuring that the Epics appeared at the correct place on the roadmap. Each Epic is allocated a start and end timestamp which you can change with drag and drop. These timestamps had to be converted into pixel dimensions to ensure they not only appeared in the correct spot, but also didn't ever overlap. With the assurance of a good-many tests, we were able to achieve this goal with time to spare.  Since this was the first game I made, I was new to a lot of things and I had to learn a lot in order to make this game.  We weren't used to working on the same set of code with other people in real time. There was definitely an adjustment period for GIT during which merges were a pain.    The main challenge we ran into was capturing different sentence structures.What's next for Alexa keepMake the app more user interactive and improve the user experienceBuilt Withamazon-alexaamazon-dynamodbechojavascriptnode.jsTry it outgithub.com      Submitted to    Amazon and Connected Lab Present: the Amazon Alexa and Echo Hackathon    Created by  I worked on the back end using node.js and DynamoDB  and helped customize the search queries using NLP.Milan DimicAlexa Skills kit backend w/ DynamoDB, Product design.Bill ChenI worked on back end using AWS and DynamoDB.Ronak PatelMaker | Developer | InnovatorMike Sililiane  Although Ambient Noise is a relatively simple Skill, I ran into a particular challenge of slot-only utterances not always being recognized properly. Sometimes I would say a sound but Alexa would act like she didn't hear anything, and other times the result was simply inaccurate. But hey, let's be honest: NLP (Natural Language Processing) is hard. While I'm sure the smart folks at Amazon are working every day to improve Alexa's interpretation of human speech, I needed an immediate solution. To resolve the issue, I created another intent (appropriately called "soundOnlyIntent") which had only one sample utterance attached to it: {Sound}. That's right - by isolating the utterance to its own intent Alexa was able to build a better language model. Ta-da! Problem solved!  We became very close with IBMs team at this hackathon. We had a hard time initially integrating Watsons Speech-to-Text service, but with their help, we were able to it working.Accomplishments that we are proud ofWe are proud of our voicemail keyword algorithm used to create Google calendar events with a title, description, location, dates, and smart reminders. The smart reminders sets calendar notifications based on your schedule.What we learnedThis was our first time working with IBM Watson and the Facebook Graph API. We had a fun time learning these developer tools. What's next for WoxiWoxi is currently tackling a small part, voicemail optimization, of a potentially huge cognitive smartphone assistant app used to automate tasks. In the short term, we want to expand our keyword algorithm to incorporate business scenarios. Woxi as an artificially intelligent secretary. In the future, we plan to utilize Watson's Retrieve and Rank service to improve our keyword algorithm and smart reminder feature using machine learning. Woxi will be able to learn from each voicemail you get and make more detailed and elegant calendar events.Built Withandroidbluemixchatbotfacebook-graphgoogle-calendarherokuibm-watsonjavanode.jsTry it outgithub.comwoxi.tech      Submitted to    HSHacks III    Created by  I worked on our keyword algorithm that characterizes each voicemail to get the important data for the smart calendar event. I also worked on the algorithm that finds the best time to notify our users using their Google Calendar Data. I worked mostly on the backend of the app. Pranav MadanahalliI worked on the Android application and working with various APIs to get the overall app to work cohesively. I also helped with the speech to text conversion and with the design of the application.Anirudh BalasubramaniamAspiring developer who likes hackathons and just wants to build cool things.I worked on the keyword algorithm to help the app detect any dates and events mentioned in the message. I also worked on the messenger bot to sync with the app and post notifications, and the overall app design.Rohit TavareKesav Viswanadha  Making AJAX requests from the HipChat client to any given Bamboo Server instance, thanks to CORS.      For some of us it was a new thing to use the new Symfony 3 framework. We definitely got our heads around it.     There were some difficulties during our work:How to write code for Pepper properly in order to make his movements more human-like, not to lock unnecessary resources and make it rather optimized?How to organize proper client-server interaction using only built-in libraries and functions?How to collect and prepare dataset for neural network that will teach it in most proper way?How to plan and built such neural network that it'll be capable of classifying human movements and not-so-much-slow-while-training in the same time?  Lack of experience with Unity and game development   We got a lot of help from the Alexa community during the work, however, this skill was not an straight-forward skill to develop with Alexa. We encountered many unusual problems which was not easy to resolve. Note that there have been many updates to the Alexa during the development time that helped us to resolve our problems.  For using daily works, we try to use Office 365.To get attendee candidate automatically, we use /people. The api will return well relation people.To fetch free time for organizer and attendee, we use /findMeetingTimesFor sending schedule request using /eventsBecause we need timezone for using /findMeetingTimes and /events, we get users timezone with /mailboxSettingsAnd we want to make great bot, we try to use Microsoft Bot Framework. For making good communication, we try to use Diaglos, State service and Channels.  The pull request page of Bitbucket cloud is not extensible like it is in Stash and Bitbucket server, so we were not able to show the code issues directly in the pull request diff. So we had to build an external integration by using the SonarQube plug-in framework and by creating pull request comments for the found code issues with Bitbucket's REST API. Also, there is no "technical user" concept in Bitbucket, so we had to create pull request comments as the repository user or with the Bitbucket team account (this is configurable in the SonarQube plug-in). Furthermore, because SonarQube does not provide OAuth to access its REST API, we decided to only support public SonarQube instances (no authentication) as we did not want to store user credentials in the cloud.  Not so many actually as Microsoft Graph API is well-documented and provides most of the functions I really needed.      Streaming video, integrating and normalizing multiple third-party Speech-to-Text APIs, the accurate stitching with limited metadata about the stream etc.,  Creating online capacity calculations based on virtual teams definition, right from the backlog screen, keeping Jira UI standards  It is challenging to break the currently established silos that govern most of Hajj data between transport, health, and hajj among over 100+ organizations.Immediate access to actionable meaningful data.How to deliver a POC in a short timeLack of team experience with UI/UXFear of smartphone availability for all Hajj, at least we start with group leads was the solution.Two original team members got logistics issues, One not receiving his flight reservation, other got sick and admitted to hospital days before the event      We had never worked with Android Studio or Java. In addition we found a bug when cloud sync is on the phone the smartwatch cannot connect using our Presto app. It was also tricky to parse the location coordinates into the messaging function.   Lack of time. I didn't want to miss the conference so, I just had a couple of hours to work on it.    Enhancing the abilities of the skill.What's next for Cooey Health MonitoringSimplifying routine health monitoring tasks with simple voice commands.Giving recommendations according to health conditions.Flashing alerts when health is critical using deep learning.Built WithjavamongodbrestspringbootTry it outwww.amazon.com      Submitted to    Amazon Alexa Skills Challenge    Created by  Ronak Mutha    None of us had ever used twilio before, so that was a learning process. It was also challenging designing the way that it asks patients for information, because the way twilio is set up is less of a chat bot and more of a message sending service. So, any information the user typed in previously has to be stored and managed ourselves.  It was rather hard to track Smart Contracts events    The Standard UIPath mail activities use the mailmessage object. This was however not suitable for the solution, because for example there is no standard shared mailbox property. Outlook have its own more elaborated object mailItem, which a lot of extra properties. Running through the outlook mail folders was somewhat challenging, some recursive approach was necessary./li>The most difficult thing in the second activity was figuring out how to link to messages as reply on each other. I found out that Microsoft has introduced a conversation feature for this.        Beta API issues with OneNote, OneDrive API does not provide definitive filetype identification. Built Withandroidareality3diosmicrosoft-graphofficeoffice-365onedriverealityscriptunityTry it outbetas.to      Submitted to    Hack ProductivityWinner                Best Student Application Award                  Created by  Yosun Changaugmented reality hacker-entrepreneur     OpenCV was a pain to get to work.  Integrating so many moving pieces together :slightly_smiling_face: ! The front-end, back-end, and machine learning tech were developed in different libraries, languages and by different people.      We had to record some lines multiple times to get the right effect, some of our voice talents were really tired after whole day in the studio.Learning APL was relatively easy but making all elements fitting together was a challenge - often the view in the simulator was different to real devices.      Making the Blockly library work with Amazon Sumerian was a bit of a challenge.  Debugging transaction in main net with parity traces.Get it running with Angular, PrimeNG and Web3.     We met a lot of problems during our integration process. None of our teammates have extensive experience working with React Native. Thus, connecting frontend and backend through simulators gives us various road blockers. One specific interesting problem that we ran into is when the front end tried to send the picture which the user took to the back end, the picture is in a  base 64 format. The format is a long string and the backend will not accept it as a POST request and gives us back a 413 error. We eventually found out that it was a default configuration that we need to enlarge to allow the server to accept larger request bandwidth and solved the problem. (and as an addition we are planning on reporting this issue to be added to documentation)  To really understand the questions and challenges refugees are dealing with when they arrive and travel through Europe.To get Watson API working the way we needed it to workThe ESRI API turned out more difficult that we hoped for, but with the help of one of ESRI's team members, we got it up and running.  Buttons.Also, half the Dragonboards we used ran Android instead of Debian. Then, every 96boards shield we borrowed had broken bootloaders. We decided to just plug in an Arduino Uno to use Arduino rather than using the built-in interface on the Dragonboard.  Iterate through many ideas and narrowing down to oneHow to build the front-endHow to find the teaching frameworkHow to customize front-end componentTo develop teaching materialsTranslating Blockly API to PyTorch  Initially, we did face challenges with respect to the usage of various Amazon Web Services. But the documentation helped us understand them better and utilize the services.               Tying PostgreSQL database into easy-to-code frontend website        Thanks to Knurlds easy to use documentation, we didnt run into any problems regarding the voice authentication part. We did,however, spend quite some time on the chrome extension (making sure that passwords are filled in not just any form, managing sessions efficiently and other minor details).  Possibly the largest challenge we ran into was optimizing the code. We had a very complex stack with more than five different languages and we wanted the user to be able to control a robot responsively through a variety of inputs, including gesture, gyroscopic control, and speech. In the end, however, by using lightweight libraries, getting rid of as much networking as possible, and writing clean code in machine-close languages, we were able to make a responsive robot with a large tech stack.Originally, our team was split in two. One group focused on the visual recognition capabilities of the robot, while another focused on using the Samsung Gear S2's gyroscopic capabilities to create a smooth control system for the robot which could be used to direct the robot in the direction of the user's walking path. The Gear S2 however, does not use Android Studio or Java to create apps, but rather an industrial grade IDE called Tizen which uses C. I had never worked with C before, and I had to pore over the API's to understand how to structure my code to access the gyroscope values. We also had to figure out how to get information from the robot's camera, to Firebase, and finally to the smartwatch which would read out the information via audio. While we were able to link information from the camera to Firebase, the development environment of the smartwatch proved to be too difficult to work with, and unfortunately we had to look to other options. Another issue we ran into was controlling the NXT through Python. The NXT robot was originally used just for concept design and prototyping, because we had originally hoped to use a drone to complete our hack. When that was not an option available to us anymore, we decided to just go ahead with the NXT. Since we were not using a normal controlller, and since we wanted to transer NXT values to Firebase, we had to create our own control system over bluetooth from the ground up using Python. We also had to custom create a control system that allowed auditory controls in which the user could give auditory commands to control the direction of the robot, which also allowed us an alternate solution to robot control once we realized the smartwatch control system may not be viable.          Synchronizing data between AngularJS and Firebase was not always straightforward and well documented for special cases. Accomplishments that we are proud ofPublished an Open-Source library for the use of other Android apps in real-time.What I learnedMaking Android library, AngularJS and FirebaseWhat's next for DynamoUIImplement A/B testing so marketers can determine which versions perform better in real time. Built Withandroid-studioangular.jsbootstrapTry it outgithub.comgithub.com      Submitted to    Hack Western 2Winner                Best Use of Open Source Technology                  Created by  I created the Android library that interacts to provide interfaces to dynamic, real-time UI components.I worked with the Angular application and helped to implement the three-way binding with firebase, the models, and the UI.Peter ChuI worked on AngularJS, firebase and UI design.Haiyu ZhenI worked on the Angular web application along with the UI and UX.Parsa AlipourThomas Lui    The main challenge we faced was how to manipulate the WiFi hardware on the device. Android is especially troublesome because different hardware manufacturers support different levels of access - the platform itself is also fragmented, which made it very difficult to initially test functionality.As we mentioned above, no public APIs exist that let us set up hotspots or easily monitor network traffic. That doesn't mean the code is totally hidden, though! We used Reflection to access private API methods, which we then leveraged to automatically turn on hotspots with pre-configured SSIDs. To get around the traffic monitoring issue, we eventually discovered a way to grab the total amount of data sent by a device since it was last booted. Via some more clever hacks, we were able to extrapolate the amount of network traffic passed through a particular device in a given time interval.Once we got past this, we had to figure out a good way to identify these SSIDs. We experimented with GPS-based identification, but eventually settled on a more simple approach wherein we prefixed SSIDs with a unique identifier.Finally, we had to come up with a performant, battery efficient mechanism for logging all of this data and choosing between potentially numerous hotspots.   TODO    Getting the data source was the big challenge and segregating the data based on locations was the difficult part. Then sentimental analysis of the live data was very critical. Data analysis and visualization of the news on the maps were some other challenges faced by us.  The Web Audio API is still under active development (read: everybody is still arguing about how it should work), and so is WebRTC. Having previously used Socket.io many times, I thought it would be a good idea to use their p2p module to stream the audio. In the end it worked out okay, but the documentation was a bit rubbish.Another major issue is the latency. When running the 'play' and 'listen' clients on the same machine, the latency is manageable, but over the network it's too much to be able to play guitar. This does somewhat invalidate this entire hack, but please ignore that fact.  Since we dont live in Sao Paulo, everything we understand about the traffic situation comes from online research and talking to our friends. We know that its hard to capture the needs of our users without experiencing the traffic situation first hand, however we believe our system addresses many of the very real pain points experienced not only by Sao Paulo, but every crowded metropolitan area.Another challenge we ran into is that our platform was designed to automatically capture GPS trip data from OpenXC-equipped Ford vehicles, however neither of us own one so we couldnt do any end-to-end integration testing. Instead we relied on reading online documentation and using simulated data to build out our application.Accomplishments that were proud ofAs a small team of just two individual developers, we were very proud of our ability to create a fully featured app from scratch that addresses the short-term and long-term needs of Sao Paulos commuters. Not only did we give users a way to support the Metro lines they care the most about but we helped them identify which routes those were through analyzing their current driving behavior. We also created an easy way for city and transit officials to gather information from and engage with their citizens through our administrative dashboard.What we learnedIn brainstorming ideas for this challenge, we came to appreciate just how difficult a problem traffic congestion actually is. Any solution must balance the needs of all stakeholders, including commuters, auto manufacturers, public transit officials, and policy makers.We challenged ourselves to think outside of the box and build something that could have a long-term impact on how ordinary citizens engage with their community to make it better. While researching existing platforms such as Kickstarter and Indiegogo, weve learned about just how powerful the crowdfunding concept is and how much of an impact it can have.We also learned that building a prototype and demo is just the first step. For each feature we implemented, we came up with another dozen that we wished we could also add. For each minute we talked with others about this idea, we wished we could spend an hour more. And for each user we had test our app, we wished we could launch with a hundred more.What's next for Roll TogetherWere excited participate in this challenge and were very happy with everything we were able to build out for Roll Together from scratch in such a short amount of time. The initial reaction weve seen for Roll Together has been tremendously supportive - many of our friends who have tried the app wish they could start tracking their own trips and pledging to new transit routes in our home city of San Francisco. There is still much to do before Roll Together can be launched in a new city. Based on the feedback we receive from this challenge, we will make some changes to our platform, try to raise funding for Roll Together, and choose a city where we can launch our first public beta. Who knows - perhaps we might find ourselves in Sao Paulo before too long!Built Withangular.jsapacheflaskionicmongodbnode.jspythonTry it outgithub.comrolltogether-app.herokuapp.comrolltogether-app.herokuapp.com      Submitted to    So Paulo Mobility Global ChallengeWinner                Grand Prize                  Created by  Kara YuVimal Bhalodia      Some Bugs are yet to be resolved but the software is at a Proof of Concept stage. Since IBM Bluemix is in beta very little of the Blockchain of has been implemented. Right now it just acts a ledger that dutyfully notes Egalitarian Routing. But the idea of doing Risk Management on the Blockchain is sound.  In future Ford can offload some of the risk using P2P finance on Blockchain.Also I used the free version of Google Map Apis which has a limit on the number of waypoints (Stops and Pickups). Thus if you look closely you will see that the last person is not dropped at his destination. By using the paid Google Premium Apis this bug can be solved.      Backend: Modifying the hyperparameters to properly learn the style features and content features to produce a believable output. We had to revert to using pre-trained models since time didn't permit much experimentation. (github)Frontend: Being able to interface with the backend and testing several frameworks proved tricky to find exactly what we need and how to implement features.    It was my first app using ES6 but a good learning experience.    No one had ever done anything like this and no one knew specifically how to get the data out of the voice gateways.  Cisco produces multiple event records per channel connection so we have to develop a primary key which allowed us to count distinct channels in use.    There is poor documentation re: Python and the Cisco AXL API which caused development delays.   We had only been a splunk customer for approximately 3 months before I started this project so there was a lot to learn.        Finding a proper way to handle matrix operations, due to the only-square and 4x4 limit of GLSL, that works for any desired number of dimensions.Finding a way to work out the faces of hypercubes (which we didn't entirely work out).Finding a way to project the hypercubes that looks nice and interesting  Detecting the handshake gesture, because everybody performs the gesture in a different way.  Getting used to Handwritten recognition was pain and hence it was tough to start with and hence learnt it as I progressed        Ensuring the focus was narrow, and considering limitations that users might experience such as limited access to internet or up to date phone technology.   Reliable tracking under different light conditions.Feedback effects from projected light.Tracking the keyboard reliably.Hooking into macOS to control window focus      We have found difficulty moving the text from the local machine logging the information involved in the RFID interaction to the virtual machine on Amazon Web Services.    As it was our team's first time creating an android app, we went through much trial and error. Learning Google Firebase and the NativeScript framework was a welcomed challenge. We also faced some technical limitations two of our 3 computers were unable to emulate the app. Thus our ability to do testing was limited and as a result slowed our overall progress.  Piping all the APIs together to create a seamless user experience. It took a long time to optimize the data (mpeg1) we were sending over the websocket to prevent lags and bugs. Running the Machine learning algorithm asynchronously on the GPU using CUDA. Generating a high-quality image of the synthesized design.Customizing Fabric.js to send data appropriately formatted to be processed by the machine learning algorithm.  My motivation was to create an experience where Alexa is a social facilitator for a group of 2-4 participants. The skill has to let them talk freely and make them feel like Lexy is listening and responding to everyone. As every teacher/parent can probably attest, it was very challenging to create a natural yet structured conversation that keeps the kids in control, lets everyone feel heard, and teaches them something in the process.A major challenge was that Alexa doesn't pick up some of the things the kids say as either they talk too fast, get excited and step out of the flow, or can't help themselves and talk together. I tried to go around this by expanding the possible utterances for each state and creating default responses for almost everything they might say.    formatting scan alert description in Redmine  Outlook API was the hardest thing to decipher.  Otherwise, finding the right libraries for each task revolving around flicking, finding nearby friends, etc. were difficult as well.What's next for SplitAutomated image recognition, and finding the people near you without even telling the app through an API.Built Withandroidangular.jsatlantic.netcircleciesriioniciosjquerymastercardnexmonode.jstesseractvenmoTry it outgithub.com      Submitted to    TechCrunch Disrupt NYC 2015Winner                MasterCard              Winner                CircleCI              Winner                Disrupt Tickets                  Created by  Joseph SongTimotius SitorusJustin WooDemian BorbaCassidy WilliamsSoftware engineer & developer evangelist at Clarifai  During the time of the Hackathon, we faced terrible issues with the internet which really affected the pace of work.We also encountered an issue on GitHub, our commits were not taking effect, PRs were not working and our GitHub Page was not propagating.    The main challenges were handling sessions and adding the report generation part along with using a flag for keeping a track of whether the medicine was taken or not. Also laws governing publication of such skills.  Getting Magenta to generate music that sounds decently nice required quite a lot of experimentation.  Finding a way to take morse code input and show the live changes in the input field.Recognising new faces in different lighting conditionsSetting sensitivity thresholds for different facial gesturesMerging the web automation and facial recognition software into a logical program flow    The UI is something we are constantly looking to get feedback and fix. We need to have streamers and viewers to start using to get feedback, for now we couldn't really test other than internally.  Deployment and persisting state in backend servers.    One of the main problems was to make our mobile application user-friendly. We are using Google Maps API with some pre-made indoor maps, but despite 4 years of development, indoor maps still work untstably on mobile platforms.Couple of tries of attaching EMBERS API were made, but we were unable to do it and custom route finding algorithm was developed.Our decision to create visualisation using JavaFX was last-moment, and it took a plenty of time to deal with relatively new technology for us.  API's restrictions (Pending approval of instagram to give public access), how to determine the music attributes from the parameters obtained when running cognitive analysis over the images, deploying to the server    The Office Add-in story is continually evolving and sometimes we have to deal with API changes and mandatory updates.  Since ClinicalTrials.gov does not have an application programming interface (API), our import service has to download a .zip folder of xml files, and parse their contents. Normalizing this data and structuring it so that it is patient friendly involved substantial time and thought.  All team members knew different programming languages, it was difficult to assign jobs evenly.    LeARn was very difficult to build for a variety of reasons. Nobody on our team had worked in Unity before or even knew C#, so to call it a learning experience is an understatement. We used complex and highly detailed models, and optimising these such that they would render well on mobile presented a significant challenge in and of itself. We are truly pushing the boundaries of Google Cardboard technology, having to cut apart our Cardboards just to allow the app to function.  It was a challenge to simplify the user interface, make the swiping interface, and implement the Google vision API.   ORB was not my first app, but it was my first game and the first experience that I have really had using the Unity3D game engine. Nevertheless, I had some challenges in creating the algorithm and figuring out specific gameplay elements. In terms of gameplay, I had designed the game with Darts in mind. This game, which you have probably heard of, involves the player throwing a set of arrows, called 'darts,' onto a board. Certain areas are larger, and others are smaller. I decided to take inspiration from this game and create the numbered walls of different sizes so, for example, the wall which increases your score to 5, will be smaller, and thus harder to get to. Algorithm-wise, this was hard for me to create. The levels are all 'procedurally,' generated which calculates the difficulty that a level should be (by the score but the hidden time it takes to complete past levels); it's almost like a piecewise function which operates as either an exponential, logarithmic, or sigmoidal function. However, with the difficulty being calculated and with different walls put together, they wouldn't fit together like bricks.  Keep Frame rate over 60 Reduce apk file size for submission in SideloadVR  We needed to have instantaneous synchronization of information between multiple team members in the Operating Room with separate browser sessions. We solved it with the use of WebSockets.    At first when we started the Pinage development process, our main focus was to design the simplest UX. The UX design takes a lot of time to start our app development. Finally after a couple of mockups, we design the UX of Pinage application in such a way that user can easily put their message using just single tap. Now user can just type theirs message, select contact and a single tap will tagged in the user current location for their desired one.After that we faced a lot of problem in detection of user arrival or departure event with minimal usage of device battery. At first we did not read the ALOHAR documentation properly. But when carefully and thoroughly read and inspect the ALOHAR developer document, we saw that how easily we can detect any Geofencing event. After developing the Pinage the second most hurdle we faced during the app testing time, as we have to tag a message in a location and we have to wait until the ALHOR fires either arrival or departure event.Accomplishments that we are proud ofWe are tried how simply user can leave a message in a particular location and receiver can get the message as quickly as possible when the receiver enter a particular area. The main hardle was how to detect a Geofencing event, i.e. how we can determine when a user enters a place/area or leave that place. We successfully implement that feature with the help of ALOHAR location API. But the integration was not so smooth. We have to test that location based message popping feature several times to finalize that feature. But finally we successfully integrate this feature with the Pinage.We also design the UX of Pinage in such a way that user can simply pin a message without performing many unnecessary steps. In Pinage user can simply pin a message. Also receiver does not need to open the app to check whether there is any message; Pinage will automatically notify an user if there is a message for him/her.What we learnedOur team developed many locations aware application in which we try to provide some information when an user enter an area. But the main problem in those apps was, we have to periodically check whether user is in proximity of a particular location. This process drains the device battery a lot. In Pinage we used the ALOHAR with our wrapper library to reduce the battery drain in determination of any Geofencing event.What's next for PinageDuring this limited time we mainly implement the necessary features. But we have planned to add new features to smooth the user experience of Pinage. For example,  till now the message lifetime of Pinage in server is one day, but we have a plan to extend this so that user can have multiple option for the message lifetime such as one hour[for short message], one week[for long lasting message] or message with lifetime validity. We are also thinking to add message conversation for a particular topic in private or public mode.  We will also add the social network like facebook, google+ for signin or to allow user to  import their contact or share their public message.Built Withalohar-mobileandroidandroid-studiomysqlphp5Try it outplay.google.com      Submitted to    Create Context-Aware Applications with Alohar Mobile's SDK    Created by  I am the team leader of this project. I also developed the full backend and the full mobile application.MD. MOFIJUL(Akash) ISLAMLecturer, Dept of CSE, University of DhakaTohidul IslamSwapnil SahaTamanna Islam Juthi      Time! We had problems with compiling existing code. The backend existed we were unable to connect to it.  Actually none.   The challenge was helping developers to build Vr websites in the simplest possible way without using a single code line.    My lack of experience and need to google simple things like how do you add a photo ... then how do you resize that photo! I also struggled to get my code onto a test site. I'm still struggling to get my stuff onto github. that is why my submission is on a crazy URL. If I can figure it out before 12 then maybe I can put it on github.  Our product is centered around historic data related to organ transplants, and the biggest challenge that we faced was gaining access to this information. Since this is a highly regulated field, the government and organizations involved in organ transplants tend to keep this information highly confidential. However, in order to replicate original trends, we created synthetic data based on factual information, and used these probabilities to populate our data set.   One of our biggest challenges was solving the problem of sharing MetroCards over the phone. If it was just one single code, then people could easily send their weekly and monthly MetroCards to their friends. We brainstormed and ultimately came to solution which was to create a QR code whos code would be based on the users account number and the current time. It would generate a new one every 10 seconds, therefore not giving enough time for it to be sent to others.What we learnedWe improved our React Native skills and used a new API we had never used before.Whats next for MTA BlinkWe hope that the MTA would adopt an app like ours, wed love to see a more efficient MTA!Built Withexpo.iofirebasegoogle-cloudnode.jsreact-nativestripeTry it outgithub.com      Submitted to    CUNY HackathonWinner                First Place                  Created by  I worked on developing the UX and UI using React Native, as well as integrating Firebase to apply payments and display information in the database. Sang ParkXin YuanDavid MargolinJohn Evangelista    Getting React to render when we want it to render (getting React to react. Haha! Joke!) Being sickManaging a live database, and fragments of text from the editor. Understanding concepts in ReactPlaying back sections of recordingsnot accidentally ddosing ourselves What we learnedMuch of the team worked with React for the first time, and also had to learn how to deal with sending Wolfram API calls from the client side and having it live update. What's next for RecapRecap will soon feature more classroom management, to make it easier for peers to connect their notes and for professors to track their student's progress. Additionally, it can support transcriptions, and then let students know which part of the lecture they might have accidentally slept through. Built Withpythonreactwolfram-technologiesTry it outgithub.com      Submitted to    MHacks 9Winner                MHacks 9: Second Place              Winner                Best use of Wolfam Tech                  Created by  Carol Chen<3 Marcel O'NeilHigh School HackerCurtis ChongLike all detectives, I follow the data and piece together a story to tell others. I clue-hunt at hackathons and Civic Tech TO with AWS.Kevin PeiSkiis like a madman, git commits with rage, and codes for the betterment of society. Wolves are cute.  The first challenge we ran into was being able to complete the capture, analysis, and blocking in real-time with zero lag.  We altered our algorithm and refactored most of our existing python code, ultimately accomplishing precise analysis with minimum resource consumption.The biggest challenge we ran into was being able to accurately detect when an image was flashing repeatedly, rather than an object moving or a scene changing. A lot of things seem like they could be flashes at first, but are clearly not when watched by a human. By comparing multiple frames one after another we were able to better analyze what is happening on the display.Accomplishments that we are proud ofOur proudest accomplishment is learning about epilepsy.  In order to understand the requirements of our project we had to learn a few things about epilepsy including triggers, effects, and number of people affected. However, we wanted to go further. Since one of our team members has had family experience with epilepsy we decided to learn more. We learned about multiple different kinds of epilepsy, triggers for different kinds, statistics, long-term effects, symptoms, and the demographic of affected persons. In our opinion, the coding may have taught us new ways to accomplish our tasks, but we think the knowledge we know have will have a greater impact on our lives. Epilepsy is something we have have to deal with first hand in our life be it friends, family, or anyone in public.What we learnedDuring the creation of Project Julius we learned a lot about Epilepsy and its causes. Using online resources we discovered how many people it affects, common triggers, long-term effects, and large incidents in public.We also learned a lot about how to process large images (upwards of 1080p) quickly and effectively using histograms of regions on the screen. The changes in the histogram values indicate a change in content, a large difference indicates a flash or scene change. We then had to further our analysis to compare multiple changes to detect the rate at which the image changes, and thus determine if we should block it.Our initial approach to capturing the screen was to use the low-level frame buffer built into Linux. We spent a few hours learning how to access and process the frame buffer but ended up deciding not to go with it. We felt that using something built into Linux would greatly decrease our potential reach so we changed our screen capturing method to be more portable and cross-platform by using OpenCV and native desktop capturing applications that can pipe into Python. We found multiple programs for the different operating systems such as CamTwist and ManyCam.What's next for Project JuliusProject Julius is in a state such that it can be applied in many situations, however it isn't ready to be released fully. We hope to continue development and create a fully refined and deployable application. We would like to contact Epilepsy Action in order to verify that our program would be effective and available for as many people as possible.LinksWebsite: projectjuli.usGithub:    github.comBuilt Withcamtwistimage-processingnumpyopencvpythonseizure-preventionTry it outprojectjuli.us      Submitted to    DeltaHacks 2015 - Hackathon for ChangeWinner                1st Place                  Created by  Kestin GoforthFIRST Robotics Alumnus.Arya SolhiRyan MarksChristopher Stokes  The first issue was finding a good source of data.  Luckily Cornell's Lab of Ornithology proved to be a treasure trove.  We were able to get detailed Bird descriptions as well as multimedia data.  Were we able to get high quality photos and audio recordings.   This really brings the Bot alive.      The first challenge was to detect shaking and we deal with it in a simple, but effective way. The second one was to write a wrapper for web3.js in Swift. The last one was to deploy it to real Testnet. We have to avoid gas limit problem and spend as less Ethers as we can on storage info.  Building the UI interface as none of us were desginers.  We're extremely new to programming on the Ethereum platform, so everything involving Solidity, Web3, building test chains, and ultimately building a sidechain, are all very unfamiliar to us.  A lot of our time over the past few weeks was spent studying up, and we still had a lot of learning to do once we got started.We really never ended up getting our API working correctly, so we'll continue to build that after ETHDenver.  We think that this is an incredibly useful product for the scientific community.  Hardware & version compatibility issues . Debugging NodeJS to communicate with the smart contract.    Our original plan was to allow the user to take a picture in our android app, and it would upload to our server, then we would play it back to them. We couldn't get android and python to communicate the picture among each other.We solved that by opting to make the entire service a web app, that can be accessed by both personal computers and mobile devices.  It was difficult to get the gestures to work the way we wanted them to with the Carnival API provided by pmd/gestigon, but we prevailed in the end by doing lots of testing with the headset.    It was our first game ever and we had to learn everything from scratch  There was a number of difficulties with mechanical tolerances and bluetooth connectivity. (And not eating the candy).  For all of the team members, this was the first time to ever use Unity. Working with a tool that we did not quite know how to use was demanding. We also faced a challenge communicating information between users on unity. Many of us were also new to game development.  Developing a web application hosted on Microsoft Azure was our biggest challenge. Initially, our team used Go to develop the back-end of the application; however, Azure never seemed to be compatible with Go. Thus, we had to switch languages, and completely re-write the entire back-end in Node.js. Another challenge we faced along the lines of Microsoft Azure was figuring out the Machine Learning. It was difficult to create training models with the help of different statistics such as chi-squared. We attempted to create an optimal training model on twitter feeds based on binary sentiment analysis. Upon building the training models, it was another challenge to create a predictive experiment based on the optimized training model, and hosting the solution on the web. Although we couldn't get the machine learning algorithm to work, we made great headway in learning this sector of Microsoft Azure. AccomplishmentsDespite all the challenges present, we were able to finally implement Microsoft Azure as a web host for our application. Even though we did not completely figure out the Microsoft Azure Machine Learning, we made great progress in learning how to utilize this tool to perform sentiment analysis on twitter feeds. In order to actually create the machine learning, we used a direct approach with sentiment analysis through node.js. On the front-end, we were able to coordinate all the back-end to co-exist with the front-end. The most important accomplishment was to develop the Azure hosting service, along with our web application.What I learnedThis hackathon was full of twists and turns and we definitely learned a lot from it. From how to use Azure to what web frameworks work best with our front end, our general takeaway for this hackathon was to remember to reach out for help. The Microsoft team was immensely effective in mentoring us and helping guide us to reach our goals; its been a game changer for us and its something well be sure to do in future hackathons. What's next for STOCKASSISTWed love to expand on every aspect of our app. This means wed love to include more and more social media outlets to better refine our data set, and thus use this to refine our machine learning algorithm as well. We also believe that this app would be effective across more platforms - from mobile to tablets to smart watches. If theres an influx of negative social media coverage for a particular company, having a mobile app ping our users would be highly effective in helping them save money in the stock market. Built Withazurehtmljavascriptjsonnode.jsTry it outstockassist.azurewebsites.netstockassist.xyz      Submitted to    HackSC 2015Winner                Microsoft Hack - Best Use of Azure              Winner                Best Data Science Hack                  Created by  I worked on machine learning (ML) though the Microsoft Azure package. Being the first time I was exposed to machine learning (ML), it was a major challenge to implement and execute; however, by the end, I learned a lot about ML and its application!Heran PatelHacker by passion. Entrepreneur at heart.Created the front end and was the designer. Presentation/marketing, managementMichelle TjoaI wrote the backend twice. I first wrote the entire backend in Go; and then after discovering compatibility issues with Azure, I rewrote the entire server using Node.js. I also spent several hours with Heran learning and writing R scripts and interfacing with the machine learning APIs. I also helped out on the frontend by writing some of the HTTP requests and communication logicMadhav GharmalkarIf it's worth doing, it's worth overdoing.I created the front end with MichelleMadeline Chan  Collaborating on an Xcode project with git was a learning experience. We ran into gitignore issues, merge conflicts, and Xcode oddities, but we managed to get a solid workflow established by the end of the event.   We built a large part of our project from scratch without readily-available APIs. We were also experiencing a lot of dependency issues, which consumed a lot of our development time.  The challenges we faced were during the prototype development using different conducting material. We had trouble in adjusting the conducting material on the pill bottle such that people can easily use it the way we were using it. We also faced a challenge during the testing of different conducting material.   One functionality we had wanted to implement was to auto-refill prescriptions via the Hy-Vee pharmacy website, however the website has encryptions so that we could not find the input text-box with pythons selenium.   Since this project uses several technologies it was challenging to figure out a way to connect different systems and their interaction. Also understanding  the Visa API network security mechanisms was challenging as this was a relatively new field       It was important to us to make sure the bonding curve logic was upgradeable, but also that we be able to verify that all proxy contracts on our platform pointed to the most-recent controller.  This combination of verifiability and upgradeability required thoughtful contract architecture.  Rinkleby going down at 3 am didn't help either XD  Since we're beginner programmers, we came in not knowing how to develop a website. A major part of our site was the authentication using Firebase. We unfortunately couldn't implement this element on our site.  reading advertising payload of BLE beaconsnetwork connectivity  We initially wanted to assign geo-accurate points of interest, walls and walking paths as features to Mapbox's special layers. We started by taking the indoor layout image of the floor and then georeference it. Even though the layer features were assigned in Mapbox Studio, we were unable to render them in Unity and make them as the reference for the indoor points. We had to compromise by making a virtual representation manually in unity for reference to where the player is positioned. As a result, we made an assumption of the position where the user is at in the application.    Plotting the mood data in a graph took a long time, as the data had to be used w Defaults to save their values upon exiting and entering the app.  Also designing the entire user interface was very time consuming, although I used a free icon pack for the artwork, designing the layout and specific dimensions of every object took a long time.  Frame rendering for Face Recognition was very heavy to perform in real time.      Rofael (the backend author) and Caleb (the frontend author) only had some experience with Elixir and Angular, respectively. We had a fair share of issues getting used to the frameworks, but were able to triumph over them. What we learnedA lot. These were tools and frameworks that we both didn't have much experience with. Thanks to relying on each other and the great documentation and communities around the products, we were able to make something pretty cool.What's next for DragonParkThe back-end and front-ends can easily support real time updates via a WebSocket connection - we just didn't have time to implement it. That'd be next. Maps integration (show a map from the user to the parking lot) and filtering (by which lots the user has access to) were among the ideas considered but were not implemented due to time constraints.Built Withangular.jselixirnginxphoenixpostgresqlTry it outgithub.comgithub.com      Submitted to    Dragon Hacks Spring 2019Winner                2nd Prize              Winner                Hacker's Choice                  Created by  Front-end and assisted with the back-endCaleb FalcioneBack-end, hosting, and submissionRofael AleezadaSDE at AWS. UAB '19.  We didn't know what we were doing. Out of the seven tools we used, four we had to learn from scratch. At first, we thought we were going to use Electron, etc., but that would have been too much. As we worked on this project, our ambitions had to scale down to our real capabilities. This was painful.  Synchronizing video Having no network is not great for a real-time networking application.Having no power is not great for a real-time networking application that needs a network.  Not storing any sensitive data that comes out of Azure security products about users and resources on places other than our Ticket system.Making it unavailable from the outside (verified via pen-testing).Not storing any Secrets in configs, only Azure KeyVault is used.Not every security provider through the Graph API supports notifications (only polling).Accomplishments that we are proud ofMaking a highly scalable solution.Very low monthly costs by not only pulling all the alerts by a set interval but also subscribing to notifications.Everything is Stateless, making is easy to resend/retry part of the solution and also to update the production environment while it's live. Making data in CosmosDB only available for the tenant that the code is executing for by using the tenant id as a partition key. By making the partition key required it is virtually impossible to retrieve data from a different tenant.What we learnedThe learning curve from day one in a Cloud world is the use of the Security Graph API itself (version control) and the output from the different providers, some providers give more in-depth information and some give basic information. Second, we needed to find a solution on duplicates from the different providers, e.g. Azure AD Identity Protection has an overlap with Microsoft Cloud App Security (Threats).What's next for Cloud Security Operations CenterAdding AI to correlate more data (sources) and use intelligence to reduce the number of false-positivesIntegrate SOC services like vulnerability scanning and new(s) malware reports for our customer (CISO) to give them a peace of mind Built With.netapplication-insightsazureazure-data-lakeazure-functionsazure-logic-appsc#cosmosdbkeyvaultservice-bus      Submitted to    Microsoft Graph Security Hackathon    Created by  Business Lead of our Managed Security solutionStefan Daelemans#WeAreInSparkMark FoppenDeveloper | .NET | #Azure | #Security minded Derk van der Woude  Getting web sockets to work properly without complete webRTC library. Getting multiplayer stable across WebVR, WebXR or Prismatic. Video codecs not supported by exo-kit. Bad wi-fi at the venue which prevented us from loading assets and interrupted work often.   The initial base processing system (Arduino 101) had built-in accelerometers and gyroscopic sensors, but no driver for our radio transceiver. When switching to a platform with driver support (Arduino Uno), we no longer had such sensors and thus had to make use of a much more limited array. After a poor circuitry connection, we had to design and build a hand-made motor driver to replace our recently Kentucky-fried circuitry.  I had to fill out this survey to submit the project.  Trying to find ways to create multiple connections in chat rooms. Socket.io helped abstract away a lot of the complexities involved with the socket protocol.   It was a challenging problem to handle more than 300Gb of videodata during hackathon. Also it was really difficult to make up interfaces for such new task.  Validation zk-Snarks inputs. Making all works together.  Wanted to make my own game in p5.js, but the handler for react didn't work well and would error. Got close to the end and ended up scraping that for just patching an open source one.Microphone sensitivity is very variable.Turns out it's really hard to play flappy bird with this...  The iMessage platform has a lot of restrictions, many of which are undocumented. Because the app exists as an extension inside the Messages app, many normal app capabilities are unavailable. We ran into a dozen of such issues, and adapted the design of the app to fit into the given constraints. Although our final result had to undergo many unforeseen design revisions, we're happy that we were able to create a working and fun application.  The math was very difficult and attempting to rush a 3D printed design was also difficult but there was a rush because 3D printing would be a very time consuming process. Thus we also had to create a lot of our latter designs around the already 3D printed parts.VuMarks were also difficult to create. VuMarks must be very easily distinguishable from each other and non-symmetric along any axis, and therefore took a while to get finely tuned and calibrated.Finally the math was a very difficult thing to visualize. We had to go from 3D space to 2D space and there were some difficulties with projections. The coders did end up writing relatively bug-free code, but not before a long, arduous thinking process.    Using so many APIs, we were having problems with the callbacks hell. Designing the chatbot so that it is user-friendly was also a big challenge for us.What I learnedDesigning product is really difficult.What's next for BobMaking Bob more human-like. Let the user customize their experience with Bob, allowing user to decide what kind of sentences structure corresponding to what commands by using IBM Bluemix.Table number860D2Built Withbluemixfacebook-messengergoogleibm-watsonmongodbnode.js      Submitted to    RevolutionUC    Created by  I use IBM Bluemix Watson IOT platform to build intents and entities to train the bot to recognize similar patterns in command. Rechannel the commands back into the server to execute exact commands user wants. Michael NguyenLe AnhTri NongTien Anh Nguyen  There were a lot of challenges with getting a web server set up, connecting a domain, and hosting the API calls through the domain to be accessible via our front end developer. We tried using Google Cloud Platform, but could not authenticate a public key to ssh to the host name. We then used Amazon-Web-services, but we then were lost with how to speak to MongoDB and also hosting the domain on the web server for the API calls.We were advised to use API Gateway and AWS Lambda but struggled to understand this (as we had reached our 16-hr mark of coding) Eventually, one of our developers got a connection to work between MongoDB and Azure, even without using the domain name.The File type for the receipts themselves were going to be .pdf files. We were able to use fpdf in python to write/create pdf files; however, we could not format them correctly like a receipt so we compromised!  ..  We completely bricked our router multiple times. Had to return one and get another.       Attributes of a character can be hard to learn. For example, we tried to learn the serif-ness of a character with StarGAN, but it does not produce comparable results with simple latent space interpolation with VAE.The representation of a vector file is vastly different from that of an image. Even if we could get a correct base VAE model to generate raster images, the recurrent units to produce command tokens and numbers are hard to train and we could not get a reasonable result.Hooking up the back-end models with the front-end UI needs a lot of communication and engineering.    Integration with DocuSign API and time =)What we learnedWe learned to understand the details of American laws.What's next for The Paperless SAFEBecome ProductHunt's "Product of month" =)Built Withcanvasjavascriptreactreact-pdf-react-signatureTry it outsafe.udevs.comgithub.com      Submitted to    2019 DocuSign Momentum Conference Official Hackathon    Created by  I worked on the front-end. It was my first time using Canvas, which was a little intimidating, but I learned a lot.Alex RomI worked with modern front-end technologies and i learned a canvas technology!Ilya PasuykJavaScript developer  The first challenge is lack of enough time for the hackerthone. The first phase began on the second week of June 2019 which involved the teachers section first for it being the most urgent.The other problem is lack of skilled collaborators from my region willing to collaborate followed by lack of enough funding and less available tailored teaching materials for the Kenyan curriculum. There has also been no available tablets that are currently being used in to test the beta app.      It took more time than what we initially thought to pick and set up a front-end platform. We went through several iterations of starter projects until we found a solution we could get running and continue to edit fast enough.We also took more time when deploying to the Google Cloud Platform is difficult to work with. JavaScript is no friend.  InnoEnergy - How to empower Public Authorities to support innovation ecosystem in order to promote entrepreneurship.  There are not enough examples of APL and Audio Player skills. Error messages are not descriptive enough, which made the troubleshooting harder.   The most challenging aspect was thinking up the whole design of the fish and the platform, as well as thinking of what is the catch a fish condition. We ended up sticking to the engineering design of the original fish game (inclines underneath the platform lining up under some of the holes), just bigger in size. The RFID sensor reads the RFID card even when it's 2 inches away, so we needed to think of another aspect we could test for in order to detect whether someone caught a fish. In the end, we decided the win condition should be a mixture of reading the RFID code while the rod position has changed significantly to show they are still holding onto the fish while it's up in the air.  I would really like a slideshow of images and videos and when a video was encountered the slideshow would pause while the video auto-played but I could not find a way to do this.        This was a very technical hack that involved many endless hours of troubleshooting, especially since none of us had done many of the technical hardware and software hacks involved. We were new to the Arduino and drones, as well as how to build the circuits we needed. This hack first involved identifying multiple UV light sources, of which we went through a couple LED light options before our final solution. Along the way, we encountered faulty hardware, and our procedure of soldering and connecting our initial ideal higher-powered UV LED lights resulted in their electrical failure. We then had to improvise and go with another set of different LED lights that were in the UV spectrum. Next, there is little documentation of hacking a Parrot Drone with an Arduino that combines all of the endpoints we included in our hack. Beginning with wiring the circuitry on breadboards to connect multiple distance sensors to the Arduino, we encountered issues with the readout and responses that we had to troubleshoot. Part of the issues were related to the voltage, and we eventually figured this out. Next, there were differences in Drone pinouts than what was posted online, we had to identify through voltage drops and differences the functions of the pinouts, and then connect the drone to the Arduino. For many hours, we were only able to connect successfully the computer to the Arduino and receive proper printouts from the sensors, and not from the Arduino to the drone. To solve this issue, we had to seek some advice on matching the voltages between our various types of hardware in order for all of our code, hardware and sensors to communicate. Based off of the advice and feedback we received, and online forums, we put together a level converter to divide voltage that allowed communication between the sensors, drone, and Arduino to finally happen (BUT, not before we faced a malfunctioning Arduino, and after using a fresh one we figured out that this had been the problem for a couple hours). Next, we had to weigh all the components and shave off any excess weight-- considering a second voltage divider to draw less energy. After that, we had to figure out the actual hardware and mechanical mounting of the LED lights. The next challenge was consolidating all of our multiple circuits into one small and lightweight board. Then finally, we wanted to command the flight of the drone to be similar to an iRoomba vacuum. We wrote javascript to automate the this type of room sweep via drone flight movements. This was one of the toughest hacks, as it was not clear in any instructional references we could find how to actually control the drone via the Arduino while using the sensors. We spent at least a full 16 hours on trying to get the communications between the devices working. Finally, we succeeded in reflecting UV light off our of drone propellers at the end, which allowed us to consider our hack a proof-of-concept for use during an epidemic. Accomplishments that we are proud ofThe amazing collaborative teamwork and the fact my team worked together non-stop to make this happen! What an incredible effort, especially given that none of us have done much of the things we had to figure out for this hackathon. We each learned how to integrate and utilize new hardware, software components and languages, and had to draw from past science classes as far back as high school for the scientific concepts.What we learnedMany many things, already alluded to in the challenges section... mostly along the lines of working with new hardware, circuits, writing Arduino code, working with Arduino libraries, interfacing multiple technologies, hacking a drone, new/unfamiliar coding languages, etc. What's next for X-Terminator DroneTrying to find the support to turn this drone into a reality where it can be utilized out in the field. What would be ideal is to use holograms on the propeller blades, instead of aluminum, and to use a high-powered UV laser. The future drone will also come with a companion rover that is a land-based robot that can manipulate obstacles and objects in the environment to deal with unfamiliar terrain and situations.Important video links that should be viewed to show we've done technically! Since we were not allowed to demo our hack the way it was meant to be used indoors:1) Ultrasonic distance sensors working: https://youtu.be/2WZT5U6yAT42) Sputter coated aluminum propellers: https://youtu.be/uWcDtlGyKJgBuilt Witharduinoccssdronehtmljavascriptnewpingnode.jsultrasonic-distance-sensorsTry it outxterminatordrone.comgithub.com      Submitted to    Hack Arizona 2016Winner                Raytheon - Best Drone Related Hack (1st Place)                  Created by  I worked on wiring up and soldering the electrical and optical components, making sure all of the circuits worked. I learned how to create a level shifter using transistors. I also sputter coated the propellers of the drone with aluminum.Sunglin WangI worked on using node and the arduino as well as sensory input from the ultrasonic sensors in order to change the course of the Parrot AR, my first exposure to all of those! While the final result isn't precisely what was originally envisioned, it's well on the way and I think shows the effort we put into it. All in all it was a great experience with a couple sleepless nights and some very intense on-the-spot learning.wendy wangI was the team leader for this project. I worked on both hardware and coding components to test what was working or wasn't working. This was my first time using the Arduino and Parrot AR, and I was able to learn how to use the API for Arduino and how to write programs for the Arduino. I also figured out how to hack the drone and connect the circuit boards of the Arduino and drone together. It was great to learn how to make the hardware to talk to the software in this type of application, and really see the marriage between interdisciplinary fields thanks to the scientific diversity of my team. Alice Ferng    Wed say that the biggest challenge was to find the answers on bunch of unknowns and bottlenecks that were popping up while discussing the idea with sponsors and other hackathon attendees. For example Weird looks about having this kind of device in toilets and privacy related issues or "intense subject matter". As well weve had some technological issues, especially using the beacons and using some sponsors APIs. - (MapQuest) :)                Lacking the resources to develop for iOS, how to distribute apps with sensitive information.    Only Isabella had a license for Gamemaker (the only Windows user), so only her computer was available to actually code on... this caused a bit of stress, to say the least.   The two main challenges we faced were performance and the integration of a touch library. I had a fantastic old multitouch library I intended on re-using, but this was not compatible with Unity's window handle, so I had to re-create it based on Unity's API. It was the first time we've hit performance issues since learning Unity, so we had to familiarise ourselves with Unity's profiling tools and in particular how Unity batches draw calls.  We realized that we didn't bring enough parts, so around a total of 4 trips to the hardware/DIY Maker store were made along with the help of MLH's hardware. There were a points were we thought this wasn't possible. We also had to prototype without breadboarding as the amperage would fry everything so a long while we were flying by. Also, the team members had no almost experience designing legs or any medical device at all whatsoever.  Fixing CORS issues - using a proxy serverWorking with the restrictions of using a chat client in an iframe - it couldn't be modifiedCreating test users for Moxtra on Azure  We started late on this project, and built Median in less than two weeks. We're really proud of the teamwork and communication we established through this project. Web development is an iterative process and we're still building now!What we learnedAs we were building the project and implementing features, we found that specific features gave rise to others. For example: originally, Median was designed for use by healthcare professionals to communicate effectively and efficiently with other healthcare professionals. However, if patients could also upload changes to their own state, healthcare professionals could utilize higher patient data resolution, allowing for more accurate and specific diagnosis. Therefore, more options were added to the patient timeline to encapsulate these cases. The entire design process was an adventure in itself and we feel there is still much more to be built on the current version of the project.What's next for MedianWith so many data input options in the hospital environment, we feel that the ability for patients and professionals to sort through the data in a quick and meaningful way would further catalyze the patient care process. Additional functionality such as larger multi-media support, with some autocomplete and search options could make Median even more useful. We are also planning on integrating Median with medical FHIR API's to integrate data and data analysis into our application. Built Withfirebasenode.jsreactTry it outamber-inferno-4829.firebaseapp.com      Submitted to    The More Disruption Please Innovation Challenge    Created by  Alex NgaiAndrew Ngai  Search for near routes and join it in just one, it is a hard topic in which to think.      Connecting raspberry pi to wifi - at first we didn't have monitor/keyboardInstalling windows for Atmel development tools required to fix problems with smart lightingForgot that PHP does not detect isset() if the value is equal to zeroSleepOne of ura rings didn't log the activity information because the person slept at a place without internet...    Some of the hardest things to accomplish while creating our product were:Creating the algorithm to decipher the meaning of a user's code in English.Using data structures to effectively organize the large amounts of data created by our programCreating user accounts that can save code automatically  Given the time constraint, we couldn't make the program interactive.  Creating a smooth, intuitive experience for the user.   There were two major challenges for us. First, to define the right training model for our AI. And second, to find good images in enough number (we needed at least 1000 images for each brand\ age group\ cloth item\ color and so on) dataset to train the algorithm. As dataset we used is quite small, our app might not work perfectly, and we plan to continue training our ML algorithm using more pictures.          We tried doing voice recognition and I learned heaps about phonemes and how that all works, but we came to a dead end and realise we needed to rewrite and use a different library so had to pause on that. I'm keen to get back to it but need to pace ourselves due to too much to do. Mobile development on Cordova using Windows has been difficult due to debugging tools being a bit faky and general slow code-debug-run cycle that you get with the build process. Emails are difficult to process with the many complex formats in the wild.      One challenge that I faced when building RokkSalt was handling the different situations that could arise when dealing with the recurring billing through Stripe and making sure to cover all of the bases. I wanted to make sure that everything that could happen was accounted for when dealing with taking payments online for this service.    We have 3 primary challenges to deal with on this project: 1. Inspire Coopetition by defining a collaborative profile that was large enough to be valuable, but didnt infringe on our customers ability to compete.  2. Building an extensible platform that was simple to use, but flexible enough to handle the complex requirements of todays healthcare program and 3. Getting organizations to deal with the problem. In many ways the inefficiency were attacking is already budgeted in. It can be difficult to help people understand how badly the current solution is hurting them..Accomplishments that we are proud ofWe launched our product at HIMSS 2016 in Las Vegas this year and we received an overwhelming positive and encouraging reception.  With that success and with our history, we have been able to secure varied and influential pilot clients. Real world application means real world feedback. What we learnedWe knew our healthcare system needed help and innovation, the depth of which we are still exploring.  We've learned to be savvy data collectors, innovators of simple solutions and great listeners.What's next for ZenPRMWeve completed our pilot phase and are bringing on our first paying customers. Now we are focused on building our brand and filling our funnel as we bring on new customers and communities nation-wide.Built Withangular.jshtml5javascriptscsssqlvb.netTry it outgetzenprm.com      Submitted to    The More Disruption Please Innovation Challenge    Created by  This team is comprised of some highly talented developers and UI/UX engineers, sales people and founders. We are committed to amazingly simple solutions for health care administrationVal GrigoriouI am responsible for the user interface and front-end development of ZenPRM. I specialize in HTML and CSS.Mike SpencerUX Engineer at Algonquin. Fan of science fiction, fantasy, classic rock, and hockey.I am responsible for the back-end development of ZenPRM. I specialize in AngularJS, .NET Web API and SQL.Charles MarraI guided early value validation with potential customers to understand how they think about and solve problems with ZenPRM. David ThiemeckeDon LeeDoing my part to improve the business of healthcare. Dev background. Now product mgmt/sales. Started #hcbiz & Health 2.0 Buffalo.Nicky Gianadda  Getting right feature vectorUnderstanding temporal nature of Harddrive dataChoosing right modeling techniquePerforming binary classification    We hard difficulties getting the Google Maps API run with our use case and also had some other minor complications but we were never stuck without knowing where to go.    There's a lot of moving parts in this hack, so first getting them to work individually, and then programming them to play nicely together was quite a task.       Some challenges we ran into were figuring out how to communicate between the glove and the web server; finding how to wire the glove so that each movement could intrusively be measured with maximum accuracy.What I learnedWe learned about arthritis and the research going on being the condition; how to read from a serial port, parse the data, and write to a MySql database; and advanced soldering techniques. Built With.netarduinobootstrapc#google-cloudgoogle-cloud-sqllesslibserialphptoroTry it outpowerglove.cntr.me      Submitted to    HackUMassWinner                Finalist              Winner                Champion              Winner                Best Hacks for Health Prize Category              Winner                HackUMass Popular Choice                  Created by  Implemented backend resources and data structures as well as handling client-server API integration between the hardware and web app.Christopher TranWorked on the mechanical and electrical systems on the Power glove and set up the communication between it and the computer that handled data processing. Anmol ModurWilliam KrodthoffZachary Thompson  iOS Push Notifications. We ran into a lot of problems implementing Azure Notifications Hub for iOS.Getting Photon to work with Azure.  Collaboration: We had challenges to share our code with GIT. Solved.Data model: The data was not very easy to understand. Some adaptions were necessary to make the data usable. Complexity of 3D models. It took us a lot of time to create 3D models and then reduce the complexity of all models to ensure good performance.Availability of WLAN: As we have an online application transferring a large amount of data, it was sometimes very hard to get it running because WLAN was N/A.UI/UX Design: We invested a lot into the interaction between Human and the 3D Models. This is why our UX Designer tested several ways in prototypes before we started implementation.Our biggest concern, related to presentation is the weak WLAN which might  cause performance problems.     Although the NL API provides salience scores, some manipulations had to take place in order to make them useable. While working with the API, we were faced with the decision of whether or not to remove stopwords from the text. Stopwords are short words such as the, at, on which provide no real significance to the meaning of the sentence. Many NLP algorithms remove stop words to better analyze phrases.For our project, we run the NL API on two sets of the audio transcripts: one containing stopwords and one with stopwords removed. This is because, as we were testing our algorithm, we noticed the API produced significantly different results for keywords depending on whether stop words were included. Since we believed both runs had useful outputs, our final output for salience takes the top five salient words from both algorithms combined.  Own smart JS-based implementation of MerkleTreePDF file and QR code generation in an Angular application  Integrating APL with Alexa skill and deployment to lambda.    We had a lot of trouble modifying Clarifai's starter app to implement the functionalities we wanted. We eventually decided to start from scratch and develop an app from the ground up. We also had some problems using the Clarifai API, as certain features were hard to interpret. We didn't have much experience with developing user interfaces, so we had some trouble generating designs for different pages.What's nextWe expect it to be hard to engage users to use the application in the long term. Therefore we would plan to implement more features that offer incentive for people to use the application. One of these features could include quizzes that reward points, based on items they have disposed of in the past. Also we thought about implementing the Snapchat API so that users can display their score to their friends, and show off that they are being environmentally friendly.Built Withandroid-studioclarifaijavamachine-learningxmlTry it outgithub.com      Submitted to    StarterHacks 2019    Created by  I focused on implementing Clarifai's Image Recognition API and classifying a picture based on the objects detected. Justin KreinerECE student at UTorontoI worked on designing the XML layouts for the UI of the app. I also wrote Java code to interface the XML layouts with the various activities in the app. I learnt a lot about XML syntax, User Interfacing, threads, intents, and activities.Mahad ZaryabUW Mechatronics Engineering I worked on implementing the camera activity and learned about threading and incorporating APIs into our android appTiffany YehI worked on prototyping and overall branding/designing of the productChia-Hui Yeh (Catherine)I worked on the algorithm that categorized items based on their clarifai tags and determined what the object was likely to be. I learned a tremendous amount about xml, android studio and android activities and threads.Dhruv MananiMechatronics Engineering Student | University of Waterloo  The Display of the Spot created some challenges:The Display is round, so the normal QR Code would lose the corners and so the scanner could not recognize it. Solution: Added an additional boarder to avoid losing the three orientation squares.The shown text is presented over the QR Code on the Spot, so some informations are lost and the scanner sometimes cannot read the data. Solution: While generating the QR Code I use the highest quality which added redundancy so that the lose of some information can be restored. Additionally I shortend the shown text to the minimum, so less information is lost. Both solutions reduces the comeliness on an Echo Show. But it is not possible to distinguish both devices, neither by the Intent request nor by the HTTP Request of the image itself.Adapting the audio player interface to the infinity image presentation hack. This means, only using it, when display is present and some additional intents to handle stoping etc. correctly.    API integrations    Learning to use a stack for the first time and using it to deliver a short time project.    One of the conceptual challenges was in whether the users would track the bus for others. We realised that we needed to make it extremely easy to start tracking once the user boards the bus to we did exactly that. We also took inspiration from apps like waze where people provide information like traffic cameras and crash data for others with no remuneration. However we conceptualised a points system that they can get based on the time they have been tracking and redeem the points for rewards towards companies partnered with the app. Programatically, one of the biggest challenges was making a progressive web app that can work despite all the security issues involved during development. Also integrating the powerful here.com api and using all its features took time to learn and implement but we were able to make it fully functional in the end.   We only had approxemately 1 week to polish the mockup we had. So time was not in our favor. We only learned about this competition 9 days ago and had to get the confirmation from PlayRescue to participate.  There were limitations with regards to various system resources that we needed to optimize due the large DNA datasets (in the order of hundreds of GBs) typically used. For example. memory as well as disk space utilization have been optimized to eliminate these bottlenecks.        Figuring out how to match the animation with the beat of the music, determining an appropriate level design, and getting git to work right.    Designing the entire databaseConnecting database back and forth with front-end Training machine learning data to improve accuracyAccomplishments that we are proud ofTackled an issue that was important to usSet up the infrastructure and foundation of a system that actually solves a real problemBuilt fully functioning web app We did it!What we learnedThe communication that must happen within a full stack environmentJS is on the client-side while SQL is on the server-side so we need a middleman to bridge the twoWhat's next for Know MeGetting users input data to generate network effect and train our machine learning modelsDeploying to mobile for user accessibilityConducting more user research Built Withajaxasp.netazurecss3html5javascriptjquerymachine-learningsqlTry it outknow-me.azurewebsites.net      Submitted to    nwHacks 2017    Created by  MoHan ZhangCharmaine LeeUBC CS. Co-Founder of UBC Hacks. Diana IftimieAurora QiuBusiness & Computer Science student at UBC, Vancouver. Technology Enthusiast, Innovation Devotee, Photography FanaticZixuan Yin    To test our chrome extension, we ended up stalking people we hate for hours...And yeah, we have no experience building a chrome extension  We thought we could build an app but could not agree on the tools to use, since most of us are not familiar enough with Android studio or other tools for app development.  Setting up the backend while allowing for foreign key constraints and unique values in Parse. In an SQL based backend system, setting up such constraints is easy but with Parse, such constraints have to be coded into the server code in Node.js. What I learnedI learned how to write code so that there is efficient communication between the mobile application and the Parse backend. What's next for Wellness DiaryMonetization - Users would connect to therapists only after paying a fee based on the experience and a "rating" that a therapist holds.In-App communication - Wellness will contain a more robust communication system within the application that allows for easier messaging as well as video and phone calls.Note:All the code for this application is in a private repository on GitHub. I will be willing to provide access to the judges if required. Please email me at everythingskc@gmail.com for access to the repository.Built Withandroidandroid-studiogradlejavamicrosoft-cognitive-servicesnode.jsparsesqlite      Submitted to    Make School's Student App Competition 2017    Created by  Santhoshkrishnachaitanya Chelikavada    I couldn't even list all the challenges I ran into. Though they always seem daunting, I eventually get past them. I would say the one that stands out is state restoration and handling save games. It took me quite a while to finally understand the concept behind what was going on in the background. The feeling I got after it started working was monumental.   something on servers here? with not being able to use semiphone and cake not working We ran out of time with adding an upload images and adding a text typing feature      Initially, our plan was to use a MUSE headband - which unfortunately was not available to us. We also intended for the experience to be in VR, however we were unable to rent any of the hardware for the project. We adapted our ideas to make them work with the equipment available to us. The most challenging problem that we had was to keep an open connection between the database and Unity, as the script was making a call to the database once each frame (about 60 calls per second). We managed to reduce the number of calls made and to keep the connection open for the duration of the experience.  The 3D JavaScript technology was new to us. At the first part of the Hackathon we struggled while getting to know the technology. But around mid-Saturday it was like breaking through a wall - things started to happen!  1)Getting the Vive2)Getting the Vive working3)Fire alarmAccomplishments that we are proud ofGit workedWhat we learnedHow to VRWhat's next for Disco MelthemExit to EABuilt WithmayaphotoshopsteamvrunityvrtkwacomTry it outgithub.com      Submitted to    Junction 2017Winner                Junction Game Jam                  Created by  I was working on the characters From modeling to texturing with some animations Nataly LeplerAri-Pekka HrknenTuomas KontolaDann MensahAmirBuganov  Due to the nature of 3D model generation, hardware limitations cause rendering and image processing to take a significant amount of time. Furthermore, since data formats and dimensional differences are numerous throughout the lengthy conversion and piping process, connecting each piece of the puzzle was no small task. To properly sample and mesh together hundreds of images, hard drive space became an issue. Additionally, IBM Watson is unable to detect object boundaries by itself. We had to use OpenCV to find out where things are and then feed image partitions into IBM Watson individually to get information.    Since we needed to make a song data encoding system from scratch, there needed to be a lot of prior design work to be done before any code could be written. A significant issue was figuring out how to take in the song data, and then put out the steps at the correct time, and then taking in the player's inputs to hit the spawned steps. It ended up being a very complex system that took hours to debug at one point. In addition, we used Unity Collaborate as a version control and repository system, which sometimes would overwrite one of our changes, forcing us to do the same changes again.  We are complete beginners with Unity so we learned as we went during the Game Jam. As a result the game is very minimalistic.    Finding out what we are hacking on.  The drag and drop feature was hard to debug.What's next for QR Magic GeneratorI plan on adding gif functionality to have movable QR codes!Built Withcsshtml5javascriptjqueryTry it outwww.qrmagic.tech      Submitted to    TinoHacks IIWinner                Top 10              Winner                Best Use of .tech Domains                  Created by  Sean Cheong      Google Maps API needed some tweaking. JSON vs. JSONp parsing took a while as well.  Collaboration was difficult for Josh and Dalton, due to their shared unfamiliarity with React Native. The two frequently wound up repeating work the other had done due to an unclear direction at first, but as the application developed, the pair found ways to focus on separate efforts more frequently.Eli ran into a number of challenges integrating ReactiveX, as it had been some time since he last used the library. Additionally, the two teams had to explore how to share code in an API form between the two projects, but eventually decided to split down the middle as bridging Reactive with Declarative code was not always straightforward.      Most of the challenges were related to security and properly managing the power-up settings and the associated Trello webhooks.Accomplishments that we are proud ofChecklist Bot is simple to tune, fast and works well ;-)It is live @OCTO and in the factory of one of our customers to organize the workshops. All the templates are stored in a single board which describes and programs (at the same time) the factory standard processes.Releasing the code in open-source is fun!Participating to Codegeist is fun and we are proud to present this product!What we've learnedWe've discovered Glitch - a great platform for testing NodeJs code - and maybe for hosting this power-up (time will tell)Padawan Louis learned a lot about writing clean code. He keeps on improving tests and code quality with the help of his software craftsmanship jedi Aurlien.What's next for Checklist Bot?Collect feedback from new users.Add new skills to the bot with a set of new actions: mail, http requests, create card, chat (it's a bot after all!) but we need to check first that it would not turn it into a bad bot.Write a post on OCTO blog to describe how we use Checklist Bot in real lifeAttract new contributorsCreditsPowered by OCTO Technology, 2017www.octo.comDesign & code by Louis Jeckel, Christophe Durand and Aurlien RambauxThe concept of checklist templates was inspired by Iain Brown' s Little Blue Monkey Pimp Your Trello Cards implemented on Google sheet. Thanks Iain for making it available and open source back in 2013!Built Withglitchhtmljavascriptnode.jsTry it outgithub.comglitch.comtrello.com      Submitted to    Atlassian Codegeist 2017    Created by  PO of the power-up and manager of Louis during his internship @ OCTOChristophe DURANDSenior Manager in an IT Consulting firm in Paris - Trello fanTech lead and advisor consultant.Aurlien RAMBAUXI coded the Power-Up itself and the backend Node.JS server. I discovered Node.JS and Trello Power-Ups a few weeks ago and I love it now !Louis Jeckel    Coming up to speed on the Adobe Extension environment. Node is familiar, but working with the Adobe API, which is heavily callback oriented, require some thought.      Surprisingly few challenges!  Using the Resumable Upload API on DriveItem was difficult. Most APIs I have worked with take a single request of bytes and then the request is done. Working with the raw data, chunking the data into smaller sizes, and sending those to the server in a way that works every time took a while to get right.  It was difficult to reduce the functionality into two smart contracts.  We also had trouble using the capsule feature of NUCypher, decrypt for receiving provider still needs work.   There weren't many examples of how to use the Watson Tradeoff Analytics tool outside of the official documentation. It took some time figuring out how to incorporate it with our project. Another problem was dealing with asynchronous Javascript issues with the load times to fetch data.    Getting all the hardware to work properly. The biggest challenge was getting the photon to accept and transmit data from the Arduino Uno. We ran into some serious problems with transmission using RX and TX pins, but we were able to use I2C instead. It would have been much harder without Pennapps' great mentors. The hardware was also hard to calibrate as there were multiple issues with sensitivity. What's next?Minimizing the cables, making the entire module more wireless/light, more analytics and information, various smart options controlled via blinking. If this was designed as a product, it would be much less bulky, with a similar, if not smaller profile than Google Glass. Built Witharduinocc++ionicir-sensorjavascriptpebblephotonTry it outgithub.com      Submitted to    PennApps XIIWinner                PennApps: Best Use of Health Dataset or Health Device                   Created by  Built all of the Arduino electronics/wiring and wrote the basic Arduino program. Also designed / 3d-printed the glasses. Jeffrey ZhaoMechanical Engineer | Hardware HackerAdded to arduino program, wrote code for photon, pebble app, ionic app. Cyrus RoshanSoftware Engineer | Hardware Hobbyist        Although the concept of Tunnel Comet is simple, but the appropriate guiding strategy must be well-designed. At first we found that too simple or sensitive design could cause Bullwhip effect in traffic. The guiding strategy must avoid providing improper acceleration and deceleration guides and should be capable of absorb Bullwhip effect. While designing the guiding strategy, we have to consider not only the interval but also the relative speed of the vehicles. Another challenge is how to design appropriate visualized guide patterns. Improper guide patterns design like not intuitive, too complicated or flashing would not only cause misunderstanding, but also lead to distraction and even dizziness.Accomplishments that were proud of1.Think up an excellent solution to improve the traffic in Hsuehshan Tunnel.2.Build a powerful simulation system which prove the efficiency of Tunnel Comet, and also serve as a perfect platform for us to test the guiding strategy and driving experience.3.Build a prototype with complete function by hardware component.4.Give this system a cool name  Tunnel Comet5.Work as perfect team and have a lot of fun1.2.3.4.5.What we learned1.Designing an excellent system requires comprehensive consideration in every detail.2.Knowledge about traffic congestion.3.Technique in developing system with Unity and Arduino1.2.3.UnityArduinoWhat's next for Tunnel Comet1.Keep improving guiding strategy and visualized guide patterns design of Tunnel Comet.2.Apply the concept to urban traffic and develop other vehicle locating and guiding technology that can cooperate with current system. Provide more possibility and flexibility to Tunnel Comet and other Intelligent Transport Systems(ITS) in the future.1.2.Built Withjavascriptunityvr-glassesTry it out140.113.34.15140.113.34.15      Submitted to    Hsuehshan Tunnel Transformer    Created by  I worked on designAlfred MaiDesign simulation system, build hardware prototypeSherlore ChaoMad scientist and also game developeranalysisLee YahuaWakeupTsaichi chen    bug-, crash-, error- ( ),         .       ,     ,   ,  ,     .      Students kept escaping the room through the nearest window. I would like to take this opportunity and say "Thank you" to the makers behind GPS technology...  When we start submit our idea to Intel, we decide to use SIM900 GSM Module because we have some problem with ESP8266. As time goes by, we facing problem with Software Serial communication between SIM900 and Arduino101. SIM900 using 5 volt in communication and Arduino101 using 3.3 volt. We thought this difference in voltage maybe the main problem why Software Serial wasn't work. After that, we decide switch to ESP8266. We used Custom ESp8266 module from GeraiCerdas named MCCloud. ESP8266 using 3.3 volt as main voltage, same with Arduino101. But Unfortunately, we still facing the same problem. We tried to solve this problem by searching in the internet but no luck. Suddenly, we tried switch the keyword from Arduino101 to Arduino Due. Voila, we got the solution. We add pinMode (rx, INPUT) and pinMode (tx, OUTPUT) to ensure Software Serial works.We also encountered problem with relay. We used this relay. The problem is relay must powered by 5 volt so relay can switch from OFF to ON. we come with solution using pinMode (relay_pin, INPUT) and connect one relay pin to 5 volt and the other to Arduino101 Digital Pin. With pinMode we ensure that the voltage through relay will 5 volt.Sometimes, we connect sensor wrong so we must soldered again. We tried to write down in a paper the wire color and the pin, so we can remember and not wrong soldering the sensor.Arduino101 have problem when we upload the firmware. We often press MASTER_RESET button to upload the firmware. Arduino101 also must powered by adapter before connect it to computer. We experience our Arduino101 died and cannot recognized by computer. We dont know the fault because we never did something wrong to Arduino101. We think it is because the board. Board getting hot when we connect it to adapter. But, after we have another Arduino101, it is OK until right now.We encountered challenge to understand AT Command from ESP8266. Article from luciorocha, alselectro, fuho, Electrodragon, and ReiLabs very helpfull to make us understand how to use ESP8266. Before we connect ESP8266 with Arduino101, we tried AT Command using UART to TTL adapter.UART to TTLTrying GET and POST method was very challenging because we must communicate between Arduino division and IT division to decide the link for getting threshold and sending data.Arduino101 cannot use every external exist library in the internet. We came with problem when we use custom/indie library such as DS1307 library and I2C LCD 16x2.IMU Arduino101 must be updated all time. When we tried to make delay about five minutes, Arduino101 cant get the gyro real value. So we decide to read gyro value in the delay time to ensure we get the right gyro value.For dashboard challenge is make live chart using Highcharts. Because the technology is new for us, so we need to learn deeply to make it live. It can be see at SFF dashboard.  We learnt about Robo-advisors in the middle of the developmentSample transactions list doesn't resemble real life scenarioAPI is still in its early stagesNowhere to sit  Integration of two beta protocols with layers of alpha tooling made for a challenge in debugging. Challenges ranged from cryptic error messages to manually inspecting bytecode.  The technologies used were new for everybody in the team so this was definitely a challenge. Also, adding the OCR functionality to the application proved to be more time consuming than we expected and it ended up pushing things really close to the deadline.  We ran into numerous issues accessing the camera and with delivering the image to the user regarding hosting and formatting of the image.It was difficult to make the smartphone camera a surveillance device that can be activated remotely as well as interpreting the images through computer vision. Additionally, extracting the real-time geolocation and sending it as an alert was challenging. We also faced problems developing our own neural network models to correctly detect the description of the intruder.It was really hard to develop a free and fully functional smart home surveillance system web application within the span of 36 hours.    I'm not a developer and this is the first WebApp I've written. There were numerous challenges learning JavaScript to write the Front End and try and make it look good. Azure Functions provided the ability to use my PowerShell skills to do the heavy lifting of interfacing with Microsoft Graph for my first ever WebApp.  The most difficult part was to make Diana context aware, so that she can make small talk, but get serious when we're asking for help!  Near the end of the competition, the hosting service we were using went down with less than 4 hours left in the hackathon.  fetch the data from google map and also the scraping the website  None of us had done a hackathon before and we did not sign up as a team. None of us had a solid concept for the hack going into the event.    Learning to deal with the zokrates CLI.    Getting speech-to-text to play nicely in a way that we could build in 24 hours was particularly challenging. We elected to create a terminal basedConnecting the Twitch Extension to our backend    Going into this project, I really wanted the app to understand you no matter how you phrased or spelled things. For example, I wanted it to understand something crazy as "cn u get meh some pizza. Im proud to say that after a lot of hard work, it does! This would be easy if I had a set of keywords like "pizza, Chinese, etc...", but I wanted Fetch to be much more flexible than that. I wanted Fetch to be able to understand any food that I didn't know about and even multi word restaurants, like "Pizza Hut". This required much more work. Fetch looks at the sentence as a whole to determine what you are looking for.  Lambda is a little slow to update DNS, so we couldn't see our webserver for a few hours.Chrome doesn't have a lot of documentation for extensions...None of us used Scalingo before, so took a little time to set up.  First, we had huge problems programming the garbage bins to open automatically with the help of Servo Motors. Second, it was no easy task to capture the exact moment the garbage item gets in front of the iPad camera. And third, we had to create quite a big data base to train the IBM Watson neural network.  Summarizing articles and scoring relevancy is hard, but creating a consistent personalized message is extremely challenging. In order for it to consistently work, we had to train our AI system for StudyTree (Ethan's startup). To overcome this challenge, we will allow users to input more information so Warmly can learn how to better warm cold email. While using AI to create a unique personalized message is the goal, often times it is simply not possible due to lack of data. In these scenarios, we supply the end users summary, keywords, quotes, and mentions about the target which enables them to create their own message. AI + Human still drastically reduce research and personalization from 20 minutes to only  <2 mins.     Figure out how to take a contract that is written to provide assets priced in dollars and invent a pricing structure for the interest rate swap, and how the pricing structure may fit into the existing Market Protocol smart contract that we have already deployed on Kovan. We encountered a hard coded Infura key that hit its rate limit today in the Airswap widget, and the widget code is not open sourced. We downloaded each file until we got to the one that had the hard coded Infura key, replaced the key with our own and uploaded the entire file structure to private services to re-serve it to our app.   We had a tough start because two of our team members forgot to sign up :CIt was a cold hackathon! Fingers were stiff but we persevered on, thanks for the Coffee and endless care from the NUS Hackers Core Team.On another note, optimizing the algorithm to check for faces in an image and stitching the dog images over were surprisingly quite challenging.Accomplishments that we are proud ofWe can make you look like dog.Braved through the cold night.Also, managed to build a Chrome extension and utilized Microsoft Cognitive Face API.What we learnedLearned to sign up for hackathon before going for it.We learned to use 3D manipulation through css and Microsoft Cognitive Services.Ps. No offense if offended.Built WithchromejavascriptTry it outgithub.comjingloon.github.iowww.instagram.com      Submitted to    Hack&Roll 2017Winner                Most Awesomely Useless Hack                  Created by  Explore Microsoft Cognitive Face API.Wei Kang ChiaStudent at National University of SingaporeContributed some stuffBernard YipTime and money and the soft shell crabJing Loon Gohstudent with occasionally weird ideasHappened to have messed around with Google extensions, utilized my experienceJoel Lim  BLHD  We initially struggled with API rate limits and approval wait times for getting enough data to train our models, but we were able to get around this by using a combination of APIs and web-scraping.Determining the best way to balance our training data was also difficult, we initially overrepresented certain classifications in the market, which resulted in our models predicting the same outcome, no matter the text. Setting caps on the number of training samples for each category helped us resolve this, however we have a lot to learn.Finally, all four of us are new to JavaScript, Node.js, React.js, and machine learning so we were challenged by the fact that we needed to teach ourselves and each other about the tools we were using as we were using them, all while making sure that we are making progress at an appropriate pace.  Since we formed the team for the first time in this Hackathon, we needed to create prototypes of web application, hardware, smartphone application, 3D model from scratch within 24 hours. That was the most difficult challenges we ran into.  Finding a dataset of DeepFakes large enough to train our model was difficult. We ended up splitting the frames into 70% dataset, and 30% of it is used for testing (all frames are different however). Automatically exporting data from JavaScript to Python was also difficult, as JavaScript can not write on external files.Therefore, we utilized a server and were able to successfully coordinate the machine learning with our web application and Chrome extension!    No member of our group has ever used a Raspberry Pi prior to this Hack-a-thon. In addition, we had no prior experience with Bluemix or Amazon's developer tools. As a result, there was a steep learning curve for integration of these numerous services.  As individuals who have mainly done projects in C++, learning Python was an interesting challenge.Different schedules made coordinating the design of the project was difficultWorking in different environments (Mac and Windows) led to some comptability problems    Lack of supports and technical documentations of different DeFI projects. We had to do most of testing on mainnet and lost funds while testing it.  JavascriptCosine similarityMath of course              The biggest challenge is that feeding outputs back into the neural network will cause the network to converge. In the beginning, my compositions often converged to a single, repeating note. This produced extremely boring music. I fixed this by introducing mutations to the network. First I tried to sample a Gaussian distribution based on the mean and standard deviation of the error vectors. Then, I switched to a small percentage of changing the note by a perfect fifth or a perfect octave, both of which do not affect the flow of the music much. Furthermore, both the treble and bass clefs converged to the same note. I fixed this problem by splitting the the network in two, as mentioned earlier. Finally, neural networks do not give very exact predictions. Unfortunately, in music, even the slightest deviation will cause the music to sound out of tune. I often had quarter tones and random durations of notes. I fixed this by rounding the pitch and duration, which significantly increased the quality of the music.                  We had multiple issues with framework (the backend is not fully executed) and initiating concept, but were able to be flexible.    Comprhension - comment bien communiquer avec Slack. La comprhension n'a pas t facile; comprendre les bonnes permissions du bot, le fonctionnement de l'OAuth, le dploiement automatique sur Azure et la cration complte et modulaire de l'application en C#. C'est avec brio que nous avons pu crer par nous-mme la solution qui nous amnera la cl du succs!  Most online resources are specific to either the API, add-ins, or Azure but not for synchronizing all together into one app. There was a lot of trial and error with documentation across the board, until we painstakingly discovered solutions that worked for all-in-one. Initial authentication was fairly straightforward when utilized via the web browser. However, in order to also enable authentication in Outlook, we then needed to rework sizeable functionality to follow the popup standard.Keeping user preferences and settings synced between our database and the results of the 0365 API. Throttling our data calls to prevent endpoint errors, while simultaneously minimizing our app load time.Swapping out callback hell for promises as we process the returning images.The 0365 message id didn't sync with the EWS mail item id because of minute encoding differences, until our regex mapped the two. -- hopefully, it works!hasAttachment = false for messages that contain in-line images, yet these images are still being returned by the attachment endpoint, without a direct reference to the containing message.Browser compatibility issues. We found no documentation specifying the browser environments for add-ins within Outlook 2016, requiring us tojump through multiple hoops to cover all possibilities.  Reomotely communicate with Pepper Robot  Unfortunately, the iOS SDK login function crashed because of the XCode and Swift updates... This issue is still very fresh and so we couldn't interface with the Photon through the app :( But we can still use a command line interface for testing purposes.        We had some initial challenges learning how to use Alexa.   Many of our early struggles were on the vision side. It was tough to find example of advanced facial recognition on the Kinect, and working with Visual studio. The python processing was also difficult since we wanted to simplify all the data in to something any user could understand. We struggled with React and cloud web services and spent much of our time bouncing around between services. In general our team put emphasis on learning new things (React, Python Flask, Visual Studio, ect) rather than staying in our comfort zone. This inherently made things difficult.Accomplishments that we proud ofAccurate, reliable face trackingVisual C++Creative metricsGoogle mapsReactHardwareSimple designMaking something that has potential to improve safetyWhat I learnedExpect anything. This project spawned after a team member was hit by a car driven by a distracted driver. Many roadblocks emerged along the project's development, but we we continued to move forward!What's next for distractednessImplement it in an actual vehicle, add a social media aspectBuilt Withc++jquerykinectpythonreactsurfacevisual-studioTry it outgithub.com159.203.247.122      Submitted to    DubHacks 2015Winner                Best Travel Hack (Concur)              Winner                Most Innovative Use of Data by GE Digital                  Created by  Worked with the Kinect API  and applied face recognition and the associated metrics in c++. Did the post processing and some backend with python. New to visual studio and the Kinect.jake-gI worked on the backend, and got the frontend talking to our server. First time using Flask, and second time using React. It was hard, but very fun!Chris GervangAmritPMichelle Chan    We had a hard time when we first ran our summarizer on the Slack conversations, because it wasn't optimized for multiple topics. We fixed this by splitting the conversation.It was hard to get our hands on an Echo, so we developed most of it without being able to see results on the go. Our Slack summarization backend saved us though. Also, we had a hard time deploying packages on AWS because it needed us to make special versions of NLTK and other packages to run.      